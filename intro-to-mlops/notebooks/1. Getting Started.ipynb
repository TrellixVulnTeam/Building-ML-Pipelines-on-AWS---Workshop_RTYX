{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps with Amazon SageMaker: Getting Started\n",
    "\n",
    "> *This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio*\n",
    "\n",
    "## Welcome\n",
    "\n",
    "In this short workshop on MLOps with Amazon SageMaker, we'll model credit risk using SageMaker's [XGBoost Algorithm implementation](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) on a customized extract of the [Statlog German Credit dataset](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)), sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). This first notebook demonstrates connecting to the pre-loaded data in Amazon S3; preparing it and training/evaluating an initial model; while later sections will cover automation in the modelling pipeline.\n",
    "\n",
    "This notebook is intended to be run in [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html), assuming the [workshop CloudFormation stack](https://github.com/apac-ml-tfc/intro-to-mmlops) has been deployed.\n",
    "\n",
    "**If you're new to Python notebooks:**\n",
    "\n",
    "- Notebooks combine commentary (like this) and code in one interactive environment. When you first start a notebook in SageMaker Studio, you may be asked to select a **kernel** environment to run it in (see above).\n",
    "- In **SageMaker Studio**, starting a new kernel may take a couple of minutes at first while a **new container (app)** is provisioned. Refer to the *Kernel starting* status at the bottom left of the screen; the *Running terminals and kernels* 'stop button' tab on the left; and your user's \"Apps\" list from the [SageMaker Studio Console](https://console.aws.amazon.com/sagemaker/home?#/studio/) for status... And wait until your kernel is ready before trying to run code!\n",
    "- Once the kernel is ready, you can select cells (the active cell has a blue bar to the right) and then run code cells by pressing the play button (▶️) in the toolbar above or pressing `Shift+Enter`\n",
    "- The status indicators in the top right (⚪️/⚫️) and bottom left (status text) of the screen show whether the kernel is busy or idle\n",
    "\n",
    "## Contents\n",
    "\n",
    "0. [Some extra installations](#0.-Some-extra-installations)\n",
    "1. [Locating our project environment](#1.-Locating-our-project-environment)\n",
    "2. [Creating a data preparation flow with SageMaker Data Wrangler](#2.-Creating-a-data-preparation-flow-with-SageMaker-Data-Wrangler)\n",
    "    - [Opening Data Wrangler](#Opening-Data-Wrangler)\n",
    "    - [Connecting the raw dataset](#Connecting-the-raw-dataset)\n",
    "    - [Analyzing the raw data](#Analyzing-the-raw-data)\n",
    "    - [Our first transform - the target column](#Our-first-transform---the-target-column)\n",
    "    - [Other string-to-boolean maps](#Other-string-to-boolean-maps)\n",
    "    - [Extracting ordinals](#Extracting-ordinals)\n",
    "    - [More-complex-text-featurizations](#More-complex-text-featurizations)\n",
    "    - [One-hot-encoding](#One-hot-encoding)\n",
    "    - [Train-test split flag](#Train-test-split-flag)\n",
    "    - [IF YOU RUN OUT OF TIME...](#IF-YOU-RUN-OUT-OF-TIME...)\n",
    "3. [Running the data transformation job](#3.-Running-the-data-transformation-job)\n",
    "4. [Extracting training/validation/test splits](#4.-Extracting-training/validation/test-splits)\n",
    "5. [Training an initial model with SageMaker XGBoost Algorithm](#5.-Training-an-initial-model-with-SageMaker-XGBoost-Algorithm)\n",
    "6. [Next steps](#Next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Some extra installations\n",
    "\n",
    "Before we get started wih the project itself, run the cell below to install some extra libraries that will be useful for us later - but may not be present in your SageMaker notebook kernel by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: altair in /opt/conda/lib/python3.7/site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas>=0.18 in /opt/conda/lib/python3.7/site-packages (from altair) (1.0.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from altair) (2.11.1)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.7/site-packages (from altair) (0.10.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from altair) (0.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from altair) (1.18.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from altair) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.18->altair) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.18->altair) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->altair) (1.1.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair) (1.5.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair) (19.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair) (45.2.0.post20200210)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->altair) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Locating our project environment\n",
    "\n",
    "By provisioning this \"project\" stack from the [AWS Service Catalog](https://console.aws.amazon.com/servicecatalog/home?#products), as below...\n",
    "\n",
    "![](imgs/01-01-service-catalog-product.png \"AWS Service Catalog screenshot showing ML project product\")\n",
    "\n",
    "...You've already:\n",
    "\n",
    "- Created a **raw data bucket** in [Amazon S3](https://s3.console.aws.amazon.com/s3/home?#) (which this notebook has *read-only* access to)\n",
    "- Loaded the **raw source dataset** into the bucket\n",
    "- Created a **sandbox data bucket** in Amazon S3 (which this notebook has *read-write* access to)\n",
    "- Stored these important locations in the [AWS Systems Manager Parameter Store](https://console.aws.amazon.com/systems-manager/parameters/?tab=Table)\n",
    "\n",
    "...So now we can **look up these project configuration parameters from the *project ID***, with the code below.\n",
    "\n",
    "> **To simplify this example**, we tied the provisioning of your SMStudio user and ML project together - so your `project_id` is hard-coded to `creditmodel-USERNAME`\n",
    ">\n",
    "> In real-world settings the relationship between data scientists and projects is many-to-many\n",
    "\n",
    "▶️ **Edit** the `project_id` below to match your deployed service catalog product, and then **run the cells** to look up the locations of your S3 buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<util.project.ProjectSession(\n",
       "  project_id=creditmodel-yudhiesh,\n",
       "  role=arn:aws:iam::867352166187:role/DataScience/SC-867352166187-pp-7pfghfxa-SageMakerExecutionRole-P4ECNYCU9XXE,\n",
       "  raw_bucket=creditmodel-yudhiesh-mlrawdata-867352166187-us-east-1,\n",
       "  sandbox_bucket=creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1\n",
       ") at 0x7f9cf6c692d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# The 'util' folder contains utilities we'll use throughout the workshop to keep the notebook itself tidy...\n",
    "# Feel free to go and check out the implementations! What utils would you create for your own environments?\n",
    "import util\n",
    "\n",
    "project_id = \"creditmodel-yudhiesh\"  # TODO: Change this if you set a different Project ID in service catalog!\n",
    "\n",
    "project_config = util.project.init(project_id)  # (Look up this project's parameters in AWS SSM)\n",
    "project_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data bucket: s3://creditmodel-yudhiesh-mlrawdata-867352166187-us-east-1/\n",
      "Sandbox bucket: s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw data bucket: s3://{project_config.raw_bucket}/\")\n",
    "print(f\"Sandbox bucket: s3://{project_config.sandbox_bucket}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (read-only) `raw_bucket` contains our input data, and we'll use the `sandbox_bucket` for storing our transformations, models, evaluations, and other artifacts. You should be able to see these buckets in the [Amazon S3 Console](https://s3.console.aws.amazon.com/s3/) - with the raw data already loaded like the below:\n",
    "\n",
    "![](imgs/01-02-raw-data-s3.png \"S3 Console screenshot showing raw data file in bucket\")\n",
    "\n",
    "> ❓ How would **you** separate and control access to *your* environment's different artifacts, to foster productivity and governance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Creating a data preparation flow with SageMaker Data Wrangler\n",
    "\n",
    "This raw data is **not yet ready for modelling**.\n",
    "\n",
    "We'll need to review and curate the features; split out training/validation/test sets; and ensure the data is in an appropriate format for using with our target algorithm. From the [SageMaker XGBoost Input/Output Interface docs](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost), we can see our data will need to be:\n",
    "\n",
    "- In CSV (or Parquet) format on Amazon S3 (at least this is already satisfied)\n",
    "- Having the **target column we want to predict at the front** of the dataset\n",
    "- Containing **only numeric fields** - no text or etc\n",
    "\n",
    "Rather than downloading the data and using traditional in-memory, code-based tools like Pandas in the notebook, we'll use [SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/) to create a data preparation recipe or **flow**, which can be then run as job in [SageMaker Processing](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/) on dedicated infrastructure.\n",
    "\n",
    "### Opening Data Wrangler\n",
    "\n",
    "In this example we've already created a blank **flow** file for you, so go ahead and open [credit-data.flow](credit-data.flow) using the folder tab to the left.\n",
    "\n",
    "> ⏰ SageMaker Data Wrangler runs in a separate SageMaker **App**, so just like this notebook there may be a short wait while the environment initializes ready for you to use.\n",
    ">\n",
    "> You'll see a warning like the below for a few seconds to a few minutes:\n",
    "\n",
    "![](imgs/01-03-data-wrangler-starting.png \"Screenshot of SageMaker Studio with Data Wrangler starting up\")\n",
    "\n",
    "Once the warning message goes away, you should see a `Data Wrangler` instance & app in your *Running Terminals and Kernels* tab on the left - and a new \"app\" in your user's detail page on the [SageMaker Studio Console](): you're ready to get started!\n",
    "\n",
    "![](imgs/01-04-apps-with-data-wrangler.png \"Screenshot of SageMaker Console apps view for user, with Data Wrangler app ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the raw dataset\n",
    "\n",
    "▶️ **Select** \"Amazon S3\" and browse to *your project's* `raw_bucket` (as we retrieved in the [Locating our project environment](#1.-Locating-our-project-environment) section above).\n",
    "\n",
    "▶️ **Select** the `german.csv` file that's been pre-populated into your bucket and (since this is a small dataset) you can **turn sampling off**.\n",
    "\n",
    "![](imgs/01-05-select-raw-data.png \"Data Wrangler screenshot with raw data file selected\")\n",
    "\n",
    "▶️ **Click** *Import dataset* to go ahead and connect the dataset once you've checked the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the raw data\n",
    "\n",
    "Importing a dataset from the \"Import\" tab creates two nodes on your **data flow view**, which Data Wrangler uses to visualize the flow you're building:\n",
    "\n",
    "1. The data source itself, and\n",
    "2. The *inferred data types* from the CSV - which we can review and customize if needed.\n",
    "\n",
    "In this example, the default types are fine... But before we start transforming our data we'll first add some **analyses** to understand it better.\n",
    "\n",
    "▶️ **Click** on the `+` next to the *Data types* node and select *Add analysis* to get started\n",
    "\n",
    "![](imgs/01-06-add-analysis.png \"Data Wrangler screenshot with 'add analysis' option selected\")\n",
    "\n",
    "#### Missing Values (Custom Chart)\n",
    "\n",
    "First, we'll use a **custom** analysis with some pre-prepared code - to analyze the proportion of missing values in the data by field.\n",
    "\n",
    "▶️ **Create a custom chart** with the code from [charts/missing.py](charts/missing.py):\n",
    "\n",
    "- Select the *Code* tab on the right\n",
    "- Set the *Name* to `Initial Missing Values`\n",
    "- Open [charts/missing.py](charts/missing.py) (from this link or the folder tab) and copy/paste in the code\n",
    "- Click *Preview* and then *Create* to store your chart\n",
    "\n",
    "![](imgs/01-07-missing-values-chart.png \"Screenshot of chart showing no missing values\")\n",
    "\n",
    "Although this chart would normally show a bar chart of missing value proportions by field, the good news is that this dataset appears to have no missing values at all!\n",
    "\n",
    "#### Quick Model (Built-In Analysis)\n",
    "\n",
    "Let's also use one of the built-in analysis, *Quick model*:\n",
    "\n",
    "▶️ **Create a Quick Model analysis**:\n",
    "\n",
    "- Click the *Create new analysis* button\n",
    "- Select *Quick Model* from the *Analysis type* drop-down\n",
    "- Set the *Analysis name* to `Initial Quick Model`\n",
    "- Set the *Label* to `credit_risk` (near the bottom of the column list)\n",
    "- Click *Preview* (and, whenever you're ready, *Create*)\n",
    "\n",
    "![](imgs/01-08-initial-quick-model.png \"Bar chart of feature importances from initial quick model\")\n",
    "\n",
    "This analysis performs a simple train/test split of your model; trains a random forest model; and outputs a chart of feature importances and the overall test set accuracy metric.\n",
    "\n",
    "While correlation analyses are still useful for identifying potentially important or redundant features; going beyond this to train a simple but non-linear model and analyse its quality of fit can take our insights to the next level during feature engineering. In this case, that the balance in the customer's checking account and the amount of the loan were highly predictive features for credit risk... While their domestic vs foreign worker status and the recorded transaction timestamp seemed essentially irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first transform - the target column\n",
    "\n",
    "Remember how our [XGBoost Algorithm specification](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost) needed numeric columns only and the target column to appear first? Let's sort that out for our first transformation:\n",
    "\n",
    "▶️ **Click** on the *Prepare* tab in Data Wrangler to go back to the data flow view.\n",
    "\n",
    "▶️ **Click** the `+` next to the *Data types* node and select to *Add transform* to open the transform view.\n",
    "\n",
    "In the transform view, we see a preview of our working dataframe with options too add pre-built and custom transformations in the *TRANSFORM > Add* tab to the right. We can also switch over to the *Previous steps* tab on the right to review the steps so far on this branch of the flow:\n",
    "\n",
    "![](imgs/01-09-add-transform-view.png \"Screenshot of Data Wrangler 'add transform' view\")\n",
    "\n",
    "▶️ **Create a numeric target column**:\n",
    "\n",
    "- Select the *Custom formula* transform section to create a custom Spark SQL transform\n",
    "- Enter *Formula* `cast(credit_risk == \"bad\" as int)` for a numeric value\n",
    "- Set *Output Column* to `credit_default` - whether the credit defaulted\n",
    "- Click *Preview* and, when you're happy the new column added is correct, *Add* to add the transformation step\n",
    "\n",
    "![](imgs/01-10-numeric-target.png \"Previewing numeric credit_default column\")\n",
    "\n",
    "▶️ **Manage the target column ordering**:\n",
    "\n",
    "- Select *Manage columns* and the default *Drop column* transform, setting the old column `credit_risk` to drop the text version\n",
    "- Click *Preview*, check the right `credit_risk` column has been deleted, and then click *Add* to save the step\n",
    "- Select *Manage columns* and the *Move column* transform, selecting to `Move to start` the new column `credit_default`\n",
    "- Click *Preview*, check the `credit_default` column has been moved to the start of the dataframe, and then click *Add*\n",
    "\n",
    "You should now have a numeric `credit_default` column at the start of your dataframe, and no `credit_risk` column anymore... Like this:\n",
    "\n",
    "![](imgs/01-11-fixed-target.png \"Screenshot of steps history with target column fixed to numeric at start\")\n",
    "\n",
    "If we go back to the *Prepare* tab in Data Wrangler, we'll now see a new 'stacked' set of *Steps* nodes have been added after the *Data types* configuration.\n",
    "\n",
    "> Since the sequence of steps is linear with no branches, they're collapsed to a single node. We can use the plus button before to add new transformations, but be aware we can also insert **branches** into the flow by selecting one of the individual sub-steps - like this:\n",
    "\n",
    "![](imgs/01-12-inserting-branches.png \"Screenshot of prepare tab inserting branch transforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other string-to-boolean maps\n",
    "\n",
    "There are two other fields in this dataset represented as strings which should be `0/1` variables instead: `telephone` and `foreign_worker`.\n",
    "\n",
    "You can use similar `cast(XYZ == \"ABC\" as int)` *Custom formula* transforms to create the new numeric fields, and the *Manage columns > Drop column* transform as we did before to drop the old fields.\n",
    "\n",
    "▶️ **Click** the `+` button after the *Steps (3)* node from the *Prepare* tab to add new transforms to the **end** of our sequence.\n",
    "\n",
    "▶️ **Convert** the `telephone` field to `has_telephone`\n",
    "\n",
    "▶️ **Convert** the `foreign_worker` field to `is_foreign_worker`\n",
    "\n",
    "Your working dataframe should now look something like this:\n",
    "\n",
    "![](imgs/01-13-numeric-telephone-foreign.png \"Telephone and foreign worker fields converted to numerics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting ordinals\n",
    "\n",
    "The `checking_acct_status`, `savings_status`, and `highest_property` fields are all encoded as text, but with values that clearly represent **ordinal sequences**... For example `No Account / Band 1 / Band 2 / etc.`\n",
    "\n",
    "For all of these fields the first number (if present) is the ordinal indicator, so we'll simply use a Regular Expression and fill missing values to encode the fields.\n",
    "\n",
    "▶️ **Extract band number** in the `checking_acct_status` field:\n",
    "\n",
    "- Use a *Search and edit > Extract using RegEx* transform with `checking_acct_status` as the input column\n",
    "- Enter the *Pattern* `\\d+`\n",
    "- Leave the *Output column blank* to just overrride the input column\n",
    "- Click *Preview* and, when you're happy the configuration is correct, *Add*.\n",
    "\n",
    "Your `checking_acct_status` field should now be reduced to just numbers with gaps where the records were previously \"No Account\":\n",
    "\n",
    "![](imgs/01-14-preview-regex-extract.png \"Previewing RegEx extract transform\")\n",
    "\n",
    "Because this was a regular expression operation, the result column is typed as text even though the values are all numeric... And because customers without an account had no number in the original text; we've introduced missing values for this class.\n",
    "\n",
    "▶️ **Convert and fill** the `checking_acct_status` field:\n",
    "\n",
    "- Add a *Parse column as type* transform on `checking_acct_status` to convert it to `Long` type (integer)\n",
    "- Add a *Handle missing > Fill missing* transform on the same column with *Fill value* `0` (again, leave the *Output column* blank to directly overwrite `checking_acct_status`)\n",
    "\n",
    "Your `checking_acct_status` column should now be listed as a numeric type, and completely filled with no gaps, like below:\n",
    "\n",
    "![](imgs/01-15-checking-acct-converted.png \"Checking account column finished\")\n",
    "\n",
    "This field is now completed... so we just need to repeat the same process for our two other ordinal fields:\n",
    "\n",
    "▶️ **Repeat the extract/convert/fill process** for the `savings_status` and `highest_property` fields\n",
    "\n",
    "- You can use the same `\\d+` regular expression for both fields\n",
    "- The `highest_property` field actually has numbers in all entries - so you still need to do the type parsing but can skip the missing value fill!\n",
    "\n",
    "All three fields `checking_acct_status`, `savings_status` and `highest_property` should now be converted to numerics with no gaps:\n",
    "\n",
    "![](imgs/01-16-ordinals-done.png \"Ordinal variable processing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex text featurizations\n",
    "\n",
    "The `marital_status_and_gender` field is interesting, because:\n",
    "\n",
    "- It combines two semantically separate features\n",
    "- Although every record is labelled as \"male\" or \"female\", the recorded marital status options differ between the two genders.\n",
    "\n",
    "...So we'll try to take account of this in our encoding.\n",
    "\n",
    "▶️ **Split** the `marital_status_and_gender` field's components:\n",
    "\n",
    "- Select a *Search and edit > Split string by delimiter* transform and select the input column\n",
    "- Set the *Delimiter* to ` : ` (**WITH A SPACE BEFORE AND AFTER THE COLON!**)\n",
    "- Set the *Max number of splits* to `2`\n",
    "- Leave the *Output column* blank so the column is updated in-place\n",
    "\n",
    "This creates a **vector column** (shown by the square bracket notation e.g. `[male, single]` in the preview). We can address vector columns in custom formulae, as well as splitting them out into separate columns.\n",
    "\n",
    "▶️ **Use a custom formula** to extract a numeric `gender_is_male` field:\n",
    "\n",
    "- Create a *Custom formula* transform with the formula: `cast(marital_status_and_gender[0] == \"male\" as int)`\n",
    "- Set the *Output column* to `gender_is_male`\n",
    "\n",
    "Check your new column has `1` for records where the original `marital_status_and_gender` started with `male`, and `0` where it was `female`!\n",
    "\n",
    "Now we just need to process the marital status part of our field...\n",
    "\n",
    "▶️ **Flatten the vectorized marital status**:\n",
    "\n",
    "- Select a *Manage vectors > Flatten* transform to extract out columns from the vectorized `marital_status_and_gender` field\n",
    "- Leave the *Output prefix* and *Vector length* settings as default. The preview should show two new columns created at the end of your dataframe: `marital_status_and_gender_0` (the gender) and `marital_status_and_gender_1` (the marital status)\n",
    "- When you're comfortable the preview looks like the below, go ahead and click *Add*\n",
    "\n",
    "![](imgs/01-17-marital-gender-split.png \"Screenshot showing gender_is_male field next to split marital/gender text\")\n",
    "\n",
    "▶️ **Featurize the marital status**:\n",
    "\n",
    "- Select the *Featurize text > Vectorize* transform\n",
    "- Set *Tokenizer* to `Custom` and set the *RegEx pattern* to a simple forward slash `/` (since our marital statuses are separated by a slash)\n",
    "- Keep the other *Vectorizer* settings as default, but...\n",
    "- Change *Apply IDF* from the default Yes to `No`\n",
    "- Select the *Input column* `marital_status_and_gender_1` (the marital status split)\n",
    "- Select *Output format* `Columns` and the *Output column* to be `marital_status`\n",
    "- Hit *Preview* and, when you're happy the setup matches the screenshot below, go ahead and click *Add*\n",
    "\n",
    "This transform creates a set of columns identifying how many times (always 0 or 1 for us) each marital status appears in the record - so basically a multi-hot encoding to account for the fact that we don't always know exactly which status applies!\n",
    "\n",
    "![](imgs/01-18-marital-status-encoding.png \"Multi-hot encoded marital status\")\n",
    "\n",
    "▶️ **Clean up temporary columns**:\n",
    "\n",
    "- Finally, use the *Manage columns > Drop column* transform to delete the three columns we no longer need: `marital_status_and_gender`, `marital_status_and_gender_0` and `marital_status_and_gender_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "You should now have just 4 text fields left in your dataset: `purpose`, `other_parties`, `other_installment_plans` and `housing`\n",
    "\n",
    "All of these are fair candidates for **one-hot** encoding, since there's no obvious ordering.\n",
    "\n",
    "▶️ **One-hot encode the target fields**:\n",
    "\n",
    "- Select the *Categorical encoding > One-hot encode* transform type\n",
    "- Change the *Output style* to `Columns` but leave other options (*Output column*, *Invalid handling strategy, etc) as the default\n",
    "- Check the transform with `purpose`, and repeat for the three other fields `other_parties`, `other_installment_plans` and `housing`\n",
    "\n",
    "In all cases the original column (e.g. `purpose`) will be deleted, and replaced with a set of new columns at the end of the dataset describing what category each record fell under (e.g. `purpose_car_new`, `purpose_repairs`, etc).\n",
    "\n",
    "> ⚠️ **CHECK:** We're now done with engineering the input features, so your data should be fully numeric with no text fields left!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split flag\n",
    "\n",
    "So far we've been engineering features on our whole dataset, which of course is nice for ensuring consistency.\n",
    "\n",
    "...But of course we'll need to segment our data into model training, validation, and test sets to explore performance. Even better if this split is:\n",
    "\n",
    "- **Unbiased** (We don't know if there's some ordering by specific features in our core dataset)\n",
    "- **Repeatable** (So we don't accidentally leak information from the test set into validation, over multiple experiments\n",
    "- **Stratified** (So there's no risk of particular groups being grossly over- or under-represented in each set\n",
    "\n",
    "In this example we'll conduct the *assignment* of this set in Data Wrangler (creating a flag column), but the *splitting* afterwards - in a realization step where train/val/test datasets are queried from our overall processed feature store. We'll do this by way of a sequence of custom Spark SQL transforms.\n",
    "\n",
    "▶️ **Assign dataset split labels to the data**:\n",
    "\n",
    "- Use a *Custom formula* transform:\n",
    "    - Formula `random(1337)`\n",
    "    - Output column `random_01`\n",
    "    - ...to repeatably assign each record a random ordering value.\n",
    "- Use a *Custom formula* transform:\n",
    "    - Formula `row_number() over (partition by credit_default order by random_01)`\n",
    "    - Output column `stratum_row_num`\n",
    "    - ...to rank each record within the strata (`credit_default` target label only)\n",
    "- Use a *Custom formula* transform:\n",
    "    - Formula `stratum_row_num / (max(stratum_row_num) over (partition by credit_default))`\n",
    "    - Output column `stratum_rank_pct`\n",
    "    - ...to express each record's randomized ranking as a percentage of the stratum size\n",
    "- Use a *Custom formula* transform:\n",
    "    - Formula `case when (stratum_rank_pct < 0.7) then \"train\" when (stratum_rank_pct < 0.85) then \"validation\" else \"test\" end`\n",
    "    - Output column `dataset`\n",
    "    - ...to assign proportions of each stratum to the different datasets train, validation, and test\n",
    "\n",
    "Your results should look something like the below. Don't be worried if all rows in the preview are for the `train` dataset - as the preview is only a sample of the total!\n",
    "\n",
    "![](imgs/01-19-assigning-datasets.png \"Working columns to split out datasets\")\n",
    "\n",
    "▶️ **Drop the temporary columns**:\n",
    "\n",
    "- Finally, use *Manage columns > Drop column* transforms to drop the `random_01`, `stratum_row_num` and `stratum_rank_pct` columns which are no longer needed: Leaving just the `dataset` label.\n",
    "\n",
    "**Our flow is finally complete!**\n",
    "\n",
    "We've converted all our raw text fields to numbers, and added just one field `dataset` at the end of the dataframe to identify which split each record falls in.\n",
    "\n",
    "Clicking on the *Prepare* tab, we still see only 3 nodes as there were no branches in our flow, but you should have worked up ~33 \"Steps\" in the transform section, if you followed along exactly!\n",
    "\n",
    "![](imgs/01-20-completed-prep-flow.png \"Prepare tab screenshot showing completed flow\")\n",
    "\n",
    "▶️ **Save** the flow file by pressing `Ctrl+S` with the `credit-data.flow` tab open!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF YOU RUN OUT OF TIME...\n",
    "\n",
    "Although it's good to get hands-on with the tool, composing all these steps might take some time to get used to at first!\n",
    "\n",
    "> ⚠️ To use your flow with the rest of this notebook, you'll need to have the `dataset` field (final section above) set up.\n",
    ">\n",
    "> **To use our pre-built flow with your data instead**, if you run out of time, you can:\n",
    ">\n",
    "> - Right click the `credit-prebuilt.flow` file and select *Open in Editor* to open the file as JSON\n",
    "> - Find (near the top) the `s3Uri` that looks like `s3://DOC-EXAMPLE-BUCKET/german.csv`\n",
    "> - Replace the `DOC-EXAMPLE-BUCKET` with the name of **your `sandbox_bucket`** (see earlier in this notebook)\n",
    "> - Save the file\n",
    ">\n",
    "> In the section below, your `target_output_name` will be `e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default` (the last node in the file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Running the data transformation job\n",
    "\n",
    "We've **defined** our sequence of transformation steps in the flow file (which is just JSON: try right-clicking it in the folder tab and selecting `Open in Editor`!)... So now we'd like to **run it against the full dataset**.\n",
    "\n",
    "Data Wrangler supports **exporting** flows to either plain Python code (for running in other environments), or as **template notebooks for triggering [SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) jobs**, either:\n",
    "\n",
    "1. Directly, to output results to **Amazon S3**\n",
    "2. With additional configurations output results to [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/)\n",
    "3. Inside a [SageMaker Pipeline](https://aws.amazon.com/sagemaker/pipelines/)\n",
    "\n",
    "For today's session, our environment unfortunately doesn't support SageMaker Feature Store... So we'll run option 1 (a processing job) and then go on to show SageMaker Pipelines!\n",
    "\n",
    "Specifically we'll **show** the auto-generated notebook, but **not run it**: Instead, you'll copy out some key values and set up the job from this notebook - to demonstrate that the auto-generated code is just a guideline.\n",
    "\n",
    "▶️ **Generate** a *Data Wrangler Job notebook*\n",
    "\n",
    "- Click on the *Export* tab in Data Wrangler\n",
    "- Select to export the final step by clicking on your *Steps* node and selecting the *final* step in the pop-up\n",
    "\n",
    "> ⚠️ **Note:** At one point there was a UI bug where, depending on your screen and the number of steps, this pop-up could be **taller than the window** - making it impossible to select the final step!\n",
    ">\n",
    "> If this happens, you can **zoom out in your browser** (`Ctrl+-` or the `View` menu) until the view is big enough for the final step to be selected\n",
    "\n",
    "- Once you've selected the final step, click the *Export step* button in the top right and select the `Data Wrangler Job` option\n",
    "\n",
    "![](imgs/01-21-flow-export.png \"Exporting the flow to a Data Wrangler Job notebook\")\n",
    "\n",
    "This will open a new \"untitled.ipynb\" auto-generated notebook, as below:\n",
    "\n",
    "> ⚠️ **Note:** If you're prompted to select a kernel for the notebook, choose `Python 3 (Data Science)`\n",
    "\n",
    "> ⚠️ **You don't need to run the contents of this notebook!** We'll just extract some key parameters and bring them back here\n",
    "\n",
    "![](imgs/01-22-dw-job-notebook.png \"Auto-generated data wrangler notebook\")\n",
    "\n",
    "> ⚠️ **You don't need to run the contents of this notebook!** We'll just extract some key parameters and bring them back here\n",
    "\n",
    "We just need to extract two values from this notebook's **Parameters** section:\n",
    "\n",
    "- The `container_uri` - the location & version of the Data Wrangler container image to use in the processing job\n",
    "- The `output_name` - the unique ID of the flow node to be output\n",
    "\n",
    "▶️ **Copy** these values from the auto-generated notebook to the cell below, and then run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_uri = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\"# TODO: Get from the example notebook (including the quote marks)\n",
    "target_output_name = \"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\" # TODO: Get from 'output_name' in the example notebook (including the quote marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these and your saved flow file (which already includes the locations of input data), we have all the information needed to run the flow as a SageMaker Processing job to create the result files in S3.\n",
    "\n",
    "First, the cell below will connect to the SageMaker APIs in this region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker  # High-level Python SDK for Amazon SageMaker\n",
    "smsess = sagemaker.Session()\n",
    "region = smsess.boto_region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Then in the next two cells, we'll use some more `util` functions (similar to the code in the auto-generated notebook) to define and run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = sagemaker.Processor(\n",
    "    role=sagemaker.get_execution_role(),  # Just use the same permissions as the current notebook's IAM role\n",
    "    image_uri=container_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Processing job with name credit-flow-2021-03-25-08-14-10\n",
      "Uploading flow file to s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/flow/credit-data.flow\n",
      "Storing results to s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler\n",
      "Uploaded credit-prebuilt.flow to s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/flow/credit-data.flow\n",
      "\n",
      "Job Name:  credit-flow-2021-03-25-08-14-10\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/flow/credit-data.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'german.csv', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://creditmodel-yudhiesh-mlrawdata-867352166187-us-east-1/german.csv', 'LocalPath': '/opt/ml/processing/german.csv', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..........................\u001b[34mSPARK_HOME=/usr/lib/spark\u001b[0m\n",
      "\u001b[34mHOSTNAME=ip-10-0-226-231.ec2.internal\u001b[0m\n",
      "\u001b[34mNB_USER=sagemaker-user\u001b[0m\n",
      "\u001b[34mSHELL=/bin/bash\u001b[0m\n",
      "\u001b[34mHADOOP_HOME=/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34mYARN_RESOURCEMANAGER_USER=root\u001b[0m\n",
      "\u001b[34mAWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/v2/credentials/qO29shxAIFddn9Qf3UGYBsmpCjuq_XnNctGu5-rqFSA\u001b[0m\n",
      "\u001b[34mPYTHONUNBUFFERED=1\u001b[0m\n",
      "\u001b[34mLC_ALL=en_US.UTF-8\u001b[0m\n",
      "\u001b[34mPYTHONIOENCODING=UTF-8\u001b[0m\n",
      "\u001b[34mPYSPARK_PYTHON=/usr/bin/python3\u001b[0m\n",
      "\u001b[34mSPARK_NO_DAEMONIZE=TRUE\u001b[0m\n",
      "\u001b[34mYARN_NODEMANAGER_USER=root\u001b[0m\n",
      "\u001b[34mHDFS_NAMENODE_USER=root\u001b[0m\n",
      "\u001b[34mPYTHONHASHSEED=0\u001b[0m\n",
      "\u001b[34mPATH=/usr/bin:/opt/program:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u001b[0m\n",
      "\u001b[34mHDFS_SECONDARYNAMENODE_USER=root\u001b[0m\n",
      "\u001b[34mPWD=/home/sagemaker-user\u001b[0m\n",
      "\u001b[34mLANG=en_US.UTF-8\u001b[0m\n",
      "\u001b[34mHADOOP_CONF_DIR=/usr/lib/hadoop/etc/hadoop\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG_FILE=/opt/ml/config/resourceconfig.json\u001b[0m\n",
      "\u001b[34mAWS_REGION=us-east-1\u001b[0m\n",
      "\u001b[34mAWS_METADATA_SERVICE_TIMEOUT=3\u001b[0m\n",
      "\u001b[34mPYTHONDONTWRITEBYTECODE=1\u001b[0m\n",
      "\u001b[34mHDFS_DATANODE_USER=root\u001b[0m\n",
      "\u001b[34mSHLVL=1\u001b[0m\n",
      "\u001b[34mAWS_METADATA_SERVICE_NUM_ATTEMPTS=3\u001b[0m\n",
      "\u001b[34mHOME=/home/sagemaker-user\u001b[0m\n",
      "\u001b[34mLANGUAGE=en_US.UTF-8\u001b[0m\n",
      "\u001b[34mSM_PROCESSING_CONFIG_FILE=/opt/ml/config/processingjobconfig.json\u001b[0m\n",
      "\u001b[34mPIP_DISABLE_PIP_VERSION_CHECK=1\u001b[0m\n",
      "\u001b[34mHDFS_PORT=8020\u001b[0m\n",
      "\u001b[34mNB_GID=100\u001b[0m\n",
      "\u001b[34mNB_UID=1000\u001b[0m\n",
      "\u001b[34m_=/usr/bin/printenv\u001b[0m\n",
      "\u001b[34mStarting DataPrep Container in Processing Job Mode\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '/entrypoint/processing_entrypoint.py', '--output-config', '{\"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\": {\"content_type\": \"CSV\"}}']\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark.cli  INFO     App and app arguments: ['/entrypoint/processing_entrypoint.py', '--output-config', '{\"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\": {\"content_type\": \"CSV\"}}']\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:867352166187:processing-job/credit-flow-2021-03-25-08-14-10', 'ProcessingJobName': 'credit-flow-2021-03-25-08-14-10', 'AppSpecification': {'ImageUri': '663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x', 'ContainerEntrypoint': None, 'ContainerArguments': ['--output-config \\'{\"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\": {\"content_type\": \"CSV\"}}\\'']}, 'ProcessingInputs': [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/flow', 'S3Uri': 's3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/flow/credit-data.flow', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}, {'InputName': 'german.csv', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/german.csv', 'S3Uri': 's3://creditmodel-yudhiesh-mlrawdata-867352166187-us-east-1/german.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::867352166187:role/DataScience/SC-867352166187-pp-7pfghfxa-SageMakerExecutionRole-P4ECNYCU9XXE', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /entrypoint/processing_entrypoint.py --output-config '{\"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\": {\"content_type\": \"CSV\"}}'\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     Detected instance type: m5.4xlarge with total memory: 65536M and total cores: 16\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.226.231</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      "\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\n",
      "\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.226.231\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 55742m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 5574m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 16\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=12 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 1\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 32\n",
      "\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m03-25 08:18 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,233 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.226.231\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-s\u001b[0m\n",
      "\u001b[34merver-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,240 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,298 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-d5a0d217-e94e-423a-8168-84ff5692f2cd\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,639 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,651 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,652 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,652 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,657 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,657 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,657 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,657 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,699 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,711 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,711 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,714 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,715 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 25 08:18:21\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,716 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,716 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,717 INFO util.GSet: 2.0% max memory 13.7 GB = 280.3 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,717 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,798 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,798 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,804 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,804 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,804 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,804 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,805 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,826 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,826 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,826 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,826 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,838 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,838 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,838 INFO util.GSet: 1.0% max memory 13.7 GB = 140.1 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,838 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,876 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,876 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,876 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,876 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,881 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,882 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,886 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,886 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,887 INFO util.GSet: 0.25% max memory 13.7 GB = 35.0 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,887 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,903 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,903 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,903 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,906 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,906 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,908 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,908 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,908 INFO util.GSet: 0.029999999329447746% max memory 13.7 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,908 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,936 INFO namenode.FSImage: Allocated new BlockPoolId: BP-519406275-10.0.226.231-1616660301929\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,951 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:21,976 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,076 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,084 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,218 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,218 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.226.231\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,781 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.226.231\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-s\u001b[0m\n",
      "\u001b[34merver-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,791 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.226.231\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.12021-03-25 08:18:22,792 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m9.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,799 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,820 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.226.231\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-s\u001b[0m\n",
      "\u001b[34merver-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,827 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,827 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.226.231\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-s\u001b[0m\n",
      "\u001b[34merver-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,834 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:22,903 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,043 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,128 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,133 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,133 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,155 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://10.0.226.231/\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,179 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,179 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,205 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,212 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,212 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,226 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,240 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,271 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,275 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,279 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,286 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,287 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,289 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,311 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,321 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,322 INFO util.log: Logging initialized @963ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,325 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,325 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,333 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,335 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,336 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,336 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,337 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,337 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,338 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,340 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,353 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,353 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,357 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,358 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,361 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,362 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,363 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,365 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,403 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,419 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,426 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,430 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,436 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,438 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,438 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,438 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,468 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,468 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,469 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,469 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,477 INFO http.HttpServer2: Jetty bound to port 9870\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,478 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,486 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,494 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,499 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,499 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,499 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,502 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,502 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,502 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,503 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,505 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,506 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,509 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,513 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@305ffe9e{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,514 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@35841320{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,514 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,518 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,525 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@15043a2f\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,526 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,527 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,528 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,529 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,529 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,529 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,529 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,529 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,531 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,531 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,536 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,538 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,545 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,546 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,551 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,554 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@451001e5\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,554 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,555 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,555 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,555 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,555 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,555 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,558 WARN monitor.ContainersMonitorImpl: NodeManager configured with 62.1 G physical memory allocated to containers, which is more than 80% of the total physical memory available (62.1 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,558 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,558 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,558 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,561 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,568 INFO util.log: Logging initialized @1211ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,572 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@39d76cb5{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,577 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,577 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,580 INFO server.AbstractConnector: Started ServerConnector@5884a914{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,580 INFO server.Server: Started @1221ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,582 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,582 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,584 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,591 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,591 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,601 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,601 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,608 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=63569 virtual-memory=317845 virtual-cores=16\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,616 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,616 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,620 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34meffectiveMinResource=<memory:0, vCores:0>\n",
      " , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99998426 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:63569, vCores:16> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,620 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,620 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,622 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,622 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,624 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,624 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:63569, vCores:16>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,627 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,629 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,629 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,630 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,636 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,659 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,677 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,679 INFO util.log: Logging initialized @1317ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,720 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,726 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,731 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,732 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,732 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,732 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,754 INFO http.HttpServer2: Jetty bound to port 36157\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,755 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,769 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,773 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,777 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,777 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,779 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,779 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,783 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,783 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,783 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,783 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,783 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,783 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,785 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,785 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,785 INFO http.HttpServer2: adding path spec: /app/*\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,789 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3fce8fd9{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,790 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,790 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,790 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1df8b5b8{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,831 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,836 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,841 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,842 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,842 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,845 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7d446ed1{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,847 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,847 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,847 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,847 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,852 INFO server.AbstractConnector: Started ServerConnector@27e47833{HTTP/1.1,[http/1.1]}{localhost:36157}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,852 INFO server.Server: Started @1495ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,880 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,889 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,889 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,890 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,890 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,890 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,892 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,892 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 25 08:18:23\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,894 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,894 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,895 INFO util.GSet: 2.0% max memory 13.7 GB = 280.3 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,895 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,910 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:33233\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,915 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,916 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,919 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,919 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,919 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,923 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,927 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.226.231:33233\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,927 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.226.231:0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,927 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,930 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,945 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,945 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO util.log: Logging initialized @1590ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,952 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,953 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,953 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,974 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,974 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,974 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,974 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,976 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,982 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,983 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,983 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,986 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,986 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,987 INFO util.GSet: 1.0% max memory 13.7 GB = 140.1 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:23,987 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,021 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,028 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,028 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,028 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,028 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,031 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,032 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,034 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,035 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,036 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,040 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO util.GSet: 0.25% max memory 13.7 GB = 35.0 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,041 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,043 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,043 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,252 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,283 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,291 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,292 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,299 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,315 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,332 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,332 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,334 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,334 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.226.231:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,348 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,352 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,355 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,359 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,360 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,360 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,379 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,380 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14f9390f{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,391 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,448 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,451 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,452 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,461 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,461 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,461 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,463 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,463 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,465 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,465 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,465 INFO util.GSet: 0.029999999329447746% max memory 13.7 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,465 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,481 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 138@algo-1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,481 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,482 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,483 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,492 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,494 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c2db68f{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,495 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3668d4{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,500 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,501 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,501 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,504 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,508 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,552 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,577 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,577 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,582 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,583 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,586 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,686 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,686 INFO namenode.FSNamesystem: Finished loading FSImage in 218 msecs\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,832 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,862 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:24,871 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,045 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,047 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,055 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,083 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,084 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,084 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,085 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,113 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,118 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,118 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,118 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,118 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,118 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,118 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 34 msec\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,125 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,189 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.226.231:8020\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,198 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,199 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,219 INFO namenode.FSDirectory: Quota initialization completed in 20 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,255 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@164a62bf{node,/,file:///tmp/jetty-algo-1-8042-_-any-8871363901689733340.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,273 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,285 INFO server.AbstractConnector: Started ServerConnector@2374d36a{HTTP/1.1,[http/1.1]}{algo-1:8042}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,285 INFO server.Server: Started @2923ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,285 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,287 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:33233\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,295 INFO client.RMProxy: Connecting to ResourceManager at /10.0.226.231:8031\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,301 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,354 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,367 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34mMar 25, 2021 8:18:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,430 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@396e6d9{cluster,/,file:///tmp/jetty-10_0_226_231-8088-_-any-4502090225093088812.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,439 INFO ipc.Client: Retrying connect to server: algo-1/10.0.226.231:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,497 INFO server.AbstractConnector: Started ServerConnector@432038ec{HTTP/1.1,[http/1.1]}{10.0.226.231:8088}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,497 INFO server.Server: Started @3136ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,501 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,571 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,581 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,637 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.226.231:8020\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,639 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,646 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 139@algo-1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,647 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 515505893. Formatting...\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,648 INFO common.Storage: Generated new storageID DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46 for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m03-25 08:18 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1']\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,669 INFO common.Storage: Analyzing storage directories for bpid BP-519406275-10.0.226.231-1616660301929\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,669 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-519406275-10.0.226.231-1616660301929\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,669 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-519406275-10.0.226.231-1616660301929 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,670 INFO common.Storage: Formatting block pool BP-519406275-10.0.226.231-1616660301929 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-519406275-10.0.226.231-1616660301929/current\u001b[0m\n",
      "\u001b[34m03-25 08:18 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-03-25T08:18:25.672335'))])\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,674 INFO datanode.DataNode: Setting up storage: nsid=515505893;bpid=BP-519406275-10.0.226.231-1616660301929;lv=-57;nsInfo=lv=-65;cid=CID-d5a0d217-e94e-423a-8168-84ff5692f2cd;nsid=515505893;c=1616660301929;bpid=BP-519406275-10.0.226.231-1616660301929;dnuuid=null\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,679 INFO datanode.DataNode: Generated and persisted new Datanode UUID c4defa83-c0b2-4e06-9945-0588e61718cb\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,704 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,704 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,704 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,705 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,718 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,719 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,720 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,720 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,737 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,753 INFO impl.FsDatasetImpl: Added new volume: DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,753 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,756 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,762 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,772 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,774 INFO impl.FsDatasetImpl: Adding block pool BP-519406275-10.0.226.231-1616660301929\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,774 INFO impl.FsDatasetImpl: Scanning block pool BP-519406275-10.0.226.231-1616660301929 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,807 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-519406275-10.0.226.231-1616660301929 on /opt/amazon/hadoop/hdfs/datanode: 33ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,808 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-519406275-10.0.226.231-1616660301929: 34ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,809 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-519406275-10.0.226.231-1616660301929 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,809 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-519406275-10.0.226.231-1616660301929/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,812 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-519406275-10.0.226.231-1616660301929 on volume /opt/amazon/hadoop/hdfs/datanode: 3ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,812 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-519406275-10.0.226.231-1616660301929: 3ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,813 INFO datanode.VolumeScanner: Now scanning bpid BP-519406275-10.0.226.231-1616660301929 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,815 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46): finished scanning block pool BP-519406275-10.0.226.231-1616660301929\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,826 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/25/21 12:04 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,827 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46): no suitable block pools found to scan.  Waiting 1814399986 ms.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,832 INFO datanode.DataNode: Block pool BP-519406275-10.0.226.231-1616660301929 (Datanode Uuid c4defa83-c0b2-4e06-9945-0588e61718cb) service to algo-1/10.0.226.231:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,869 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.226.231:9866, datanodeUuid=c4defa83-c0b2-4e06-9945-0588e61718cb, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-d5a0d217-e94e-423a-8168-84ff5692f2cd;nsid=515505893;c=1616660301929) storage c4defa83-c0b2-4e06-9945-0588e61718cb\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,871 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.226.231:9866\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,871 INFO blockmanagement.BlockReportLeaseManager: Registered DN c4defa83-c0b2-4e06-9945-0588e61718cb (10.0.226.231:9866).\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,871 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,883 INFO datanode.DataNode: Block pool Block pool BP-519406275-10.0.226.231-1616660301929 (Datanode Uuid c4defa83-c0b2-4e06-9945-0588e61718cb) service to algo-1/10.0.226.231:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,883 INFO datanode.DataNode: For namenode algo-1/10.0.226.231:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,884 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,884 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,895 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,896 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,908 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,908 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,910 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,912 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,912 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,922 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,931 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,935 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,937 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46 for DN 10.0.226.231:9866\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,941 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,942 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:25,942 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,013 INFO BlockStateChange: BLOCK* processReport 0xf4a60a09d74a1b67: Processing first storage report for DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46 from datanode c4defa83-c0b2-4e06-9945-0588e61718cb\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,015 INFO BlockStateChange: BLOCK* processReport 0xf4a60a09d74a1b67: from storage DS-8cc7e09c-7c09-4eef-9c71-fea0bcb86e46 node DatanodeRegistration(10.0.226.231:9866, datanodeUuid=c4defa83-c0b2-4e06-9945-0588e61718cb, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-d5a0d217-e94e-423a-8168-84ff5692f2cd;nsid=515505893;c=1616660301929), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,030 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,031 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,033 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,034 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,034 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,053 INFO datanode.DataNode: Successfully sent block report 0xf4a60a09d74a1b67,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 95 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,054 INFO datanode.DataNode: Got finalize command for block pool BP-519406275-10.0.226.231-1616660301929\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,073 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,535 INFO ipc.Client: Retrying connect to server: algo-1/10.0.226.231:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,689 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 33233 httpPort: 8042) registered with capability: <memory:63569, vCores:16>, assigned nodeId algo-1:33233\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,692 INFO rmnode.RMNodeImpl: algo-1:33233 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,704 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 460953702\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,705 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -1832780423\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,705 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:33233 with total resource of <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:26,711 INFO capacity.CapacityScheduler: Added node algo-1:33233 clusterResource: <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,056 INFO spark.SparkContext: Running Spark version 3.0.0-amzn-0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,100 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,101 INFO resource.ResourceUtils: Resources for spark.driver:\n",
      "\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,102 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,102 INFO spark.SparkContext: Submitted application: processing_entrypoint.py\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,162 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,162 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,162 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,162 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,162 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,390 INFO util.Utils: Successfully started service 'sparkDriver' on port 40703.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,418 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,451 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,477 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,478 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,513 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,527 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-d77e9bf9-7847-4fde-bfca-ea9aab2a6989\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,550 INFO memory.MemoryStore: MemoryStore started with capacity 1007.8 MiB\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,589 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,696 INFO util.log: Logging initialized @3710ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,780 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,801 INFO server.Server: Started @3815ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,831 INFO server.AbstractConnector: Started ServerConnector@a6dda1c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,831 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,868 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ba5744b{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,871 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fbee4f8{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,871 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b611d09{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,872 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45709690{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,873 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@650f9612{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,873 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@121b818f{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,874 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f40fd24{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,876 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c492c3c{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,877 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dbca28e{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,877 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e7dac1b{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,878 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c46008e{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,878 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@561d8565{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,879 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d700fed{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,880 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d20bd42{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,880 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50f5611b{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,881 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e1e2b06{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,881 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@109b6f41{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,882 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b2b8f63{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,882 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@122a96a6{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,883 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@276ad4da{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,891 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38b720d1{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,892 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@305fc3dd{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,893 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ec90f55{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,893 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d75ba4b{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,894 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a8b862d{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:29,895 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.226.231:4040\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,150 INFO client.RMProxy: Connecting to ResourceManager at /10.0.226.231:8032\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,414 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,433 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,576 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,576 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,598 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63569 MB per container)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,598 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,598 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,600 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,607 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:30,626 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:33,934 INFO yarn.Client: Uploading resource file:/tmp/spark-ea931401-eb0b-406b-a889-38e05e429399/__spark_libs__5420084128314386305.zip -> file:/root/.sparkStaging/application_1616660305706_0001/__spark_libs__5420084128314386305.zip\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,868 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> file:/root/.sparkStaging/application_1616660305706_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,872 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> file:/root/.sparkStaging/application_1616660305706_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,968 INFO yarn.Client: Uploading resource file:/tmp/spark-ea931401-eb0b-406b-a889-38e05e429399/__spark_conf__4116727664652423845.zip -> file:/root/.sparkStaging/application_1616660305706_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,985 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,985 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,985 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,986 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:35,986 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,011 INFO yarn.Client: Submitting application application_1616660305706_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,096 INFO capacity.CapacityScheduler: Application 'application_1616660305706_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,096 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,113 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,115 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,115 INFO rmapp.RMAppImpl: Storing application with id application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,117 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.226.231#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,123 INFO recovery.RMStateStore: Storing info for app: application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,123 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,124 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,125 INFO capacity.ParentQueue: Application added - appId: application_1616660305706_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,126 INFO capacity.CapacityScheduler: Accepted application application_1616660305706_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,138 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,164 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,165 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,214 INFO capacity.LeafQueue: Application application_1616660305706_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,215 INFO capacity.LeafQueue: Application added - appId: application_1616660305706_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,215 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1616660305706_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,222 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,243 INFO impl.YarnClientImpl: Submitted application application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,800 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1616660305706_0001_000001 container=null queue=default clusterResource=<memory:63569, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,804 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,805 INFO fica.FiCaSchedulerNode: Assigned container container_1616660305706_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:33233, which has 1 containers, <memory:896, vCores:1> used and <memory:62673, vCores:15> available after allocation\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,805 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,822 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:33233 for container : container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,829 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,830 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,830 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.014094921 absoluteUsedCapacity=0.014094921 used=<memory:896, vCores:1> cluster=<memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,830 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1616660305706_0001 AttemptId: appattempt_1616660305706_0001_000001 MasterContainer: Container: [ContainerId: container_1616660305706_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:33233, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.226.231:33233 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,830 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,841 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,845 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,858 INFO amlauncher.AMLauncher: Launching masterappattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,912 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1616660305706_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:33233, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.226.231:33233 }, ExecutionType: GUARANTEED, ] for AM appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,913 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:36,916 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,023 INFO ipc.Server: Auth successful for appattempt_1616660305706_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,101 INFO containermanager.ContainerManagerImpl: Start request for container_1616660305706_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,145 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,152 INFO application.ApplicationImpl: Application application_1616660305706_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,153 INFO application.ApplicationImpl: Adding container_1616660305706_0001_01_000001 to application application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,153 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.226.231#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,156 INFO application.ApplicationImpl: Application application_1616660305706_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,159 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,159 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,165 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1616660305706_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:33233, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.226.231:33233 }, ExecutionType: GUARANTEED, ] for AM appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,166 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,166 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1616660305706_0001, attemptId: appattempt_1616660305706_0001_000001launchTime: 1616660317165\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,166 INFO recovery.RMStateStore: Updating info for app: application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,168 INFO localizer.ResourceLocalizationService: Created localizer for container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,227 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1616660305706_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,241 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,246 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1616660305706_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,246 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,250 INFO yarn.Client: Application report for application_1616660305706_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,252 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1616660316113\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1616660305706_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:37,795 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:38,255 INFO yarn.Client: Application report for application_1616660305706_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:39,258 INFO yarn.Client: Application report for application_1616660305706_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:39,673 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:39,674 INFO scheduler.ContainerScheduler: Starting container [container_1616660305706_0001_01_000001]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:39,702 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:39,702 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:39,706 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:40,260 INFO yarn.Client: Application report for application_1616660305706_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,264 INFO yarn.Client: Application report for application_1616660305706_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,916 INFO ipc.Server: Auth successful for appattempt_1616660305706_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,931 INFO monitor.ContainersMonitorImpl: container_1616660305706_0001_01_000001's ip = 10.0.226.231, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,937 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1616660305706_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,939 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,939 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.226.231#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011APPATTEMPTID=appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,940 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:41,940 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,178 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1616660305706_0001), /proxy/application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,267 INFO yarn.Client: Application report for application_1616660305706_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,267 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.226.231\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1616660316113\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1616660305706_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,269 INFO cluster.YarnClientSchedulerBackend: Application application_1616660305706_0001 has started running.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,278 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41985.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,278 INFO netty.NettyBlockTransferService: Server created on 10.0.226.231:41985\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,280 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,291 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.226.231, 41985, None)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,294 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.226.231:41985 with 1007.8 MiB RAM, BlockManagerId(driver, 10.0.226.231, 41985, None)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,298 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.226.231, 41985, None)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,298 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.226.231, 41985, None)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,477 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,479 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae2cdbb{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,510 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,542 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34mINFO:root:Running with these Spark Configs:\u001b[0m\n",
      "\u001b[34mINFO:root:[('spark.driver.extraLibraryPath',\n",
      "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native'),\n",
      " ('spark.executor.memoryOverhead', '5574m'),\n",
      " ('spark.executor.memory', '55742m'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.default.parallelism', '32'),\n",
      " ('spark.executor.cores', '16'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.ui.filters',\n",
      "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
      " ('spark.driver.host', '10.0.226.231'),\n",
      " ('spark.driver.extraClassPath',\n",
      "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
      " ('spark.driver.appUIAddress', 'http://10.0.226.231:4040'),\n",
      " ('spark.driver.memoryOverhead', '204m'),\n",
      " ('spark.app.id', 'application_1616660305706_0001'),\n",
      " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
      "  'http://algo-1:8088/proxy/application_1616660305706_0001'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.executor.extraClassPath',\n",
      "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
      " ('spark.ui.proxyBase', '/proxy/application_1616660305706_0001'),\n",
      " ('spark.executorEnv.PYTHONPATH',\n",
      "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
      " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n",
      " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
      "  'algo-1'),\n",
      " ('spark.driver.defaultJavaOptions',\n",
      "  \"-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC \"\n",
      "  '-XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 '\n",
      "  '-XX:+CMSClassUnloadingEnabled'),\n",
      " ('spark.master', 'yarn'),\n",
      " ('spark.driver.memory', '2048m'),\n",
      " ('spark.driver.port', '40703'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.executor.extraLibraryPath',\n",
      "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native'),\n",
      " ('spark.executor.instances', '1'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.yarn.isPython', 'true'),\n",
      " ('spark.executor.defaultJavaOptions',\n",
      "  \"-verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails \"\n",
      "  '-XX:+PrintGCDateStamps -XX:+UseParallelGC '\n",
      "  '-XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=4 '\n",
      "  '-XX:ParallelGCThreads=12'),\n",
      " ('spark.app.name', 'processing_entrypoint.py')]\u001b[0m\n",
      "\u001b[34mINFO:root:Output Config: {\n",
      "    \"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\": {\n",
      "        \"content_type\": \"CSV\"\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mINFO:root:No `default` configuration provided. Content type will default to CSV for all outputs not specified in the output config.\u001b[0m\n",
      "\u001b[34mINFO:entrypoint:Processing Job Config: {\n",
      "    \"ProcessingJobArn\": \"arn:aws:sagemaker:us-east-1:867352166187:processing-job/credit-flow-2021-03-25-08-14-10\",\n",
      "    \"ProcessingJobName\": \"credit-flow-2021-03-25-08-14-10\",\n",
      "    \"AppSpecification\": {\n",
      "        \"ImageUri\": \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\",\n",
      "        \"ContainerEntrypoint\": null,\n",
      "        \"ContainerArguments\": [\n",
      "            \"--output-config '{\\\"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\\\": {\\\"content_type\\\": \\\"CSV\\\"}}'\"\n",
      "        ]\n",
      "    },\n",
      "    \"ProcessingInputs\": [\n",
      "        {\n",
      "            \"InputName\": \"flow\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "                \"LocalPath\": \"/opt/ml/processing/flow\",\n",
      "                \"S3Uri\": \"s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/flow/credit-data.flow\",\n",
      "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3InputMode\": \"File\",\n",
      "                \"S3CompressionType\": \"None\",\n",
      "                \"S3DownloadMode\": \"StartOfJob\"\n",
      "            },\n",
      "            \"DatasetDefinition\": null\n",
      "        },\n",
      "        {\n",
      "            \"InputName\": \"german.csv\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "                \"LocalPath\": \"/opt/ml/processing/german.csv\",\n",
      "                \"S3Uri\": \"s3://creditmodel-yudhiesh-mlrawdata-867352166187-us-east-1/german.csv\",\n",
      "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3InputMode\": \"File\",\n",
      "                \"S3CompressionType\": \"None\",\n",
      "                \"S3DownloadMode\": \"StartOfJob\"\n",
      "            },\n",
      "            \"DatasetDefinition\": null\n",
      "        }\n",
      "    ],\n",
      "    \"ProcessingOutputConfig\": {\n",
      "        \"Outputs\": [\n",
      "            {\n",
      "                \"OutputName\": \"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\",\n",
      "                \"AppManaged\": false,\n",
      "                \"S3Output\": {\n",
      "                    \"LocalPath\": \"/opt/ml/processing/output\",\n",
      "                    \"S3Uri\": \"s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler\",\n",
      "                    \"S3UploadMode\": \"EndOfJob\"\n",
      "                },\n",
      "                \"FeatureStoreOutput\": null\n",
      "            }\n",
      "        ],\n",
      "        \"KmsKeyId\": null\n",
      "    },\n",
      "    \"ProcessingResources\": {\n",
      "        \"ClusterConfig\": {\n",
      "            \"InstanceCount\": 1,\n",
      "            \"InstanceType\": \"ml.m5.4xlarge\",\n",
      "            \"VolumeSizeInGB\": 30,\n",
      "            \"VolumeKmsKeyId\": null\n",
      "        }\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::867352166187:role/DataScience/SC-867352166187-pp-7pfghfxa-SageMakerExecutionRole-P4ECNYCU9XXE\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:40,796 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:40,798 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:40,799 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,186 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,187 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,187 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,188 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,188 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,311 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,488 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,746 INFO client.RMProxy: Connecting to ResourceManager at /10.0.226.231:8030\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:41,808 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,067 INFO client.TransportClientFactory: Successfully created connection to /10.0.226.231:40703 after 73 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,191 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,409 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] Default YARN executor launch context:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> file:/root/.sparkStaging/application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       -Xmx55742m \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:ConcGCThreads=4' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-XX:ParallelGCThreads=12' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       '-Dspark.driver.port=40703' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.226.231:40703 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       16 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       application_1616660305706_0001 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --resourceProfileId \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       0 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1616660305706_0001/pyspark.zip\" } size: 732492 timestamp: 1616660315000 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1616660305706_0001/__spark_libs__5420084128314386305.zip\" } size: 398449508 timestamp: 1616660314000 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1616660305706_0001/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1616660315000 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1616660305706_0001/__spark_conf__.zip\" } size: 262561 timestamp: 1616660315000 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,485 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,486 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,519 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 16 core(s) and 61316 MB memory (including 5574 MB of overhead)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000001/stderr] 2021-03-25 08:18:42,548 INFO yarn.YarnAllocator: Submitted 1 unloWARNING:root:Skipping reading sagemaker.read_csv as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.join_tables as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.concatenate_datasets as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.cast_type as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.infer_and_cast_type as it's part of whitelist\u001b[0m\n",
      "\u001b[34mINFO:root:Resolving logical graph\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,807 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1616660305706_0001_000001 container=null queue=default clusterResource=<memory:63569, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,807 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,807 INFO fica.FiCaSchedulerNode: Assigned container container_1616660305706_0001_01_000002 of capacity <memory:61316, vCores:1> on host algo-1:33233, which has 2 containers, <memory:62212, vCores:2> used and <memory:1357, vCores:14> available after allocation\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,808 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000002#011RESOURCE=<memory:61316, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,808 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.97865313 absoluteUsedCapacity=0.97865313 used=<memory:62212, vCores:2> cluster=<memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,808 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:42,817 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,007 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:33233 for container : container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,008 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,141 INFO ipc.Server: Auth successful for appattempt_1616660305706_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,144 INFO containermanager.ContainerManagerImpl: Start request for container_1616660305706_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,146 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.226.231#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,146 INFO application.ApplicationImpl: Adding container_1616660305706_0001_01_000002 to application application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,147 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,147 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,148 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,148 INFO scheduler.ContainerScheduler: Starting container [container_1616660305706_0001_01_000002]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,162 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,162 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,165 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:43,809 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917962  224 -r-x------   1 root     root       227528 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-cloudhsm-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917963  952 -r-x------   1 root     root       971509 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-rekognition-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917964   24 -r-x------   1 root     root        24085 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-workmailmessageflow-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917965  180 -r-x------   1 root     root       182585 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-autoscalingplans-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917966  392 -r-x------   1 root     root       398329 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-elastictranscoder-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917700 2032 -r-x------   1 root     root      2079715 Mar 25 08:18 ./__spark_libs__/spark-hive-thriftserver_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917701  552 -r-x------   1 root     root       561459 Mar 25 08:18 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917702  280 -r-x------   1 root     root       283653 Mar 25 08:18 ./__spark_libs__/curator-recipes-2.13.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917703  764 -r-x------   1 root     root       780664 Mar 25 08:18 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917704  136 -r-x------   1 root     root       136363 Mar 25 08:18 ./__spark_libs__/HikariCP-2.5.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917967  104 -r-x------   1 root     root       105715 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-importexport-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917705 4064 -r-x------   1 root     root      4158923 Mar 25 08:18 ./__spark_libs__/hadoop-common-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917968  804 -r-x------   1 root     root       819646 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-directory-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917706  388 -r-x------   1 root     root       395195 Mar 25 08:18 ./__spark_libs__/javolution-5.5.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917969  552 -r-x------   1 root     root       563962 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-logs-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917707   60 -r-x------   1 root     root        58684 Mar 25 08:18 ./__spark_libs__/chill-java-0.9.5.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917708   56 -r-x------   1 root     root        54509 Mar 25 08:18 ./__spark_libs__/native_system-java-1.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917970  700 -r-x------   1 root     root       714959 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-appstream-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917709   56 -r-x------   1 root     root        55236 Mar 25 08:18 ./__spark_libs__/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917971  248 -r-x------   1 root     root       252694 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-serverlessapplicationrepository-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917972 1824 -r-x------   1 root     root      1866513 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-lightsail-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917710 7384 -r-x------   1 root     root      7558789 Mar 25 08:18 ./__spark_libs__/spark-sql_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917711  228 -r-x------   1 root     root       232248 Mar 25 08:18 ./__spark_libs__/jackson-core-asl-1.9.13.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917973  300 -r-x------   1 root     root       306575 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-acmpca-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917712 1144 -r-x------   1 root     root      1168113 Mar 25 08:18 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917974  268 -r-x------   1 root     root       272292 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-secretsmanager-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917713  132 -r-x------   1 root     root       134595 Mar 25 08:18 ./__spark_libs__/commons-crypto-1.0.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917714   80 -r-x------   1 root     root        80980 Mar 25 08:18 ./__spark_libs__/kerb-admin-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917975  236 -r-x------   1 root     root       238845 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-mediastore-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917715   28 -r-x------   1 root     root        27749 Mar 25 08:18 ./__spark_libs__/orc-shims-1.5.10.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917716   64 -r-x------   1 root     root        65464 Mar 25 08:18 ./__spark_libs__/kerb-common-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917976  776 -r-x------   1 root     root       791754 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-ses-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917977   56 -r-x------   1 root     root        53928 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-kinesisvideosignalingchannels-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917978  580 -r-x------   1 root     root       592481 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-kms-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917979  832 -r-x------   1 root     root       850563 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-comprehend-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917980  156 -r-x------   1 root     root       156858 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-detective-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917717  196 -r-x------   1 root     root       200223 Mar 25 08:18 ./__spark_libs__/hk2-api-2.6.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917718 1988 -r-x------   1 root     root      2035066 Mar 25 08:18 ./__spark_libs__/commons-math3-3.4.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917981  668 -r-x------   1 root     root       680714 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-iotanalytics-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917982  512 -r-x------   1 root     root       522970 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-docdb-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917719 1364 -r-x------   1 root     root      1393617 Mar 25 08:18 ./__spark_libs__/hadoop-yarn-server-common-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917983  176 -r-x------   1 root     root       177649 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-iot1clickdevices-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917984  720 -r-x------   1 root     root       736882 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-wafv2-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917985   76 -r-x------   1 root     root        74675 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-ssooidc-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917720   40 -r-x------   1 root     root        38370 Mar 25 08:18 ./__spark_libs__/hive-vector-code-gen-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917986  288 -r-x------   1 root     root       293017 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-accessanalyzer-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917987  104 -r-x------   1 root     root       103083 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-outposts-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917721  576 -r-x------   1 root     root       588337 Mar 25 08:18 ./__spark_libs__/commons-collections-3.2.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917722  656 -r-x------   1 root     root       668235 Mar 25 08:18 ./__spark_libs__/guice-4.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917723 2252 -r-x------   1 root     root      2305169 Mar 25 08:18 ./__spark_libs__/netlib-native_ref-win-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917724   16 -r-x------   1 root     root        15529 Mar 25 08:18 ./__spark_libs__/jackson-jaxrs-json-provider-2.10.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917725 1112 -r-x------   1 root     root      1137921 Mar 25 08:18 ./__spark_libs__/spark-streaming_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917726  232 -r-x------   1 root     root       234942 Mar 25 08:18 ./__spark_libs__/hive-storage-api-2.7.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917727 1712 -r-x------   1 root     root      1749371 Mar 25 08:18 ./__spark_libs__/netlib-native_ref-linux-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917988  148 -r-x------   1 root     root       150318 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-simpledb-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917989   44 -r-x------   1 root     root        44094 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-marketplacecommerceanalytics-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917728   20 -r-x------   1 root     root        18763 Mar 25 08:18 ./__spark_libs__/token-provider-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917729  992 -r-x------   1 root     root      1013367 Mar 25 08:18 ./__spark_libs__/jaxb-runtime-2.3.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917990  404 -r-x------   1 root     root       411539 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-servicediscovery-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917730  244 -r-x------   1 root     root       246445 Mar 25 08:18 ./__spark_libs__/libthrift-0.12.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917991  812 -r-x------   1 root     root       829863 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-lambda-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917731  420 -r-x------   1 root     root       427780 Mar 25 08:18 ./__spark_libs__/jodd-core-3.5.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917992  448 -r-x------   1 root     root       455261 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-cloudsearch-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917732   20 -r-x------   1 root     root        16642 Mar 25 08:18 ./__spark_libs__/metrics-json-4.1.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917993  216 -r-x------   1 root     root       218500 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-codegurureviewer-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917994  936 -r-x------   1 root     root       954384 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-appmesh-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917733  320 -r-x------   1 root     root       326874 Mar 25 08:18 ./__spark_libs__/httpcore-4.4.11.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917995  132 -r-x------   1 root     root       133434 Mar 25 08:18 ./__spark_libs__/aws-glue-datacatalog-spark-client-3.0.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917734 3152 -r-x------   1 root     root      3224708 Mar 25 08:18 ./__spark_libs__/derby-10.12.1.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917996  440 -r-x------   1 root     root       447924 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-groundstation-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917997  352 -r-x------   1 root     root       359656 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-applicationinsights-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917998  728 -r-x------   1 root     root       741552 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-elasticsearch-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917735   20 -r-x------   1 root     root        19827 Mar 25 08:18 ./__spark_libs__/opencsv-2.3.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917736  544 -r-x------   1 root     root       556575 Mar 25 08:18 ./__spark_libs__/scala-xml_2.12-1.2.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917737   28 -r-x------   1 root     root        26514 Mar 25 08:18 ./__spark_libs__/stax-api-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917738   80 -r-x------   1 root     root        79845 Mar 25 08:18 ./__spark_libs__/compress-lzf-1.0.3.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917739 1180 -r-x------   1 root     root      1208080 Mar 25 08:18 ./__spark_libs__/netlib-native_ref-linux-armhf-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917999  492 -r-x------   1 root     root       501702 Mar 25 08:18 ./__spark_libs__/hadoop-aws.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917740   36 -r-x------   1 root     root        33786 Mar 25 08:18 ./__spark_libs__/machinist_2.12-0.6.8.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917741   12 -r-x------   1 root     root         9498 Mar 25 08:18 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0-tests.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917742   64 -r-x------   1 root     root        62050 Mar 25 08:18 ./__spark_libs__/commons-logging-1.1.3.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918000  492 -r-x------   1 root     root       501832 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-amplify-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917743  164 -r-x------   1 root     root       164422 Mar 25 08:18 ./__spark_libs__/core-1.1.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917744   12 -r-x------   1 root     root         8685 Mar 25 08:18 ./__spark_libs__/jniloader-1.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917745  200 -r-x------   1 root     root       204650 Mar 25 08:18 ./__spark_libs__/kerby-pkix-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917746  188 -r-x------   1 root     root       190432 Mar 25 08:18 ./__spark_libs__/gson-2.2.4.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917747    8 -r-x------   1 root     root         5711 Mar 25 08:18 ./__spark_libs__/minlog-1.3.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917748   32 -r-x------   1 root     root        30370 Mar 25 08:18 ./__spark_libs__/spark-sketch_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918001 1028 -r-x------   1 root     root      1051487 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-codedeploy-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918002 1468 -r-x------   1 root     root      1501270 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-mediaconvert-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917749   28 -r-x------   1 root     root        28355 Mar 25 08:18 ./__spark_libs__/spark-ganglia-lgpl_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918003  260 -r-x------   1 root     root       262707 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-support-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917750  100 -r-x------   1 root     root        99555 Mar 25 08:18 ./__spark_libs__/xz-1.5.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917751  128 -r-x------   1 root     root       127223 Mar 25 08:18 ./__spark_libs__/remotetea-oncrpc-1.1.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917752   76 -r-x------   1 root     root        76520 Mar 25 08:18 ./__spark_libs__/spark-launcher_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917753   12 -r-x------   1 root     root        12131 Mar 25 08:18 ./__spark_libs__/jpam-1.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918004  408 -r-x------   1 root     root       416480 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-cloudwatch-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917754  280 -r-x------   1 root     root       284184 Mar 25 08:18 ./__spark_libs__/commons-codec-1.10.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917755  132 -r-x------   1 root     root       131590 Mar 25 08:18 ./__spark_libs__/hk2-utils-2.6.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918005  868 -r-x------   1 root     root       887584 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-iotsitewise-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918006   80 -r-x------   1 root     root        80424 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-mediastoredata-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917756   92 -r-x------   1 root     root        93210 Mar 25 08:18 ./__spark_libs__/super-csv-2.2.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917757    8 -r-x------   1 root     root         4722 Mar 25 08:18 ./__spark_libs__/jcip-annotations-1.0-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918007 1808 -r-x------   1 root     root      1848502 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-waf-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917758   84 -r-x------   1 root     root        85815 Mar 25 08:18 ./__spark_libs__/jersey-media-jaxb-2.30.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918008   96 -r-x------   1 root     root        96881 Mar 25 08:18 ./__spark_libs__/aws-glue-datacatalog-client-common-3.0.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917759  336 -r-x------   1 root     root       341862 Mar 25 08:18 ./__spark_libs__/jackson-module-scala_2.12-2.10.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917760   16 -r-x------   1 root     root        15935 Mar 25 08:18 ./__spark_libs__/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918009  424 -r-x------   1 root     root       430755 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-servermigration-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917761   44 -r-x------   1 root     root        41472 Mar 25 08:18 ./__spark_libs__/slf4j-api-1.7.30.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918010  620 -r-x------   1 root     root       631376 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-inspector-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917762  900 -r-x------   1 root     root       918300 Mar 25 08:18 ./__spark_libs__/hive-serde-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917763  132 -r-x------   1 root     root       134696 Mar 25 08:18 ./__spark_libs__/breeze-macros_2.12-1.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918011  136 -r-x------   1 root     root       138924 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-marketplacecatalog-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918012  428 -r-x------   1 root     root       435660 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-datasync-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917764   44 -r-x------   1 root     root        41123 Mar 25 08:18 ./__spark_libs__/commons-cli-1.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918013  156 -r-x------   1 root     root       158415 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-rdsdata-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917765  680 -r-x------   1 root     root       693271 Mar 25 08:18 ./__spark_libs__/spark-hive_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918014 1040 -r-x------   1 root     root      1061132 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-s3-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917766   20 -r-x------   1 root     root        18140 Mar 25 08:18 ./__spark_libs__/jakarta.inject-2.6.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918015  112 -r-x------   1 root     root       113976 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-elasticinference-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917767  760 -r-x------   1 root     root       774384 Mar 25 08:18 ./__spark_libs__/httpclient-4.5.9.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917768   96 -r-x------   1 root     root        96221 Mar 25 08:18 ./__spark_libs__/commons-pool-1.5.4.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917769 1972 -r-x------   1 root     root      2016766 Mar 25 08:18 ./__spark_libs__/datanucleus-core-4.1.17.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918016  248 -r-x------   1 root     root       252611 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-computeoptimizer-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917770   36 -r-x------   1 root     root        33031 Mar 25 08:18 ./__spark_libs__/jsr305-3.0.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917771  304 -r-x------   1 root     root       310891 Mar 25 08:18 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917772  404 -r-x------   1 root     root       410874 Mar 25 08:18 ./__spark_libs__/kryo-shaded-4.0.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917773   48 -r-x------   1 root     root        46646 Mar 25 08:18 ./__spark_libs__/jackson-dataformat-yaml-2.10.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917774   20 -r-x------   1 root     root        18497 Mar 25 08:18 ./__spark_libs__/flatbuffers-java-1.9.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917775   36 -r-x------   1 root     root        36175 Mar 25 08:18 ./__spark_libs__/json4s-jackson_2.12-3.6.6.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918017 1192 -r-x------   1 root     root      1220227 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-servicecatalog-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917776  100 -r-x------   1 root     root       100636 Mar 25 08:18 ./__spark_libs__/jsp-api-2.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918018  240 -r-x------   1 root     root       242276 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-ivs-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917777   56 -r-x------   1 root     root        54391 Mar 25 08:18 ./__spark_libs__/objenesis-2.5.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917778   16 -r-x------   1 root     root        13312 Mar 25 08:18 ./__spark_libs__/hive-shims-scheduler-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917779  424 -r-x------   1 root     root       431147 Mar 25 08:18 ./__spark_libs__/spark-graphx_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917780   84 -r-x------   1 root     root        82756 Mar 25 08:18 ./__spark_libs__/kerb-server-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917781    8 -r-x------   1 root     root         4592 Mar 25 08:18 ./__spark_libs__/jul-to-slf4j-1.7.30.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917782  120 -r-x------   1 root     root       120527 Mar 25 08:18 ./__spark_libs__/hive-shims-common-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918019  320 -r-x------   1 root     root       325184 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-sqs-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918020   84 -r-x------   1 root     root        82717 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-pi-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917783 1024 -r-x------   1 root     root      1048530 Mar 25 08:18 ./__spark_libs__/parquet-jackson-1.10.1-spark-amzn-2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918021   72 -r-x------   1 root     root        70061 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-pricing-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917784 1168 -r-x------   1 root     root      1194003 Mar 25 08:18 ./__spark_libs__/arpack_combined_all-0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918022 1044 -r-x------   1 root     root      1067592 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-ecs-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918023   48 -r-x------   1 root     root        47606 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-cloudwatchmetrics-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917785   28 -r-x------   1 root     root        27006 Mar 25 08:18 ./__spark_libs__/aopalliance-repackaged-2.6.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917786   52 -r-x------   1 root     root        51355 Mar 25 08:18 ./__spark_libs__/spark-unsafe_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917787  296 -r-x------   1 root     root       302558 Mar 25 08:18 ./__spark_libs__/snakeyaml-1.24.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917788 1152 -r-x------   1 root     root      1175798 Mar 25 08:18 ./__spark_libs__/JTransforms-3.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918024  368 -r-x------   1 root     root       374969 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-codeguruprofiler-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917789 1024 -r-x------   1 root     root      1045744 Mar 25 08:18 ./__spark_libs__/leveldbjni-all-1.8.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918025   76 -r-x------   1 root     root        76943 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-costandusagereport-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917790   96 -r-x------   1 root     root        95806 Mar 25 08:18 ./__spark_libs__/javax.servlet-api-3.1.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917791 1800 -r-x------   1 root     root      1839499 Mar 25 08:18 ./__spark_libs__/netlib-native_ref-osx-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917792 5156 -r-x------   1 root     root      5276900 Mar 25 08:18 ./__spark_libs__/scala-library-2.12.10.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918026  144 -r-x------   1 root     root       146064 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-resourcegroupstaggingapi-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917793 2140 -r-x------   1 root     root      2189117 Mar 25 08:18 ./__spark_libs__/guava-14.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917794  112 -r-x------   1 root     root       112235 Mar 25 08:18 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917795  184 -r-x------   1 root     root       186708 Mar 25 08:18 ./__spark_libs__/avro-mapred-1.8.2-hadoop2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918063  492 -r-x------   1 root     root       501702 Mar 25 08:18 ./__spark_libs__/hadoop-aws-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917796  432 -r-x------   1 root     root       438843 Mar 25 08:18 ./__spark_libs__/hive-common-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918027   68 -r-x------   1 root     root        66730 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-migrationhubconfig-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917797 9292 -r-x------   1 root     root      9511792 Mar 25 08:18 ./__spark_libs__/spark-catalyst_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918028  188 -r-x------   1 root     root       191353 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-resourcegroups-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918029  520 -r-x------   1 root     root       528775 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-connect-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917798   60 -r-x------   1 root     root        59890 Mar 25 08:18 ./__spark_libs__/spark-kvstore_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917799 13504 -r-x------   1 root     root     13826799 Mar 25 08:18 ./__spark_libs__/breeze_2.12-1.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917800   88 -r-x------   1 root     root        88732 Mar 25 08:18 ./__spark_libs__/okio-1.15.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917801   16 -r-x------   1 root     root        15169 Mar 25 08:18 ./__spark_libs__/spark-tags_2.12-3.0.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917802 1688 -r-x------   1 root     root      1726527 Mar 25 08:18 ./__spark_libs__/ehcache-3.3.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917803  112 -r-x------   1 root     root       110600 Mar 25 08:18 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917804 1256 -r-x------   1 root     root      1282424 Mar 25 08:18 ./__spark_libs__/ivy-2.4.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918030  464 -r-x------   1 root     root       473688 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-kinesisvideo-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918031   48 -r-x------   1 root     root        47976 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-test-utils-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917805   76 -r-x------   1 root     root        76733 Mar 25 08:18 ./__spark_libs__/jersey-hk2-2.30.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917806  624 -r-x------   1 root     root       637428 Mar 25 08:18 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917807 10424 -r-x------   1 root     root     10672015 Mar 25 08:18 ./__spark_libs__/scala-compiler-2.12.10.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917808  176 -r-x------   1 root     root       176285 Mar 25 08:18 ./__spark_libs__/automaton-1.11-8.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917809  908 -r-x------   1 root     root       927721 Mar 25 08:18 ./__spark_libs__/jersey-server-2.30.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918032 1560 -r-x------   1 root     root      1595675 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-api-gateway-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917810  416 -r-x------   1 root     root       423175 Mar 25 08:18 ./__spark_libs__/okhttp-3.12.6.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918033  764 -r-x------   1 root     root       779663 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-codepipeline-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918034 1796 -r-x------   1 root     root      1835621 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-dynamodb-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917811   68 -r-x------   1 root     root        67897 Mar 25 08:18 ./__spark_libs__/jackson-annotations-2.10.0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917812 1404 -r-x------   1 root     root      1437215 Mar 25 08:18 ./__spark_libs__/arrow-vector-0.15.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917813   36 -r-x------   1 root     root        36708 Mar 25 08:18 ./__spark_libs__/kerb-util-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918035  132 -r-x------   1 root     root       133434 Mar 25 08:18 ./__spark_libs__/aws-glue-datacatalog-spark-client.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918036  596 -r-x------   1 root     root       609816 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-costexplorer-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917814    4 -r-x------   1 root     root         2497 Mar 25 08:18 ./__spark_libs__/javax.inject-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918037  344 -r-x------   1 root     root       348636 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-transcribe-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918038  728 -r-x------   1 root     root       743779 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-sesv2-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918039  636 -r-x------   1 root     root       647972 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-backup-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918040  148 -r-x------   1 root     root       148240 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-sts-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918041 3036 -r-x------   1 root     root      3106546 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-iot-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917815  220 -r-x------   1 root     root       222980 Mar 25 08:18 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917816  788 -r-x------   1 root     root       805848 Mar 25 08:18 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917817  116 -r-x------   1 root     root       115498 Mar 25 08:18 ./__spark_libs__/jakarta.xml.bind-api-2.3.2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918042  396 -r-x------   1 root     root       403163 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-discovery-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918043   44 -r-x------   1 root     root        43408 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-marketplaceentitlement-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918044  388 -r-x------   1 root     root       396097 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-elasticloadbalancing-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917818   28 -r-x------   1 root     root        25475 Mar 25 08:18 ./__spark_libs__/json-1.8.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917819  308 -r-x------   1 root     root       313702 Mar 25 08:18 ./__spark_libs__/libfb303-0.9.3.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918045  396 -r-x------   1 root     root       401479 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-efs-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918046 2092 -r-x------   1 root     root      2140825 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-glue-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918047  816 -r-x------   1 root     root       831491 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-guardduty-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917820   20 -r-x------   1 root     root        20409 Mar 25 08:18 ./__spark_libs__/kerb-simplekdc-1.0.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918048  372 -r-x------   1 root     root       378146 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-schemas-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918049  544 -r-x------   1 root     root       554655 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-elasticloadbalancingv2-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918050 1068 -r-x------   1 root     root      1091650 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-opsworks-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918051  124 -r-x------   1 root     root       124989 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-macie-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918052 1068 -r-x------   1 root     root      1091741 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-cloudfront-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918053   80 -r-x------   1 root     root        78834 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-honeycode-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917821  344 -r-x------   1 root     root       349025 Mar 25 08:18 ./__spark_libs__/json4s-scalap_2.12-3.6.6.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918054  888 -r-x------   1 root     root       905885 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-emr-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918055  288 -r-x------   1 root     root       293007 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-licensemanager-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917822  332 -r-x------   1 root     root       336803 Mar 25 08:18 ./__spark_libs__/antlr4-runtime-4.7.1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918056  204 -r-x------   1 root     root       205056 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-codestarnotifications-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918057 2800 -r-x------   1 root     root      2864744 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-sagemaker-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917823 10632 -r-x------   1 root     root     10883684 Mar 25 08:18 ./__spark_libs__/hive-exec-2.3.7-amzn-0-core.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917824   56 -r-x------   1 root     root        56273 Mar 25 08:18 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917825   36 -r-x------   1 root     root        34601 Mar 25 08:18 ./__spark_libs__/spire-util_2.12-0.17.0-M1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918058 1000 -r-x------   1 root     root      1023417 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-macie2-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917826  496 -r-x------   1 root     root       503880 Mar 25 08:18 ./__spark_libs__/commons-lang3-3.9.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918059  392 -r-x------   1 root     root       398284 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-dataexchange-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917827  116 -r-x------   1 root     root       117481 Mar 25 08:18 ./__spark_libs__/hive-jdbc-2.3.7-amzn-0.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917828  224 -r-x------   1 root     root       225999 Mar 25 08:18 ./__spark_libs__/hadoop-yarn-registry-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918060  400 -r-x------   1 root     root       408589 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-eks-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918061  364 -r-x------   1 root     root       369379 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-worklink-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917829 1784 -r-x------   1 root     root      1826785 Mar 25 08:18 ./__spark_libs__/netlib-native_ref-win-i686-1.1-natives.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918062  696 -r-x------   1 root     root       711839 Mar 25 08:18 ./__spark_libs__/aws-java-sdk-elasticbeanstalk-1.11.828.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917830   96 -r-x------   1 root     root        95077 Mar 25 08:18 ./__spark_libs__/parquet-common-1.10.1-spark-amzn-2.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 917831  384 -r-x------   1 root     root       392124 Mar 25 08:18 ./__spark_libs__/velocity-1.5.jar\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918131    4 drwx--x---   2 root     root         4096 Mar 25 08:18 ./tmp\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/directory.info] 918136    4 -rwx------   1 root     root          668 Mar 25 08:18INFO:root:Adding head\u001b[0m\n",
      "\u001b[34mINFO:root:Adding column limit\u001b[0m\n",
      "\u001b[34mINFO:root:Adding caching\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,267 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/sagemaker-user/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,268 INFO internal.SharedState: Warehouse path is 'file:/home/sagemaker-user/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,282 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,284 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59aab76e{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,284 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,285 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32199085{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,285 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,286 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73119fd{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,286 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,286 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a59ff06{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,287 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7567a27{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,962 INFO monitor.ContainersMonitorImpl: container_1616660305706_0001_01_000002's ip = 10.0.226.231, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:44,966 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1616660305706_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:45,148 INFO datasources.InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:45,211 INFO datasources.InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:45,390 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 5574, script: , vendor: , cores -> name: cores, amount: 16, script: , vendor: , memory -> name: memory, amount: 55742, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:46,031 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:46,093 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.226.231:40028) with ID 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,293 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1279@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,300 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,302 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,302 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,936 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,937 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,939 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,940 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:44,940 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,334 INFO client.TransportClientFactory: Successfully created connection to /10.0.226.231:40703 after 77 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,468 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,468 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,468 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,468 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,468 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,526 INFO client.TransportClientFactory: Successfully created connection to /10.0.226.231:40703 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,597 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/blockmgr-2a8e489d-dc88-485d-a767-75ea5eca0e06\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,647 INFO memory.MemoryStore: MemoryStore started with capacity 28.9 GiB\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,955 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.226.231:40703\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,966 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,968 INFO resource.ResourceUtils: Resources for spark.executor:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:45,968 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,099 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,103 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,144 WARN executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3336)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3356)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:46,217 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:46763 with 28.9 GiB RAM, BlockManagerId(1, algo-1, 46763, None)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:47,463 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:47,467 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:47,468 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:47,471 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:47,925 INFO codegen.CodeGenerator: Code generated in 212.359048 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:47,981 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,025 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,027 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,030 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,077 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,083 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,163 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,176 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,176 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,176 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,177 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,181 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,235 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,237 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,238 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.226.231:41985 (size: 5.4 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,238 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,251 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,252 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,284 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:48,543 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:46763 (size: 5.4 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.util.Success.$anonfun$map$1(Try.scala:255)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.util.Success.map(Try.scala:213)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] #011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,196 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46763.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,197 INFO netty.NettyBlockTransferService: Server created on algo-1:46763\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,198 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,210 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 46763, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,220 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 46763, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:46,220 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 46763, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,306 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,314 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,505 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,659 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1384 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,662 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,667 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.474 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,670 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,670 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,672 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.508703 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,698 INFO codegen.CodeGenerator: Code generated in 13.678772 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,757 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,757 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,757 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,757 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,764 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 313.9 KiB, free 1007.2 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,774 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.2 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,774 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,775 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,776 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:49,776 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:50,304 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.226.231:41985 in memory (size: 5.4 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:50,319 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:46763 in memory (size: 5.4 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:50,329 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:50,338 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:50,341 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:46763 in memory (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,143 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,144 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,144 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,145 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marital_status_and_gender: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,183 INFO codegen.CodeGenerator: Code generated in 20.882852 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,187 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,197 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,197 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,198 INFO spark.SparkContext: Created broadcast 3 from rdd at CountVectorizer.scala:191\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,202 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,203 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,235 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:197\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,236 INFO scheduler.DAGScheduler: Got job 1 (count at CountVectorizer.scala:197) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,236 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at CountVectorizer.scala:197)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,236 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,239 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,239 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[16] at map at CountVectorizer.scala:191), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,285 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 30.2 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,287 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,287 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.226.231:41985 (size: 14.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,288 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,288 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[16] at map at CountVectorizer.scala:191) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,288 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,290 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7755 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:51,304 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:46763 (size: 14.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,430 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,492 INFO client.TransportClientFactory: Successfully created connection to /10.0.226.231:41985 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,538 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,547 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 116 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:48,627 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,358 INFO codegen.CodeGenerator: Code generated in 234.249054 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,409 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,495 INFO codegen.CodeGenerator: Code generated in 11.973949 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,497 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,503 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,506 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,567 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:49,648 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1903 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:51,292 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:51,292 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:51,296 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:51,302 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:51,305 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 8 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:52,927 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,141 INFO storage.BlockManagerInfo: Added rdd_16_0 in memory on algo-1:46763 (size: 202.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,158 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1869 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,158 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,159 INFO scheduler.DAGScheduler: ResultStage 1 (count at CountVectorizer.scala:197) finished in 1.919 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,160 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,160 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,160 INFO scheduler.DAGScheduler: Job 1 finished: count at CountVectorizer.scala:197, took 1.924432 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,210 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:233\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,211 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.226.231:41985 in memory (size: 14.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,213 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:46763 in memory (size: 14.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,218 INFO scheduler.DAGScheduler: Registering RDD 17 (flatMap at CountVectorizer.scala:212) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,220 INFO scheduler.DAGScheduler: Got job 2 (count at CountVectorizer.scala:233) with 32 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,220 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at CountVectorizer.scala:233)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,220 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,221 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,224 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at flatMap at CountVectorizer.scala:212), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,238 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.3 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,239 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,240 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.226.231:41985 (size: 15.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,241 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,243 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at flatMap at CountVectorizer.scala:212) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,243 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,248 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,261 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:46763 (size: 15.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,385 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,385 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,386 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (flatMap at CountVectorizer.scala:212) finished in 0.159 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,387 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,387 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,387 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,387 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,390 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[20] at map at CountVectorizer.scala:230), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,399 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.4 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,400 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,400 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.226.231:41985 (size: 3.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,401 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,401 INFO scheduler.DAGScheduler: Submitting 32 missing tasks from ResultStage 3 (MapPartitionsRDD[20] at map at CountVectorizer.scala:230) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,401 INFO cluster.YarnScheduler: Adding task set 3.0 with 32 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,404 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 3, algo-1, executor 1, partition 1, NODE_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,404 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 3.0 (TID 4, algo-1, executor 1, partition 8, NODE_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,404 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 3.0 (TID 5, algo-1, executor 1, partition 14, NODE_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,404 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 3.0 (TID 6, algo-1, executor 1, partition 20, NODE_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,405 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 3.0 (TID 7, algo-1, executor 1, partition 25, NODE_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,405 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,405 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 9, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,406 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 10, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,406 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 11, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,406 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 12, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,406 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 3.0 (TID 13, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,407 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 3.0 (TID 14, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,407 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 3.0 (TID 15, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,407 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 3.0 (TID 16, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,408 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 3.0 (TID 17, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,408 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 3.0 (TID 18, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,433 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:46763 (size: 3.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,464 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,546 INFO storage.BlockManagerInfo: Added rdd_20_11 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,546 INFO storage.BlockManagerInfo: Added rdd_20_12 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,546 INFO storage.BlockManagerInfo: Added rdd_20_4 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,547 INFO storage.BlockManagerInfo: Added rdd_20_3 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,547 INFO storage.BlockManagerInfo: Added rdd_20_7 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,547 INFO storage.BlockManagerInfo: Added rdd_20_6 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,551 INFO storage.BlockManagerInfo: Added rdd_20_2 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,551 INFO storage.BlockManagerInfo: Added rdd_20_10 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,551 INFO storage.BlockManagerInfo: Added rdd_20_9 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,551 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 3.0 (TID 19, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,552 INFO storage.BlockManagerInfo: Added rdd_20_5 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,552 INFO storage.BlockManagerInfo: Added rdd_20_0 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,552 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 3.0 (TID 20, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,553 INFO storage.BlockManagerInfo: Added rdd_20_1 in memory on algo-1:46763 (size: 168.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,553 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 3.0 (TID 21, algo-1, executor 1, partition 16, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,554 INFO storage.BlockManagerInfo: Added rdd_20_20 in memory on algo-1:46763 (size: 160.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,554 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 3.0 (TID 22, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,555 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 3.0 (TID 23, algo-1, executor 1, partition 18, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,556 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 3.0 (TID 24, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,556 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 11) in 150 ms on algo-1 (executor 1) (1/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,556 INFO storage.BlockManagerInfo: Added rdd_20_25 in memory on algo-1:46763 (size: 160.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,557 INFO storage.BlockManagerInfo: Added rdd_20_14 in memory on algo-1:46763 (size: 160.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,557 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 3.0 (TID 18) in 149 ms on algo-1 (executor 1) (2/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,557 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 3.0 (TID 16) in 150 ms on algo-1 (executor 1) (3/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,558 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 3.0 (TID 13) in 152 ms on algo-1 (executor 1) (4/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,559 INFO storage.BlockManagerInfo: Added rdd_20_8 in memory on algo-1:46763 (size: 160.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,565 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 3.0 (TID 17) in 158 ms on algo-1 (executor 1) (5/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,565 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 10) in 160 ms on algo-1 (executor 1) (6/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,574 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 3.0 (TID 25, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,575 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 3.0 (TID 14) in 168 ms on algo-1 (executor 1) (7/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,575 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 3.0 (TID 26, algo-1, executor 1, partition 22, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,576 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 9) in 171 ms on algo-1 (executor 1) (8/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,576 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 3.0 (TID 27, algo-1, executor 1, partition 23, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,577 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 3.0 (TID 6) in 173 ms on algo-1 (executor 1) (9/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,577 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 3.0 (TID 28, algo-1, executor 1, partition 24, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,578 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 3) in 175 ms on algo-1 (executor 1) (10/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,578 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 3.0 (TID 29, algo-1, executor 1, partition 26, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,578 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 173 ms on algo-1 (executor 1) (11/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,579 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 3.0 (TID 30, algo-1, executor 1, partition 27, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,579 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 3.0 (TID 5) in 175 ms on algo-1 (executor 1) (12/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,580 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 3.0 (TID 31, algo-1, executor 1, partition 28, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,580 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 12) in 174 ms on algo-1 (executor 1) (13/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,581 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 3.0 (TID 32, algo-1, executor 1, partition 29, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,581 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 3.0 (TID 4) in 177 ms on algo-1 (executor 1) (14/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,582 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 3.0 (TID 33, algo-1, executor 1, partition 30, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,583 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 3.0 (TID 15) in 176 ms on algo-1 (executor 1) (15/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,583 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 3.0 (TID 34, algo-1, executor 1, partition 31, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,583 INFO storage.BlockManagerInfo: Added rdd_20_15 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,583 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 3.0 (TID 7) in 178 ms on algo-1 (executor 1) (16/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,584 INFO storage.BlockManagerInfo: Added rdd_20_13 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,592 INFO storage.BlockManagerInfo: Added rdd_20_17 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,592 INFO storage.BlockManagerInfo: Added rdd_20_16 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,593 INFO storage.BlockManagerInfo: Added rdd_20_19 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,594 INFO storage.BlockManagerInfo: Added rdd_20_18 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,605 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 3.0 (TID 20) in 53 ms on algo-1 (executor 1) (17/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,605 INFO storage.BlockManagerInfo: Added rdd_20_21 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,605 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 3.0 (TID 19) in 54 ms on algo-1 (executor 1) (18/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,608 INFO storage.BlockManagerInfo: Added rdd_20_23 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,608 INFO storage.BlockManagerInfo: Added rdd_20_26 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,609 INFO storage.BlockManagerInfo: Added rdd_20_22 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,610 INFO storage.BlockManagerInfo: Added rdd_20_27 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,613 INFO storage.BlockManagerInfo: Added rdd_20_28 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,614 INFO storage.BlockManagerInfo: Added rdd_20_30 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,618 INFO storage.BlockManagerInfo: Added rdd_20_29 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,619 INFO storage.BlockManagerInfo: Added rdd_20_31 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,620 INFO storage.BlockManagerInfo: Added rdd_20_24 in memory on algo-1:46763 (size: 24.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,626 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 3.0 (TID 22) in 72 ms on algo-1 (executor 1) (19/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,626 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 3.0 (TID 24) in 70 ms on algo-1 (executor 1) (20/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,627 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 3.0 (TID 23) in 72 ms on algo-1 (executor 1) (21/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,628 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 3.0 (TID 21) in 75 ms on algo-1 (executor 1) (22/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,630 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 3.0 (TID 25) in 56 ms on algo-1 (executor 1) (23/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,630 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 3.0 (TID 29) in 52 ms on algo-1 (executor 1) (24/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,631 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 3.0 (TID 27) in 55 ms on algo-1 (executor 1) (25/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,631 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 3.0 (TID 30) in 52 ms on algo-1 (executor 1) (26/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,631 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 3.0 (TID 26) in 56 ms on algo-1 (executor 1) (27/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,632 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 3.0 (TID 31) in 52 ms on algo-1 (executor 1) (28/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,633 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 3.0 (TID 32) in 52 ms on algo-1 (executor 1) (29/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,633 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 3.0 (TID 33) in 51 ms on algo-1 (executor 1) (30/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,635 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 3.0 (TID 28) in 58 ms on algo-1 (executor 1) (31/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,636 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 3.0 (TID 34) in 53 ms on algo-1 (executor 1) (32/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,636 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,637 INFO scheduler.DAGScheduler: ResultStage 3 (count at CountVectorizer.scala:233) finished in 0.241 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,637 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,637 INFO cluster.YarnScheduler: Killing all running tasks in stage 3: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,637 INFO scheduler.DAGScheduler: Job 2 finished: count at CountVectorizer.scala:233, took 0.427644 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,663 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:236\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,665 INFO scheduler.DAGScheduler: Got job 3 (top at CountVectorizer.scala:236) with 32 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,665 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (top at CountVectorizer.scala:236)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,665 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,666 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,666 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at top at CountVectorizer.scala:236), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,672 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 6.5 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,674 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,674 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.226.231:41985 (size: 3.4 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,674 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,675 INFO scheduler.DAGScheduler: Submitting 32 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at top at CountVectorizer.scala:236) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,675 INFO cluster.YarnScheduler: Adding task set 5.0 with 32 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,677 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 35, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,677 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 36, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,677 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 37, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,677 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 38, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,678 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 39, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,678 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.0 (TID 40, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,678 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 5.0 (TID 41, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,679 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 5.0 (TID 42, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,679 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 5.0 (TID 43, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,679 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 5.0 (TID 44, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,679 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 5.0 (TID 45, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,679 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 5.0 (TID 46, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,680 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 5.0 (TID 47, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,680 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 5.0 (TID 48, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,680 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 5.0 (TID 49, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,680 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 5.0 (TID 50, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,700 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:46763 (size: 3.4 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,736 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 5.0 (TID 51, algo-1, executor 1, partition 16, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,737 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 5.0 (TID 52, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,738 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 5.0 (TID 53, algo-1, executor 1, partition 18, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,751 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 5.0 (TID 54, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,755 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 5.0 (TID 55, algo-1, executor 1, partition 20, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,759 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 5.0 (TID 56, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,760 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 5.0 (TID 57, algo-1, executor 1, partition 22, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,760 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 5.0 (TID 58, algo-1, executor 1, partition 23, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,762 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 5.0 (TID 59, algo-1, executor 1, partition 24, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,790 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 5.0 (TID 60, algo-1, executor 1, partition 25, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,791 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 5.0 (TID 61, algo-1, executor 1, partition 26, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,792 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 5.0 (TID 62, algo-1, executor 1, partition 27, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,792 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 36) in 115 ms on algo-1 (executor 1) (1/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,793 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 5.0 (TID 42) in 115 ms on algo-1 (executor 1) (2/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,793 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 5.0 (TID 50) in 113 ms on algo-1 (executor 1) (3/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,794 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 37) in 117 ms on algo-1 (executor 1) (4/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,795 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 5.0 (TID 63, algo-1, executor 1, partition 28, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,795 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 5.0 (TID 64, algo-1, executor 1, partition 29, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,796 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 5.0 (TID 49) in 116 ms on algo-1 (executor 1) (5/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,796 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 5.0 (TID 45) in 117 ms on algo-1 (executor 1) (6/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,797 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 5.0 (TID 65, algo-1, executor 1, partition 30, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,797 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 39) in 120 ms on algo-1 (executor 1) (7/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,798 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 5.0 (TID 44) in 119 ms on algo-1 (executor 1) (8/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,798 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 38) in 121 ms on algo-1 (executor 1) (9/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,799 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 5.0 (TID 41) in 121 ms on algo-1 (executor 1) (10/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,799 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 5.0 (TID 51) in 64 ms on algo-1 (executor 1) (11/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,803 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 35) in 126 ms on algo-1 (executor 1) (12/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,804 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 5.0 (TID 53) in 66 ms on algo-1 (executor 1) (13/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,804 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 5.0 (TID 43) in 125 ms on algo-1 (executor 1) (14/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,805 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 5.0 (TID 66, algo-1, executor 1, partition 31, PROCESS_LOCAL, 7154 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,811 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 5.0 (TID 52) in 74 ms on algo-1 (executor 1) (15/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,812 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 5.0 (TID 48) in 132 ms on algo-1 (executor 1) (16/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,813 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 5.0 (TID 54) in 62 ms on algo-1 (executor 1) (17/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,814 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 5.0 (TID 58) in 53 ms on algo-1 (executor 1) (18/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,814 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 5.0 (TID 56) in 59 ms on algo-1 (executor 1) (19/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,814 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 5.0 (TID 55) in 60 ms on algo-1 (executor 1) (20/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,815 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 5.0 (TID 57) in 56 ms on algo-1 (executor 1) (21/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,815 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 5.0 (TID 59) in 53 ms on algo-1 (executor 1) (22/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,818 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 5.0 (TID 47) in 138 ms on algo-1 (executor 1) (23/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,819 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 5.0 (TID 46) in 140 ms on algo-1 (executor 1) (24/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,820 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.0 (TID 40) in 142 ms on algo-1 (executor 1) (25/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,825 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 5.0 (TID 61) in 34 ms on algo-1 (executor 1) (26/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,825 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 5.0 (TID 60) in 35 ms on algo-1 (executor 1) (27/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,825 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 5.0 (TID 65) in 28 ms on algo-1 (executor 1) (28/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,825 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 5.0 (TID 63) in 31 ms on algo-1 (executor 1) (29/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,825 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 5.0 (TID 66) in 20 ms on algo-1 (executor 1) (30/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,827 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 5.0 (TID 64) in 32 ms on algo-1 (executor 1) (31/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,827 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 5.0 (TID 62) in 35 ms on algo-1 (executor 1) (32/32)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,827 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,828 INFO scheduler.DAGScheduler: ResultStage 5 (top at CountVectorizer.scala:236) finished in 0.160 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,828 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,828 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,829 INFO scheduler.DAGScheduler: Job 3 finished: top at CountVectorizer.scala:236, took 0.165457 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,830 INFO rdd.MapPartitionsRDD: Removing RDD 16 from persistence list\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,838 INFO rdd.MapPartitionsRDD: Removing RDD 20 from persistence list\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,843 INFO storage.BlockManager: Removing RDD 16\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,844 INFO storage.BlockManager: Removing RDD 20\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,917 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,923 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,924 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,924 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,924 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,954 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,955 INFO scheduler.DAGScheduler: Got job 4 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,955 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,955 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,955 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,956 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,970 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 85.6 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,972 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,972 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,973 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,973 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,973 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,976 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 67, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7807 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:53,989 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,086 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 67) in 112 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,086 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,086 INFO scheduler.DAGScheduler: ResultStage 6 (runJob at SparkHadoopWriter.scala:78) finished in 0.129 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,086 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,086 INFO cluster.YarnScheduler: Killing all running tasks in stage 6: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,087 INFO scheduler.DAGScheduler: Job 4 finished: runJob at SparkHadoopWriter.scala:78, took 0.132596 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,089 INFO io.SparkHadoopWriter: Job job_20210325081853_0023 committed.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:51,306 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 30.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,785 INFO codegen.CodeGenerator: Code generated in 31.320841 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,905 INFO codegen.CodeGenerator: Code generated in 20.686701 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,915 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,918 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,926 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,928 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:52,937 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,138 INFO memory.MemoryStore: Block rdd_16_0 stored as values in memory (estimated size 202.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,154 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1665 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,250 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,250 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,255 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,260 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,262 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,263 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,320 INFO storage.BlockManager: Found block rdd_16_0 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,379 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 1929 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,409 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,409 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 3)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,410 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,410 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,410 INFO executor.Executor: Running task 8.0 in stage 3.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,411 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,411 INFO executor.Executor: Running task 14.0 in stage 3.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,411 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,412 INFO executor.Executor: Running task 20.0 in stage 3.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,412 INFO executor.Executor: Running task 25.0 in stage 3.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,413 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,414 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 8)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,414 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,414 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,416 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,416 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,422 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,422 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 9)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,422 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,422 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,423 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,423 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,423 INFO executor.Executor: Running task 5.0 in stage 3.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,423 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,424 INFO executor.Executor: Running task 6.0 in stage 3.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,428 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,429 INFO executor.Executor: Running task 7.0 in stage 3.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,429 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,429 INFO executor.Executor: Running task 9.0 in stage 3.0 (TID 15)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,429 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,430 INFO executor.Executor: Running task 10.0 in stage 3.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,430 INFO executor.Executor: Running task 11.0 in stage 3.0 (TID 17)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,430 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,431 INFO executor.Executor: Running task 12.0 in stage 3.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,432 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,435 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 18 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,435 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.4 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,456 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,457 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,457 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,457 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,457 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,457 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,458 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,459 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,459 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,459 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,459 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,498 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,525 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,525 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,525 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,525 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,525 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,526 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,525 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,526 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,526 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,526 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,526 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,526 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,527 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,529 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,529 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,529 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,529 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,529 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,530 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,530 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,530 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,530 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_11 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_4 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_12 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_3 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_7 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_6 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,541 INFO memory.MemoryStore: Block rdd_20_2 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,543 INFO memory.MemoryStore: Block rdd_20_10 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,543 INFO memory.MemoryStore: Block rdd_20_9 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,543 INFO memory.MemoryStore: Block rdd_20_0 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,543 INFO memory.MemoryStore: Block rdd_20_5 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,548 INFO executor.Executor: Finished task 11.0 in stage 3.0 (TID 17). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,548 INFO memory.MemoryStore: Block rdd_20_1 stored as values in memory (estimated size 168.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,548 INFO executor.Executor: Finished task 12.0 in stage 3.0 (TID 18). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,548 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 11). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,549 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 10). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,552 INFO memory.MemoryStore: Block rdd_20_20 stored as values in memory (estimated size 160.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,553 INFO executor.Executor: Finished task 6.0 in stage 3.0 (TID 13). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,553 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,553 INFO executor.Executor: Finished task 10.0 in stage 3.0 (TID 16). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,553 INFO executor.Executor: Running task 13.0 in stage 3.0 (TID 19)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,549 INFO memory.MemoryStore: Block rdd_20_25 stored as values in memory (estimated size 160.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,549 INFO executor.Executor: Finished task 7.0 in stage 3.0 (TID 14). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,549 INFO memory.MemoryStore: Block rdd_20_14 stored as values in memory (estimated size 160.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,554 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,554 INFO executor.Executor: Running task 15.0 in stage 3.0 (TID 20)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,556 INFO memory.MemoryStore: Block rdd_20_8 stored as values in memory (estimated size 160.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,557 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 9). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,557 INFO executor.Executor: Finished task 20.0 in stage 3.0 (TID 6). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,559 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,560 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,560 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,560 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 24\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,560 INFO executor.Executor: Running task 18.0 in stage 3.0 (TID 23)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,562 INFO executor.Executor: Running task 16.0 in stage 3.0 (TID 21)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,562 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 3). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,561 INFO executor.Executor: Finished task 9.0 in stage 3.0 (TID 15). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,561 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 8). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,560 INFO executor.Executor: Running task 19.0 in stage 3.0 (TID 24)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,563 INFO executor.Executor: Finished task 14.0 in stage 3.0 (TID 5). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,563 INFO executor.Executor: Finished task 5.0 in stage 3.0 (TID 12). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,560 INFO executor.Executor: Running task 17.0 in stage 3.0 (TID 22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,563 INFO executor.Executor: Finished task 8.0 in stage 3.0 (TID 4). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,569 INFO executor.Executor: Finished task 25.0 in stage 3.0 (TID 7). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,576 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,576 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,577 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,577 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,577 INFO memory.MemoryStore: Block rdd_20_15 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,578 INFO memory.MemoryStore: Block rdd_20_13 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,580 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,580 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 26\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,580 INFO executor.Executor: Running task 21.0 in stage 3.0 (TID 25)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,580 INFO executor.Executor: Running task 22.0 in stage 3.0 (TID 26)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,580 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,581 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 28\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,581 INFO executor.Executor: Running task 24.0 in stage 3.0 (TID 28)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,581 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,581 INFO executor.Executor: Running task 26.0 in stage 3.0 (TID 29)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,581 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 30\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,582 INFO executor.Executor: Running task 23.0 in stage 3.0 (TID 27)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,585 INFO executor.Executor: Running task 27.0 in stage 3.0 (TID 30)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,586 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 31\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,586 INFO executor.Executor: Running task 28.0 in stage 3.0 (TID 31)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 32\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO executor.Executor: Running task 29.0 in stage 3.0 (TID 32)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,587 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,588 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,588 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO executor.Executor: Running task 30.0 in stage 3.0 (TID 33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 34\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO memory.MemoryStore: Block rdd_20_19 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO memory.MemoryStore: Block rdd_20_17 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO executor.Executor: Running task 31.0 in stage 3.0 (TID 34)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,589 INFO memory.MemoryStore: Block rdd_20_16 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,590 INFO memory.MemoryStore: Block rdd_20_18 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,592 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,598 INFO executor.Executor: Finished task 15.0 in stage 3.0 (TID 20). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,599 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,600 INFO executor.Executor: Finished task 13.0 in stage 3.0 (TID 19). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,600 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,600 INFO memory.MemoryStore: Block rdd_20_21 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,600 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,601 INFO memory.MemoryStore: Block rdd_20_23 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,602 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,602 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,602 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,602 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,603 INFO memory.MemoryStore: Block rdd_20_26 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,603 INFO memory.MemoryStore: Block rdd_20_22 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,604 INFO executor.Executor: Finished task 17.0 in stage 3.0 (TID 22). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,605 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,605 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,605 INFO memory.MemoryStore: Block rdd_20_27 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,606 INFO executor.Executor: Finished task 19.0 in stage 3.0 (TID 24). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,606 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,606 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,607 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,607 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,610 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,610 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,610 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,610 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,610 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,610 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,611 INFO executor.Executor: Finished task 18.0 in stage 3.0 (TID 23). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,612 INFO memory.MemoryStore: Block rdd_20_28 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,612 INFO memory.MemoryStore: Block rdd_20_30 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,613 INFO memory.MemoryStore: Block rdd_20_29 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,613 INFO executor.Executor: Finished task 16.0 in stage 3.0 (TID 21). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,613 INFO memory.MemoryStore: Block rdd_20_31 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,613 INFO memory.MemoryStore: Block rdd_20_24 stored as values in memory (estimated size 24.0 B, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,616 INFO executor.Executor: Finished task 21.0 in stage 3.0 (TID 25). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,616 INFO executor.Executor: Finished task 26.0 in stage 3.0 (TID 29). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,616 INFO executor.Executor: Finished task 23.0 in stage 3.0 (TID 27). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,616 INFO executor.Executor: Finished task 27.0 in stage 3.0 (TID 30). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,616 INFO executor.Executor: Finished task 22.0 in stage 3.0 (TID 26). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,620 INFO executor.Executor: Finished task 28.0 in stage 3.0 (TID 31). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,623 INFO executor.Executor: Finished task 29.0 in stage 3.0 (TID 32). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,623 INFO executor.Executor: Finished task 30.0 in stage 3.0 (TID 33). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,623 INFO executor.Executor: Finished task 31.0 in stage 3.0 (TID 34). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,623 INFO executor.Executor: Finished task 24.0 in stage 3.0 (TID 28). 1305 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,682 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 35\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,682 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 35)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,682 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 36\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,683 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 36)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,683 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 37\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,683 INFO executor.Executor: Running task 2.0 in stage 5.0 (TID 37)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,684 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 38\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,684 INFO executor.Executor: Running task 3.0 in stage 5.0 (TID 38)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,684 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 39\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,684 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,684 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 40\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,685 INFO executor.Executor: Running task 4.0 in stage 5.0 (TID 39)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,685 INFO executor.Executor: Running task 5.0 in stage 5.0 (TID 40)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 41\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 42\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.Executor: Running task 6.0 in stage 5.0 (TID 41)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 43\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.Executor: Running task 7.0 in stage 5.0 (TID 42)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 44\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.Executor: Running task 8.0 in stage 5.0 (TID 43)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 45\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 46\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.Executor: Running task 10.0 in stage 5.0 (TID 45)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,686 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 47\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,687 INFO executor.Executor: Running task 12.0 in stage 5.0 (TID 47)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,687 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 48\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,687 INFO executor.Executor: Running task 13.0 in stage 5.0 (TID 48)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,687 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 49\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,687 INFO executor.Executor: Running task 11.0 in stage 5.0 (TID 46)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,687 INFO executor.Executor: Running task 9.0 in stage 5.0 (TID 44)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,688 INFO executor.Executor: Running task 14.0 in stage 5.0 (TID 49)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,688 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 50\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,689 INFO executor.Executor: Running task 15.0 in stage 5.0 (TID 50)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,698 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,700 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 15 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,702 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 6.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_14 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_12 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_6 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_4 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_0 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_10 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_8 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_13 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_7 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_1 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_3 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_5 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_9 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,718 INFO storage.BlockManager: Found block rdd_20_2 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,719 INFO storage.BlockManager: Found block rdd_20_15 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,719 INFO storage.BlockManager: Found block rdd_20_11 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,733 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 36). 2651 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,733 INFO executor.Executor: Finished task 8.0 in stage 5.0 (TID 43). 2648 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,733 INFO executor.Executor: Finished task 14.0 in stage 5.0 (TID 49). 2650 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,737 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 51\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,738 INFO executor.Executor: Running task 16.0 in stage 5.0 (TID 51)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,738 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 52\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,739 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 53\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,739 INFO executor.Executor: Running task 18.0 in stage 5.0 (TID 53)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,739 INFO executor.Executor: Running task 17.0 in stage 5.0 (TID 52)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,743 INFO storage.BlockManager: Found block rdd_20_16 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,743 INFO storage.BlockManager: Found block rdd_20_17 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,743 INFO storage.BlockManager: Found block rdd_20_18 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,749 INFO executor.Executor: Finished task 10.0 in stage 5.0 (TID 45). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,749 INFO executor.Executor: Finished task 7.0 in stage 5.0 (TID 42). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,751 INFO executor.Executor: Finished task 15.0 in stage 5.0 (TID 50). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,752 INFO executor.Executor: Finished task 2.0 in stage 5.0 (TID 37). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,752 INFO executor.Executor: Finished task 4.0 in stage 5.0 (TID 39). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,753 INFO executor.Executor: Finished task 13.0 in stage 5.0 (TID 48). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,753 INFO executor.Executor: Finished task 5.0 in stage 5.0 (TID 40). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,754 INFO executor.Executor: Finished task 9.0 in stage 5.0 (TID 44). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,754 INFO executor.Executor: Finished task 6.0 in stage 5.0 (TID 41). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,754 INFO executor.Executor: Finished task 3.0 in stage 5.0 (TID 38). 2469 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,755 INFO executor.Executor: Finished task 16.0 in stage 5.0 (TID 51). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,755 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 54\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,755 INFO executor.Executor: Running task 19.0 in stage 5.0 (TID 54)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,756 INFO executor.Executor: Finished task 18.0 in stage 5.0 (TID 53). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,755 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 35). 2469 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,756 INFO executor.Executor: Finished task 11.0 in stage 5.0 (TID 46). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,757 INFO executor.Executor: Finished task 12.0 in stage 5.0 (TID 47). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,757 INFO executor.Executor: Finished task 17.0 in stage 5.0 (TID 52). 2469 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,759 INFO storage.BlockManager: Found block rdd_20_19 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,759 INFO executor.Executor: Finished task 19.0 in stage 5.0 (TID 54). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,767 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 55\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,767 INFO executor.Executor: Running task 20.0 in stage 5.0 (TID 55)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,768 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 56\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,769 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 57\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,769 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 58\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,769 INFO executor.Executor: Running task 22.0 in stage 5.0 (TID 57)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,769 INFO executor.Executor: Running task 21.0 in stage 5.0 (TID 56)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,769 INFO executor.Executor: Running task 23.0 in stage 5.0 (TID 58)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,773 INFO storage.BlockManager: Found block rdd_20_23 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,776 INFO executor.Executor: Finished task 23.0 in stage 5.0 (TID 58). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,779 INFO storage.BlockManager: Found block rdd_20_20 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,779 INFO storage.BlockManager: Found block rdd_20_21 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,779 INFO storage.BlockManager: Found block rdd_20_22 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,780 INFO executor.Executor: Finished task 21.0 in stage 5.0 (TID 56). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,780 INFO executor.Executor: Finished task 20.0 in stage 5.0 (TID 55). 2563 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,781 INFO executor.Executor: Finished task 22.0 in stage 5.0 (TID 57). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,789 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 59\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,790 INFO executor.Executor: Running task 24.0 in stage 5.0 (TID 59)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,793 INFO storage.BlockManager: Found block rdd_20_24 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,794 INFO executor.Executor: Finished task 24.0 in stage 5.0 (TID 59). 2426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,808 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 60\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,808 INFO executor.Executor: Running task 25.0 in stage 5.0 (TID 60)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 61\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.Executor: Running task 26.0 in stage 5.0 (TID 61)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 62\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.Executor: Running task 27.0 in stage 5.0 (TID 62)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 63\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 64\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,809 INFO executor.Executor: Running task 28.0 in stage 5.0 (TID 63)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,810 INFO executor.Executor: Running task 29.0 in stage 5.0 (TID 64)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,811 INFO storage.BlockManager: Found block rdd_20_25 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,811 INFO storage.BlockManager: Found block rdd_20_26 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,812 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 65\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,812 INFO executor.Executor: Finished task 25.0 in stage 5.0 (TID 60). 2563 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,812 INFO executor.Executor: Running task 30.0 in stage 5.0 (TID 65)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,812 INFO executor.Executor: Finished task 26.0 in stage 5.0 (TID 61). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,813 INFO storage.BlockManager: Found block rdd_20_28 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,814 INFO storage.BlockManager: Found block rdd_20_30 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,814 INFO storage.BlockManager: Found block rdd_20_27 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,815 INFO executor.Executor: Finished task 28.0 in stage 5.0 (TID 63). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,815 INFO executor.Executor: Finished task 30.0 in stage 5.0 (TID 65). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,815 INFO executor.Executor: Finished task 27.0 in stage 5.0 (TID 62). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,816 INFO storage.BlockManager: Found block rdd_20_29 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,817 INFO executor.Executor: Finished task 29.0 in stage 5.0 (TID 64). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,818 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 66\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,819 INFO executor.Executor: Running task 31.0 in stage 5.0 (TID 66)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,821 INFO storage.BlockManager: Found block rdd_20_31 locally\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,822 INFO executor.Executor: Finished task 31.0 in stage 5.0 (TID 66). 2383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,847 INFO storage.BlockManager: Removing RDD 16\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,221 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,234 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,238 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,238 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,239 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,239 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,239 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,239 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,286 INFO codegen.CodeGenerator: Code generated in 9.859669 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,295 INFO scheduler.DAGScheduler: Registering RDD 26 (parquet at CountVectorizer.scala:371) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,295 INFO scheduler.DAGScheduler: Got map stage job 5 (parquet at CountVectorizer.scala:371) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,296 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 7 (parquet at CountVectorizer.scala:371)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,296 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,296 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,297 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[26] at parquet at CountVectorizer.scala:371), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,304 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.8 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,305 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,305 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,306 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,306 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[26] at parquet at CountVectorizer.scala:371) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,306 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,308 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 68, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7629 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,318 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,376 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 68) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,376 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,377 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (parquet at CountVectorizer.scala:371) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,377 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,377 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,377 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,377 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,421 INFO spark.SparkContext: Starting job: parquet at CountVectorizer.scala:371\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,422 INFO scheduler.DAGScheduler: Got job 6 (parquet at CountVectorizer.scala:371) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,422 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (parquet at CountVectorizer.scala:371)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,422 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,422 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,423 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (ShuffledRowRDD[27] at parquet at CountVectorizer.scala:371), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,447 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 179.0 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,449 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,450 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,450 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,451 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ShuffledRowRDD[27] at parquet at CountVectorizer.scala:371) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,451 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,453 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 69, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,466 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:54,523 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,847 INFO storage.BlockManager: Removing RDD 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,978 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 67\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,978 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 67)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,982 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,988 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,990 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:53,991 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,024 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,026 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,026 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,026 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,080 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081853_0023_m_000000_0' to file:/tmp/tmpxwsjlo6n/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,081 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081853_0023_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,082 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 67). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,310 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 68\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,310 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 68)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,312 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,317 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,318 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,319 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,374 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 68). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,455 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 69\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,455 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 69)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,459 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,460 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,465 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,466 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,467 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 179.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,522 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,522 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,525 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,525 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,525 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,533 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,533 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,534 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,534 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,534 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,535 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,570 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,573 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,606 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,606 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,606 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,607 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,695 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"vocabulary\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : \"string\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,251 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 69) in 799 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,251 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,252 INFO scheduler.DAGScheduler: ResultStage 9 (parquet at CountVectorizer.scala:371) finished in 0.828 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,252 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,252 INFO cluster.YarnScheduler: Killing all running tasks in stage 9: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,253 INFO scheduler.DAGScheduler: Job 6 finished: parquet at CountVectorizer.scala:371, took 0.831750 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,254 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,254 INFO datasources.FileFormatWriter: Write Job bb6080f0-0402-4e89-a41f-e2f578d9b5e2 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,257 INFO datasources.FileFormatWriter: Finished processing stats for write job bb6080f0-0402-4e89-a41f-e2f578d9b5e2.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,366 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 656.0 B, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,368 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 266.0 B, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,368 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.226.231:41985 (size: 266.0 B, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,368 INFO spark.SparkContext: Created broadcast 11 from broadcast at CountVectorizer.scala:306\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,700 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,739 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,741 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:46763 in memory (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,746 INFO storage.BlockManager: Removing RDD 20\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,751 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.226.231:41985 in memory (size: 15.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,758 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:46763 in memory (size: 15.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,765 INFO storage.BlockManager: Removing RDD 16\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,777 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,782 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,796 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,798 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,804 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,804 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,804 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,805 INFO datasources.FileSourceStrategy: Output Data Schema: struct<purpose: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,807 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,809 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,828 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.226.231:41985 in memory (size: 3.4 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,833 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:46763 in memory (size: 3.4 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,844 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,845 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.226.231:41985 in memory (size: 3.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,846 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:46763 in memory (size: 3.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,852 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,853 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,853 INFO spark.SparkContext: Created broadcast 12 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,854 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,854 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,860 INFO scheduler.DAGScheduler: Registering RDD 33 (collect at StringIndexer.scala:204) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,860 INFO scheduler.DAGScheduler: Got map stage job 7 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,860 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 10 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,860 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,860 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,861 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[33] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,877 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.1 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,878 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,878 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.226.231:41985 (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,879 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,879 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[33] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,879 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,880 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 70, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,888 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:46763 (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:55,929 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group vocabulary (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       optional binary element (UTF8);\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,733 INFO compress.CodecPool: Got brand-new compressor [.snappy]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:54,979 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 77\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,242 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081854_0009_m_000000_69' to file:/tmp/tmpxwsjlo6n/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,242 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081854_0009_m_000000_69: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,245 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 69). 3281 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,747 INFO storage.BlockManager: Removing RDD 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,766 INFO storage.BlockManager: Removing RDD 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,882 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 70\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,882 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 70)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,883 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,887 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,889 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 5 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,365 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 70) in 485 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,365 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,366 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (collect at StringIndexer.scala:204) finished in 0.504 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,366 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,366 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,366 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,366 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,373 INFO adaptive.ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 47.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,412 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,412 INFO scheduler.DAGScheduler: Got job 8 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,412 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,412 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,412 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,413 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[36] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,415 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 21.8 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,417 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,417 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.226.231:41985 (size: 10.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,417 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,417 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[36] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,417 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,418 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 71, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,425 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:46763 (size: 10.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,443 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,527 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 71) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,527 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,528 INFO scheduler.DAGScheduler: ResultStage 12 (collect at StringIndexer.scala:204) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,528 INFO scheduler.DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,528 INFO cluster.YarnScheduler: Killing all running tasks in stage 12: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,528 INFO scheduler.DAGScheduler: Job 8 finished: collect at StringIndexer.scala:204, took 0.116396 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,551 INFO codegen.CodeGenerator: Code generated in 19.483975 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,812 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,812 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,812 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,812 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,820 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,820 INFO scheduler.DAGScheduler: Got job 9 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,821 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,821 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,821 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,821 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[38] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,830 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 85.6 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,831 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,831 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,832 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,832 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[38] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,832 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,833 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 72, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7719 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,839 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,851 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 72) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,851 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,852 INFO scheduler.DAGScheduler: ResultStage 13 (runJob at SparkHadoopWriter.scala:78) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,852 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,852 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,852 INFO scheduler.DAGScheduler: Job 9 finished: runJob at SparkHadoopWriter.scala:78, took 0.032256 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,853 INFO io.SparkHadoopWriter: Job job_20210325081856_0038 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,879 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,879 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,879 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,879 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,879 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,880 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,880 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,880 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,895 INFO codegen.CodeGenerator: Code generated in 11.51652 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,900 INFO scheduler.DAGScheduler: Registering RDD 41 (parquet at StringIndexer.scala:498) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,900 INFO scheduler.DAGScheduler: Got map stage job 10 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,900 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 14 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,900 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,900 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,901 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[41] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,903 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.8 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,904 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,905 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,905 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,905 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[41] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,905 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,906 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 73, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7784 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,914 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,918 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 73) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,918 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,919 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (parquet at StringIndexer.scala:498) finished in 0.018 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,919 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,919 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,919 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,919 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,937 INFO spark.SparkContext: Starting job: parquet at StringIndexer.scala:498\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,938 INFO scheduler.DAGScheduler: Got job 11 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,938 INFO scheduler.DAGScheduler: Final stage: ResultStage 16 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,938 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,938 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,939 INFO scheduler.DAGScheduler: Submitting ResultStage 16 (ShuffledRowRDD[42] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,953 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 179.1 KiB, free 1007.2 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,955 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,955 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,955 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,956 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (ShuffledRowRDD[42] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,956 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,957 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 74, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,963 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,973 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,987 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 74) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,988 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,988 INFO scheduler.DAGScheduler: ResultStage 16 (parquet at StringIndexer.scala:498) finished in 0.049 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,988 INFO scheduler.DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,988 INFO cluster.YarnScheduler: Killing all running tasks in stage 16: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,988 INFO scheduler.DAGScheduler: Job 11 finished: parquet at StringIndexer.scala:498, took 0.050961 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,989 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,989 INFO datasources.FileFormatWriter: Write Job 547f7ee3-6078-43b7-856b-4ffd65f0085c committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:56,990 INFO datasources.FileFormatWriter: Finished processing stats for write job 547f7ee3-6078-43b7-856b-4ffd65f0085c.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,071 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,071 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,071 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,071 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,079 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,080 INFO scheduler.DAGScheduler: Got job 12 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,080 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,080 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,080 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,081 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[46] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,088 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.6 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,089 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,090 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,090 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,090 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[46] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,090 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,091 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 75, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7723 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,098 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,108 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 75) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,108 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,108 INFO scheduler.DAGScheduler: ResultStage 17 (runJob at SparkHadoopWriter.scala:78) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,108 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,108 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,108 INFO scheduler.DAGScheduler: Job 12 finished: runJob at SparkHadoopWriter.scala:78, took 0.029149 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,109 INFO io.SparkHadoopWriter: Job job_20210325081857_0046 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,132 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,132 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,133 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,133 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,133 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,133 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,133 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,133 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,145 INFO codegen.CodeGenerator: Code generated in 8.752029 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,150 INFO scheduler.DAGScheduler: Registering RDD 49 (parquet at OneHotEncoder.scala:408) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,150 INFO scheduler.DAGScheduler: Got map stage job 13 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,150 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,150 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,151 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,151 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[49] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,153 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 7.8 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,155 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,155 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,155 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,156 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[49] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,156 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,156 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 76, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7549 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,162 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 76) in 11 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (parquet at OneHotEncoder.scala:408) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,167 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,185 INFO spark.SparkContext: Starting job: parquet at OneHotEncoder.scala:408\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,187 INFO scheduler.DAGScheduler: Got job 14 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,187 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,187 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,187 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,187 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRowRDD[50] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,890 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,922 INFO datasources.FileScanRDD: TID: 70 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,924 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,928 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,930 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,934 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,966 INFO codegen.CodeGenerator: Code generated in 6.809257 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,979 INFO codegen.CodeGenerator: Code generated in 6.010991 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:55,990 INFO codegen.CodeGenerator: Code generated in 6.847466 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,008 INFO codegen.CodeGenerator: Code generated in 9.264705 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,084 INFO codegen.CodeGenerator: Code generated in 16.772683 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,363 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 70). 2166 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,420 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 71\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,420 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 71)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,421 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,421 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,424 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,425 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,426 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 21.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,442 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,442 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,444 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,445 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,445 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,470 INFO codegen.CodeGenerator: Code generated in 14.979163 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,525 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 71). 3675 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,834 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 72\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,834 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 72)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,836 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,839 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,840 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,840 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,844 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,845 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,845 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,845 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,849 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081856_0038_m_000000_0' to file:/tmp/tmplebxpja1/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,849 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081856_0038_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,850 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 72). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,908 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 73\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,908 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 73)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,909 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,913 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,914 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,915 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,917 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 73). 1607 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,958 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 74\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,958 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 74)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,959 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,959 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,963 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,964 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,964 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 179.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,972 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,972 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,974 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,975 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,975 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,976 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,977 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,978 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,979 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"labelsArray\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"elementType\" : \"string\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group labelsArray (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       optional group element (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]           optional binary element (UTF8);\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,981 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 253\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,985 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081856_0016_m_000000_74' to file:/tmp/tmplebxpja1/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,985 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081856_0016_m_000000_74: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:56,986 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 74). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,092 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 75\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,093 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 75)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,094 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,097 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,098 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,099 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,102 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,102 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,102 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,103 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,106 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081857_0046_m_000000_0' to file:/tmp/tmprkl4beue/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,106 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081857_0046_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,106 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 75). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,157 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 76\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,202 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 179.0 KiB, free 1006.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,204 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1006.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,204 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,204 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,205 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (ShuffledRowRDD[50] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,205 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,206 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 77, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,213 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,220 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,244 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 77) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,244 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,245 INFO scheduler.DAGScheduler: ResultStage 20 (parquet at OneHotEncoder.scala:408) finished in 0.057 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,245 INFO scheduler.DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,245 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,245 INFO scheduler.DAGScheduler: Job 14 finished: parquet at OneHotEncoder.scala:408, took 0.059730 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,246 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,246 INFO datasources.FileFormatWriter: Write Job e5abb371-3391-47aa-9017-9670a7c31c1c committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,246 INFO datasources.FileFormatWriter: Finished processing stats for write job e5abb371-3391-47aa-9017-9670a7c31c1c.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,330 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.226.231:41985 in memory (size: 10.9 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,331 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:46763 in memory (size: 10.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,335 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,336 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,342 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,342 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,347 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,352 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,356 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,357 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,363 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,364 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,368 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,369 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:46763 in memory (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,372 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,373 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,376 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.226.231:41985 in memory (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,377 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:46763 in memory (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,493 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,494 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,494 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,494 INFO datasources.FileSourceStrategy: Output Data Schema: struct<other_parties: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,513 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,521 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,521 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,521 INFO spark.SparkContext: Created broadcast 21 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,522 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,523 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,525 INFO scheduler.DAGScheduler: Registering RDD 56 (collect at StringIndexer.scala:204) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,525 INFO scheduler.DAGScheduler: Got map stage job 15 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,525 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,525 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,525 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,527 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[56] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,533 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.1 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,534 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,535 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.226.231:41985 (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,535 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,535 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[56] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,535 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,536 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 78, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,543 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:46763 (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,551 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,577 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 78) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,577 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,578 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (collect at StringIndexer.scala:204) finished in 0.049 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,578 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,578 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,578 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,578 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,579 INFO adaptive.ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 43.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,601 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,601 INFO scheduler.DAGScheduler: Got job 16 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,602 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,602 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,602 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,602 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[59] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,604 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 21.8 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,605 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,605 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.226.231:41985 (size: 11.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,606 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,606 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[59] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,606 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,607 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 79, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,614 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:46763 (size: 11.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,619 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,654 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 79) in 47 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,654 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,655 INFO scheduler.DAGScheduler: ResultStage 23 (collect at StringIndexer.scala:204) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,655 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,655 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,655 INFO scheduler.DAGScheduler: Job 16 finished: collect at StringIndexer.scala:204, took 0.054024 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,684 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,684 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,685 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,685 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,692 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,693 INFO scheduler.DAGScheduler: Got job 17 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,693 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,693 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,693 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,693 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[61] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,700 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 85.6 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,701 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,702 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,702 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,702 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[61] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,702 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,703 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 80, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7725 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,709 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,718 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 80) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,718 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,719 INFO scheduler.DAGScheduler: ResultStage 24 (runJob at SparkHadoopWriter.scala:78) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,719 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,719 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,719 INFO scheduler.DAGScheduler: Job 17 finished: runJob at SparkHadoopWriter.scala:78, took 0.026908 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,720 INFO io.SparkHadoopWriter: Job job_20210325081857_0061 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,741 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,742 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,748 INFO scheduler.DAGScheduler: Registering RDD 64 (parquet at StringIndexer.scala:498) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,748 INFO scheduler.DAGScheduler: Got map stage job 18 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,748 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,748 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,748 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,749 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[64] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,750 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 7.8 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,751 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,752 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,752 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,754 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[64] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,754 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,755 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 81, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7629 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,761 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,767 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 81) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,767 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,767 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (parquet at StringIndexer.scala:498) finished in 0.018 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,767 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,768 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,768 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,768 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,785 INFO spark.SparkContext: Starting job: parquet at StringIndexer.scala:498\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,785 INFO scheduler.DAGScheduler: Got job 19 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,785 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,785 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,785 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,786 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (ShuffledRowRDD[65] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,799 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 179.1 KiB, free 1007.2 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,801 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,801 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,802 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,802 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (ShuffledRowRDD[65] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,802 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,803 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 82, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,808 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,818 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,831 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 82) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,831 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,831 INFO scheduler.DAGScheduler: ResultStage 27 (parquet at StringIndexer.scala:498) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,831 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,832 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,832 INFO scheduler.DAGScheduler: Job 19 finished: parquet at StringIndexer.scala:498, took 0.046736 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,833 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,833 INFO datasources.FileFormatWriter: Write Job 9cbded68-177a-42f7-bc31-224f3b97519a committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,833 INFO datasources.FileFormatWriter: Finished processing stats for write job 9cbded68-177a-42f7-bc31-224f3b97519a.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,892 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,892 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,892 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,892 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,899 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,900 INFO scheduler.DAGScheduler: Got job 20 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,900 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,900 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,900 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,900 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[69] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,907 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 85.6 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,909 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,909 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,909 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,909 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[69] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,909 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,910 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 83, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7723 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,916 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,948 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 83) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,948 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,948 INFO scheduler.DAGScheduler: ResultStage 28 (runJob at SparkHadoopWriter.scala:78) finished in 0.047 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,948 INFO scheduler.DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,948 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,948 INFO scheduler.DAGScheduler: Job 20 finished: runJob at SparkHadoopWriter.scala:78, took 0.048951 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,949 INFO io.SparkHadoopWriter: Job job_20210325081857_0069 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,971 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,972 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,980 INFO scheduler.DAGScheduler: Registering RDD 72 (parquet at OneHotEncoder.scala:408) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,980 INFO scheduler.DAGScheduler: Got map stage job 21 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,980 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,980 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,980 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,981 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[72] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,984 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 7.8 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,984 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,985 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,985 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,985 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[72] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,985 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,986 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 84, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7549 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,993 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 84) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (parquet at OneHotEncoder.scala:408) finished in 0.017 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:57,998 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,015 INFO spark.SparkContext: Starting job: parquet at OneHotEncoder.scala:408\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,016 INFO scheduler.DAGScheduler: Got job 22 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,016 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,016 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,016 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,016 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (ShuffledRowRDD[73] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,030 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 179.0 KiB, free 1006.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,032 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1006.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,032 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,032 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,032 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (ShuffledRowRDD[73] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,032 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,033 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 85, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,039 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,047 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,058 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 85) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,058 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,058 INFO scheduler.DAGScheduler: ResultStage 31 (parquet at OneHotEncoder.scala:408) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,058 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,058 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,059 INFO scheduler.DAGScheduler: Job 22 finished: parquet at OneHotEncoder.scala:408, took 0.043259 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,059 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,060 INFO datasources.FileFormatWriter: Write Job c47f9a9d-aa29-4b90-85f9-6d97f6ac9da3 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,060 INFO datasources.FileFormatWriter: Finished processing stats for write job c47f9a9d-aa29-4b90-85f9-6d97f6ac9da3.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,158 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 76)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,159 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,162 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,163 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,163 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,166 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 76). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,207 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 77\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,208 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 77)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,208 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,209 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,212 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,213 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,214 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 179.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,220 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,220 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,221 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,222 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,222 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,223 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,223 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,223 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,224 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,224 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,224 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,224 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,224 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,224 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,225 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,226 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"categorySizes\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : \"integer\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group categorySizes (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       required int32 element;\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,235 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,241 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081857_0020_m_000000_77' to file:/tmp/tmprkl4beue/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,242 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081857_0020_m_000000_77: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,242 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 77). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,537 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 78\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,538 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 78)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,539 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,542 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,543 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,544 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,546 INFO datasources.FileScanRDD: TID: 78 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,548 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,550 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,551 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,555 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,575 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 78). 2166 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,608 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 79\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,608 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 79)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,609 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,610 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,614 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,615 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,616 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 21.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,618 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,619 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,620 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,621 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,621 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,652 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 79). 3547 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,704 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 80\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,705 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 80)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,706 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,709 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,710 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,710 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,713 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,714 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,714 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,714 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,716 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081857_0061_m_000000_0' to file:/tmp/tmpo4leu3r5/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,717 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081857_0061_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,717 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 80). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,756 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 81\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,756 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 81)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,757 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,760 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,761 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,762 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,764 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 81). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,804 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 82\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,804 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 82)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,804 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,805 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,808 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,809 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,809 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 179.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,818 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,818 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,819 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,820 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,820 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,821 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,821 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,821 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,821 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,821 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,822 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,822 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,822 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,823 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,824 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"labelsArray\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"elementType\" : \"string\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group labelsArray (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       optional group element (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]           optional binary element (UTF8);\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,826 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,829 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081857_0027_m_000000_82' to file:/tmp/tmpo4leu3r5/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,829 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081857_0027_m_000000_82: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,829 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 82). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,911 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 83\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,912 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 83)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,913 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,916 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,917 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,917 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,938 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,938 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,938 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,939 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,944 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081857_0069_m_000000_0' to file:/tmp/tmps1y5opx3/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,944 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081857_0069_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,945 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 83). 1201 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,987 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 84\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,987 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 84)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,988 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,992 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,993 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,994 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:57,996 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 84). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,034 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 85\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,035 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 85)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,035 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,036 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,039 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,040 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,040 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 179.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,047 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 7, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,047 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,048 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,049 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,049 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,050 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,051 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,052 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"categorySizes\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : \"integer\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,245 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,247 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:46763 in memory (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,251 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:46763 in memory (size: 11.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,251 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.226.231:41985 in memory (size: 11.0 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,253 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,253 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,255 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,256 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,258 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,259 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,260 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,260 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,262 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,263 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,264 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,265 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,267 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.226.231:41985 in memory (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,267 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:46763 in memory (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,284 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,284 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,284 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,284 INFO datasources.FileSourceStrategy: Output Data Schema: struct<other_installment_plans: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,301 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,309 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,309 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,309 INFO spark.SparkContext: Created broadcast 30 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,310 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,310 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,312 INFO scheduler.DAGScheduler: Registering RDD 79 (collect at StringIndexer.scala:204) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,313 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,313 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 32 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,313 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,313 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,313 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[79] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,319 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 19.1 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,320 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,321 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.226.231:41985 (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,321 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,321 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[79] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,321 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,322 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 86, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,328 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:46763 (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,336 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,366 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 86) in 44 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,366 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,367 INFO scheduler.DAGScheduler: ShuffleMapStage 32 (collect at StringIndexer.scala:204) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,367 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,367 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,367 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,367 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,368 INFO adaptive.ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 43.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,387 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,388 INFO scheduler.DAGScheduler: Got job 24 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,388 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,388 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,388 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,388 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[82] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,390 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 21.8 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,391 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,391 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.226.231:41985 (size: 11.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,391 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,391 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[82] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,391 INFO cluster.YarnScheduler: Adding task set 34.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,392 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 87, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,400 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:46763 (size: 11.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,403 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,430 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 87) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,430 INFO cluster.YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,430 INFO scheduler.DAGScheduler: ResultStage 34 (collect at StringIndexer.scala:204) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,430 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,430 INFO cluster.YarnScheduler: Killing all running tasks in stage 34: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,431 INFO scheduler.DAGScheduler: Job 24 finished: collect at StringIndexer.scala:204, took 0.043401 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,458 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,458 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,458 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,458 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,465 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,466 INFO scheduler.DAGScheduler: Got job 25 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,466 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,466 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,466 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,466 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[84] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,473 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 85.6 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,474 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,474 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,475 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,475 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[84] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,475 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,476 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 88, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7735 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,482 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,491 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 88) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,491 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,492 INFO scheduler.DAGScheduler: ResultStage 35 (runJob at SparkHadoopWriter.scala:78) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,492 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,492 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,492 INFO scheduler.DAGScheduler: Job 25 finished: runJob at SparkHadoopWriter.scala:78, took 0.026535 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,493 INFO io.SparkHadoopWriter: Job job_20210325081858_0084 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,514 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,514 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,515 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,515 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,515 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,515 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,515 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,515 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,523 INFO scheduler.DAGScheduler: Registering RDD 87 (parquet at StringIndexer.scala:498) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,523 INFO scheduler.DAGScheduler: Got map stage job 26 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,523 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 36 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,523 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,523 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,523 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[87] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,526 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 7.8 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,527 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,527 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,527 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,528 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[87] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,528 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,529 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 89, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7613 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,534 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 89) in 11 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (parquet at StringIndexer.scala:498) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,539 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,556 INFO spark.SparkContext: Starting job: parquet at StringIndexer.scala:498\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,556 INFO scheduler.DAGScheduler: Got job 27 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,556 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,556 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,556 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,556 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (ShuffledRowRDD[88] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,570 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 179.1 KiB, free 1007.2 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,571 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,571 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,572 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,572 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (ShuffledRowRDD[88] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,572 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,573 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 90, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,578 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,585 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,597 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 90) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,597 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,597 INFO scheduler.DAGScheduler: ResultStage 38 (parquet at StringIndexer.scala:498) finished in 0.040 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,597 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,597 INFO cluster.YarnScheduler: Killing all running tasks in stage 38: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,598 INFO scheduler.DAGScheduler: Job 27 finished: parquet at StringIndexer.scala:498, took 0.041754 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,598 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,598 INFO datasources.FileFormatWriter: Write Job 467cc506-baa9-4ce3-b522-20723910f100 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,599 INFO datasources.FileFormatWriter: Finished processing stats for write job 467cc506-baa9-4ce3-b522-20723910f100.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,669 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,669 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,669 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,669 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,677 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,677 INFO scheduler.DAGScheduler: Got job 28 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,677 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,677 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,677 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,677 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[92] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,684 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 85.6 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,685 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,686 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,686 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,686 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[92] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,686 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,687 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 91, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7723 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,692 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,701 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 91) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,701 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,701 INFO scheduler.DAGScheduler: ResultStage 39 (runJob at SparkHadoopWriter.scala:78) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,701 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,701 INFO cluster.YarnScheduler: Killing all running tasks in stage 39: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,702 INFO scheduler.DAGScheduler: Job 28 finished: runJob at SparkHadoopWriter.scala:78, took 0.024983 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,702 INFO io.SparkHadoopWriter: Job job_20210325081858_0092 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,722 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,723 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,729 INFO scheduler.DAGScheduler: Registering RDD 95 (parquet at OneHotEncoder.scala:408) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,729 INFO scheduler.DAGScheduler: Got map stage job 29 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,729 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 40 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,729 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,729 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,730 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[95] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,731 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 7.8 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,732 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,732 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,733 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,733 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[95] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,733 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,734 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 92, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7549 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,739 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,743 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 92) in 10 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,743 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,744 INFO scheduler.DAGScheduler: ShuffleMapStage 40 (parquet at OneHotEncoder.scala:408) finished in 0.014 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,744 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,744 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,744 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,744 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,760 INFO spark.SparkContext: Starting job: parquet at OneHotEncoder.scala:408\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,761 INFO scheduler.DAGScheduler: Got job 30 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,761 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,761 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,761 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,761 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (ShuffledRowRDD[96] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,774 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 179.0 KiB, free 1006.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,776 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 64.3 KiB, free 1006.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,776 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.226.231:41985 (size: 64.3 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,776 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,777 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (ShuffledRowRDD[96] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,777 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,777 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 93, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,783 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:46763 (size: 64.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,790 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,800 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 93) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,800 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,800 INFO scheduler.DAGScheduler: ResultStage 42 (parquet at OneHotEncoder.scala:408) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,800 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,800 INFO cluster.YarnScheduler: Killing all running tasks in stage 42: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,800 INFO scheduler.DAGScheduler: Job 30 finished: parquet at OneHotEncoder.scala:408, took 0.039887 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,801 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,801 INFO datasources.FileFormatWriter: Write Job 5cb6e8f4-de3d-4179-836f-d535003349ef committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:58,802 INFO datasources.FileFormatWriter: Finished processing stats for write job 5cb6e8f4-de3d-4179-836f-d535003349ef.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,019 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,019 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,019 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,019 INFO datasources.FileSourceStrategy: Output Data Schema: struct<housing: string>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,023 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,027 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:46763 in memory (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,032 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.226.231:41985 in memory (size: 64.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,032 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:46763 in memory (size: 64.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,037 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,038 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,040 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.226.231:41985 in memory (size: 9.6 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,041 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:46763 in memory (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,043 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,044 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,045 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.226.231:41985 in memory (size: 11.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,046 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:46763 in memory (size: 11.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,049 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,050 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,052 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,053 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,053 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,054 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,055 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,063 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,063 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,063 INFO spark.SparkContext: Created broadcast 39 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,064 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,064 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,067 INFO scheduler.DAGScheduler: Registering RDD 102 (collect at StringIndexer.scala:204) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,067 INFO scheduler.DAGScheduler: Got map stage job 31 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,067 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 43 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,067 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,067 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,067 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[102] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,075 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 19.1 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,076 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,077 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.226.231:41985 (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,077 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,077 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[102] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,077 INFO cluster.YarnScheduler: Adding task set 43.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,078 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 94, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,083 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:46763 (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,090 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,110 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 94) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,110 INFO cluster.YarnScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,111 INFO scheduler.DAGScheduler: ShuffleMapStage 43 (collect at StringIndexer.scala:204) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,111 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,111 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,111 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,111 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,112 INFO adaptive.ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 43.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,131 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,131 INFO scheduler.DAGScheduler: Got job 32 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,131 INFO scheduler.DAGScheduler: Final stage: ResultStage 45 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,131 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,131 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,132 INFO scheduler.DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[105] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,133 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 21.8 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,134 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,134 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.226.231:41985 (size: 11.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,134 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,135 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[105] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,135 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,135 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 95, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,141 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:46763 (size: 11.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,144 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,167 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 95) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,167 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,167 INFO scheduler.DAGScheduler: ResultStage 45 (collect at StringIndexer.scala:204) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,167 INFO scheduler.DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,167 INFO cluster.YarnScheduler: Killing all running tasks in stage 45: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,168 INFO scheduler.DAGScheduler: Job 32 finished: collect at StringIndexer.scala:204, took 0.036931 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,193 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,193 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,193 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,193 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,201 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group categorySizes (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       required int32 element;\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,053 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,056 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081858_0031_m_000000_85' to file:/tmp/tmps1y5opx3/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,056 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081858_0031_m_000000_85: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,057 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 85). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,323 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 86\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,323 INFO executor.Executor: Running task 0.0 in stage 32.0 (TID 86)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,324 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,327 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,328 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,328 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 19.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,330 INFO datasources.FileScanRDD: TID: 86 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,332 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,335 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,201 INFO scheduler.DAGScheduler: Got job 33 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,201 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,201 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,337 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,340 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,365 INFO executor.Executor: Finished task 0.0 in stage 32.0 (TID 86). 2166 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,395 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 87\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,395 INFO executor.Executor: Running task 0.0 in stage 34.0 (TID 87)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,396 INFO spark.MapOutputTrackerWorker: Updating epoch to 9 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,397 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,399 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,201 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,400 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,401 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 21.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,403 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,403 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,404 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,405 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,405 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,428 INFO executor.Executor: Finished task 0.0 in stage 34.0 (TID 87). 3538 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,477 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 88\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,477 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 88)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,478 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,481 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,482 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,483 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,486 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,486 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,486 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,201 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[107] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,486 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,489 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081858_0084_m_000000_0' to file:/tmp/tmpus1i75vs/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,489 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081858_0084_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,490 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 88). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,530 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 89\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,530 INFO executor.Executor: Running task 0.0 in stage 36.0 (TID 89)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,531 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 34 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,534 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,535 INFO broadcast.TorrentBroadcast: Reading broadcast variable 34 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,535 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,538 INFO executor.Executor: Finished task 0.0 in stage 36.0 (TID 89). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,574 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 90\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,574 INFO executor.Executor: Running task 0.0 in stage 38.0 (TID 90)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,574 INFO spark.MapOutputTrackerWorker: Updating epoch to 10 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,575 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 35 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,578 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,578 INFO broadcast.TorrentBroadcast: Reading broadcast variable 35 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,579 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 179.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,585 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,585 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,586 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,587 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,587 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,588 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,588 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,588 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,588 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,588 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,588 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,589 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,590 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,590 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,590 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,590 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,590 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,590 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"labelsArray\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"elementType\" : \"string\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group labelsArray (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       optional group element (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]           optional binary element (UTF8);\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,592 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,595 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081858_0038_m_000000_90' to file:/tmp/tmpus1i75vs/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,595 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081858_0038_m_000000_90: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,595 INFO executor.Executor: Finished task 0.0 in stage 38.0 (TID 90). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,688 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 91\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,688 INFO executor.Executor: Running task 0.0 in stage 39.0 (TID 91)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,689 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,692 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,693 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,693 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,696 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,697 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,697 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,697 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,699 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081858_0092_m_000000_0' to file:/tmp/tmpc_f9km9h/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,699 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081858_0092_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,700 INFO executor.Executor: Finished task 0.0 in stage 39.0 (TID 91). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,735 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 92\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,735 INFO executor.Executor: Running task 0.0 in stage 40.0 (TID 92)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,736 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,738 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,740 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,740 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,742 INFO executor.Executor: Finished task 0.0 in stage 40.0 (TID 92). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,778 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 93\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,779 INFO executor.Executor: Running task 0.0 in stage 42.0 (TID 93)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,779 INFO spark.MapOutputTrackerWorker: Updating epoch to 11 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,780 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,782 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 64.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,783 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,784 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 179.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,789 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,789 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,790 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,791 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,791 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,792 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,793 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,794 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"categorySizes\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : \"integer\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group categorySizes (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       required int32 element;\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,795 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,798 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081858_0042_m_000000_93' to file:/tmp/tmpc_f9km9h/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,798 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081858_0042_m_000000_93: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:58,798 INFO executor.Executor: Finished task 0.0 in stage 42.0 (TID 93). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,079 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 94\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,079 INFO executor.Executor: Running task 0.0 in stage 43.0 (TID 94)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,080 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,083 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,083 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,084 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 19.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,086 INFO datasources.FileScanRDD: TID: 94 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,087 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 39 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,089 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,090 INFO broadcast.TorrentBroadcast: Reading broadcast variable 39 took 3 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,208 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 85.6 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,210 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,210 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,210 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,210 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[107] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,210 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,211 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 96, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7719 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,216 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,225 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 96) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,225 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,225 INFO scheduler.DAGScheduler: ResultStage 46 (runJob at SparkHadoopWriter.scala:78) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,225 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,225 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,225 INFO scheduler.DAGScheduler: Job 33 finished: runJob at SparkHadoopWriter.scala:78, took 0.024463 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,226 INFO io.SparkHadoopWriter: Job job_20210325081859_0107 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,245 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,245 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,246 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,246 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,246 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,246 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,246 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,246 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,252 INFO scheduler.DAGScheduler: Registering RDD 110 (parquet at StringIndexer.scala:498) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,252 INFO scheduler.DAGScheduler: Got map stage job 34 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,252 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,252 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,252 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,252 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[110] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,254 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 7.8 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,255 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,255 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,255 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,255 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[110] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,255 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,256 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 97, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7613 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,261 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,265 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 97) in 9 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,265 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,266 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (parquet at StringIndexer.scala:498) finished in 0.014 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,266 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,266 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,266 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,266 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,284 INFO spark.SparkContext: Starting job: parquet at StringIndexer.scala:498\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,285 INFO scheduler.DAGScheduler: Got job 35 (parquet at StringIndexer.scala:498) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,285 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (parquet at StringIndexer.scala:498)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,285 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,285 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,285 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (ShuffledRowRDD[111] at parquet at StringIndexer.scala:498), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,298 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 179.1 KiB, free 1007.2 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,299 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,300 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,300 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,300 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (ShuffledRowRDD[111] at parquet at StringIndexer.scala:498) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,300 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,301 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 98, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,306 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,313 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,323 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 98) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,323 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,323 INFO scheduler.DAGScheduler: ResultStage 49 (parquet at StringIndexer.scala:498) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,323 INFO scheduler.DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,323 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,323 INFO scheduler.DAGScheduler: Job 35 finished: parquet at StringIndexer.scala:498, took 0.039364 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,324 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,324 INFO datasources.FileFormatWriter: Write Job 5d94c3fa-e1ac-4e12-86e0-1dae0df0828e committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,325 INFO datasources.FileFormatWriter: Finished processing stats for write job 5d94c3fa-e1ac-4e12-86e0-1dae0df0828e.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,384 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,384 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,384 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,384 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,392 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,394 INFO scheduler.DAGScheduler: Got job 36 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,394 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,394 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,394 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,394 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[115] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,401 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 85.6 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,402 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,402 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.226.231:41985 (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,402 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,403 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[115] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,403 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,404 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 99, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7723 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,409 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:46763 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,417 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 99) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,417 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,417 INFO scheduler.DAGScheduler: ResultStage 50 (runJob at SparkHadoopWriter.scala:78) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,417 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,417 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,417 INFO scheduler.DAGScheduler: Job 36 finished: runJob at SparkHadoopWriter.scala:78, took 0.025630 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,418 INFO io.SparkHadoopWriter: Job job_20210325081859_0115 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,435 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,435 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,435 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,435 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,436 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,436 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,436 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,436 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,442 INFO scheduler.DAGScheduler: Registering RDD 118 (parquet at OneHotEncoder.scala:408) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,442 INFO scheduler.DAGScheduler: Got map stage job 37 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,442 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,442 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,442 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,442 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[118] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,444 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 7.8 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,445 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,445 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.226.231:41985 (size: 4.3 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,445 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,445 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[118] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,445 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,446 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 100, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7549 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,451 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:46763 (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 100) in 9 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (parquet at OneHotEncoder.scala:408) finished in 0.012 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,455 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,473 INFO spark.SparkContext: Starting job: parquet at OneHotEncoder.scala:408\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,473 INFO scheduler.DAGScheduler: Got job 38 (parquet at OneHotEncoder.scala:408) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,473 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (parquet at OneHotEncoder.scala:408)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,473 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,473 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,473 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (ShuffledRowRDD[119] at parquet at OneHotEncoder.scala:408), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,487 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 179.0 KiB, free 1006.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,488 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 1006.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,488 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.226.231:41985 (size: 64.2 KiB, free: 1007.6 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,489 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,491 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (ShuffledRowRDD[119] at parquet at OneHotEncoder.scala:408) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,491 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,492 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 101, algo-1, executor 1, partition 0, NODE_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,497 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:46763 (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,504 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,514 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 101) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,514 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,514 INFO scheduler.DAGScheduler: ResultStage 53 (parquet at OneHotEncoder.scala:408) finished in 0.040 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,514 INFO scheduler.DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,514 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,514 INFO scheduler.DAGScheduler: Job 38 finished: parquet at OneHotEncoder.scala:408, took 0.041647 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,515 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,515 INFO datasources.FileFormatWriter: Write Job 0083bfe2-13a7-469c-957c-92175ff692c0 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:18:59,516 INFO datasources.FileFormatWriter: Finished processing stats for write job 0083bfe2-13a7-469c-957c-92175ff692c0.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,024 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,025 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,032 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.226.231:41985 in memory (size: 29.9 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,032 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:46763 in memory (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,033 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,035 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,036 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.226.231:41985 in memory (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,037 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:46763 in memory (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,038 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.226.231:41985 in memory (size: 9.6 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,039 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:46763 in memory (size: 9.6 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,040 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,041 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,043 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.226.231:41985 in memory (size: 4.3 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,044 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:46763 in memory (size: 4.3 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,045 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.226.231:41985 in memory (size: 64.2 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,046 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:46763 in memory (size: 64.2 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,047 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.226.231:41985 in memory (size: 11.0 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,048 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:46763 in memory (size: 11.0 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,085 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,101 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:00,101 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,094 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,109 INFO executor.Executor: Finished task 0.0 in stage 43.0 (TID 94). 2166 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,136 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 95\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,136 INFO executor.Executor: Running task 0.0 in stage 45.0 (TID 95)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,137 INFO spark.MapOutputTrackerWorker: Updating epoch to 12 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,137 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 41 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,140 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,141 INFO broadcast.TorrentBroadcast: Reading broadcast variable 41 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,142 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 21.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,144 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 11, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,144 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,145 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,145 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,145 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,165 INFO executor.Executor: Finished task 0.0 in stage 45.0 (TID 95). 3546 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,212 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 96\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,212 INFO executor.Executor: Running task 0.0 in stage 46.0 (TID 96)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,213 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 42 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,215 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,216 INFO broadcast.TorrentBroadcast: Reading broadcast variable 42 took 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,217 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,220 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,220 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,220 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,220 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,223 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081859_0107_m_000000_0' to file:/tmp/tmph89u4esx/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,223 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081859_0107_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,224 INFO executor.Executor: Finished task 0.0 in stage 46.0 (TID 96). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,257 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 97\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,257 INFO executor.Executor: Running task 0.0 in stage 47.0 (TID 97)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,259 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 43 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,261 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,262 INFO broadcast.TorrentBroadcast: Reading broadcast variable 43 took 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,262 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,264 INFO executor.Executor: Finished task 0.0 in stage 47.0 (TID 97). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,302 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 98\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,302 INFO executor.Executor: Running task 0.0 in stage 49.0 (TID 98)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,303 INFO spark.MapOutputTrackerWorker: Updating epoch to 13 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,303 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 44 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,305 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,306 INFO broadcast.TorrentBroadcast: Reading broadcast variable 44 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,307 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 179.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,313 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 12, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,313 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,314 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,314 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,314 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,315 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,316 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,317 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"labelsArray\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"elementType\" : \"string\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : true\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group labelsArray (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       optional group element (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]           optional binary element (UTF8);\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]         }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,318 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,321 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081859_0049_m_000000_98' to file:/tmp/tmph89u4esx/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,321 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081859_0049_m_000000_98: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,321 INFO executor.Executor: Finished task 0.0 in stage 49.0 (TID 98). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,405 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 99\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,405 INFO executor.Executor: Running task 0.0 in stage 50.0 (TID 99)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,406 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 45 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,408 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,409 INFO broadcast.TorrentBroadcast: Reading broadcast variable 45 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,410 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 85.6 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,413 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,413 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,413 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,413 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,415 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081859_0115_m_000000_0' to file:/tmp/tmpxjntdegl/model/metadata\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,415 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081859_0115_m_000000_0: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,416 INFO executor.Executor: Finished task 0.0 in stage 50.0 (TID 99). 1158 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,447 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,447 INFO executor.Executor: Running task 0.0 in stage 51.0 (TID 100)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,448 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 46 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,451 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,451 INFO broadcast.TorrentBroadcast: Reading broadcast variable 46 took 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,452 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 7.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,454 INFO executor.Executor: Finished task 0.0 in stage 51.0 (TID 100). 1608 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,493 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 101\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,493 INFO executor.Executor: Running task 0.0 in stage 53.0 (TID 101)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,494 INFO spark.MapOutputTrackerWorker: Updating epoch to 14 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,494 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 47 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,496 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,497 INFO broadcast.TorrentBroadcast: Reading broadcast variable 47 took 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,498 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 179.0 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,503 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 13, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,503 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,504 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,505 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,505 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,506 INFO codec.CodecConfig: Compression: SNAPPY\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Dictionary is on\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Validation is off\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,507 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,508 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"type\" : \"struct\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   \"fields\" : [ {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"name\" : \"categorySizes\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"type\" : {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"type\" : \"array\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"elementType\" : \"integer\",\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       \"containsNull\" : false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     },\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"nullable\" : true,\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     \"metadata\" : { }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   } ]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] and corresponding Parquet message type:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] message spark_schema {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   optional group categorySizes (LIST) {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     repeated group list {\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]       required int32 element;\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]     }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]   }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] }\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,041 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,042 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,042 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,042 INFO datasources.FileSourceStrategy: Output Data Schema: struct<txn_id: string, txn_timestamp: string, checking_acct_status: string, duration_months: string, credit_history: string ... 21 more fields>\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,184 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,184 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,184 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:01,184 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,021 INFO codegen.CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext is 12002 bytes\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,021 INFO codegen.CodeGenerator: Code generated in 139.963433 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,024 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 313.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,033 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,033 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.226.231:41985 (size: 29.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,034 INFO spark.SparkContext: Created broadcast 48 from csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,035 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,035 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,134 INFO scheduler.DAGScheduler: Registering RDD 127 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,134 INFO scheduler.DAGScheduler: Got map stage job 39 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,134 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 54 (csv at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,134 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,135 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,135 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[127] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,140 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 121.4 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,142 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 1007.4 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,142 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.226.231:41985 (size: 36.1 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,142 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,142 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[127] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,143 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,143 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 102, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7744 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:02,150 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:46763 (size: 36.1 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,167 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:46763 (size: 29.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,198 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:46763 (size: 266.0 B, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr]        \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,509 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,512 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081859_0053_m_000000_101' to file:/tmp/tmpxjntdegl/model/data\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,512 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081859_0053_m_000000_101: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:18:59,513 INFO executor.Executor: Finished task 0.0 in stage 53.0 (TID 101). 3195 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,145 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 102\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,145 INFO executor.Executor: Running task 0.0 in stage 54.0 (TID 102)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,146 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 49 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,149 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,150 INFO broadcast.TorrentBroadcast: Reading broadcast variable 49 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,151 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 121.4 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:02,690 INFO codegen.CodeGenerator: Code generated in 14.411437 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,127 INFO datasources.FileScanRDD: TID: 102 - Reading current file: path: file:///opt/ml/processing/german.csv/german.csv, range: 0-152051, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,161 INFO codegen.CodeGenerator: Code generated in 24.406951 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,163 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 48 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,164 INFO codegen.CodeGenerator: Code generated in 27.258532 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,166 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,168 INFO broadcast.TorrentBroadcast: Reading broadcast variable 48 took 4 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,828 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 102) in 1685 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,829 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,829 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 38451\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,830 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (csv at NativeMethodAccessorImpl.java:0) finished in 1.695 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,831 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,831 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,831 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,831 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,836 INFO adaptive.ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 15068.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,890 INFO codegen.CodeGenerator: Code generated in 18.258102 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,911 INFO codegen.CodeGenerator: Code generated in 12.557809 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:03,931 INFO codegen.CodeGenerator: Code generated in 13.504268 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,001 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,002 INFO scheduler.DAGScheduler: Got job 40 (csv at NativeMethodAccessorImpl.java:0) with 5 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,002 INFO scheduler.DAGScheduler: Final stage: ResultStage 56 (csv at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,002 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,002 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,002 INFO scheduler.DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[133] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,020 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 282.3 KiB, free 1007.1 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,022 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 98.5 KiB, free 1007.0 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,022 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.226.231:41985 (size: 98.5 KiB, free: 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,022 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,023 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 56 (MapPartitionsRDD[133] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,023 INFO cluster.YarnScheduler: Adding task set 56.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,024 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 103, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,024 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 56.0 (TID 104, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,024 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 56.0 (TID 105, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,024 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 56.0 (TID 106, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,024 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 56.0 (TID 107, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7336 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,032 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:46763 (size: 98.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:04,129 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.226.231:40028\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,174 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 561.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,194 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,197 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 266.0 B, free 28.8 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,199 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,200 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 920.0 B, free 28.8 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,301 INFO codegen.CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext is 12002 bytes\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,302 INFO codegen.CodeGenerator: Code generated in 135.880949 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,337 INFO codegen.CodeGenerator: Code generated in 15.700756 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,813 INFO python.PythonUDFRunner: Times: total = 854, boot = 402, init = 212, finish = 240\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:03,826 INFO executor.Executor: Finished task 0.0 in stage 54.0 (TID 102). 4802 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,025 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 103\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 104\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.Executor: Running task 0.0 in stage 56.0 (TID 103)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 105\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.Executor: Running task 1.0 in stage 56.0 (TID 104)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 106\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.Executor: Running task 2.0 in stage 56.0 (TID 105)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.Executor: Running task 3.0 in stage 56.0 (TID 106)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 107\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO executor.Executor: Running task 4.0 in stage 56.0 (TID 107)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,026 INFO spark.MapOutputTrackerWorker: Updating epoch to 15 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,027 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 50 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,031 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 98.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,032 INFO broadcast.TorrentBroadcast: Reading broadcast variable 50 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,033 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 282.3 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,129 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 14, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,129 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.226.231:40703)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,129 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 14, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,131 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,131 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,131 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (26.2 KiB) non-empty blocks including 1 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,131 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,132 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,133 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,133 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,137 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (61.8 KiB) non-empty blocks including 1 (61.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,137 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:05,638 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 56.0 (TID 105) in 1614 ms on algo-1 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:05,639 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 56.0 (TID 107) in 1615 ms on algo-1 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,138 INFO storage.ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) [/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:44.695+0000: [GC (Allocation Failure) [PSYoungGen: 252416K->13111K(294400K)] 252416K->13135K(967680K), 0.0089865 secs] [Times: user=0.04 sys=0.01, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:45.171+0000: [GC (Metadata GC Threshold) [PSYoungGen: 161575K->11948K(294400K)] 161599K->11980K(967680K), 0.0060671 secs] [Times: user=0.03 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:45.177+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 11948K->0K(294400K)] [ParOldGen: 32K->11473K(393216K)] 11980K->11473K(687616K), [Metaspace: 20348K->20348K(22528K)], 0.0194615 secs] [Times: user=0.10 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:45.993+0000: [GC (Allocation Failure) [PSYoungGen: 252416K->10646K(294400K)] 263889K->22128K(687616K), 0.0047773 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:46.231+0000: [GC (Metadata GC Threshold) [PSYoungGen: 118440K->8770K(409088K)] 129922K->20260K(802304K), 0.0044405 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:46.235+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8770K->0K(409088K)] [ParOldGen: 11489K->16689K(623104K)] 20260K->16689K(1032192K), [Metaspace: 33980K->33978K(36864K)], 0.0223829 secs] [Times: user=0.11 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:49.527+0000: [GC (Allocation Failure) [PSYoungGen: 367104K->17538K(409088K)] 383793K->34236K(1032192K), 0.0096726 secs] [Times: user=0.03 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:51.675+0000: [GC (Metadata GC Threshold) [PSYoungGen: 162991K->16593K(504832K)] 179688K->33299K(1127936K), 0.0077746 secs] [Times: user=0.04 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:51.683+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 16593K->0K(504832K)] [ParOldGen: 16705K->29987K(877568K)] 33299K->29987K(1382400K), [Metaspace: 54493K->54493K(59392K)], 0.0722384 secs] [Times: user=0.54 sys=0.01, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:54.547+0000: [GC (Allocation Failure) [PSYoungGen: 487936K->19954K(516608K)] 517923K->49979K(1394176K), 0.0124571 secs] [Times: user=0.05 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:18:57.919+0000: [GC (Allocation Failure) [PSYoungGen: 516594K->22527K(658944K)] 546619K->56817K(1536512K), 0.0162457 secs] [Times: user=0.06 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:19:03.577+0000: [GC (Metadata GC Threshold) [PSYoungGen: 490524K->26370K(663552K)] 524813K->257284K(1541120K), 0.0623236 secs] [Times: user=0.54 sys=0.10, real=0.06 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdout] 2021-03-25T08:19:03.639+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 26370K->0K(663552K)] [ParOldGen: 230913K->115780K(1363456K)] 257284K->115780K(2027008K), [Metaspace: 88990K->87938K(100352K)], 0.0733859 secs] [Times: user=0.25 sys=0.07, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_0000host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,138 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,149 INFO codegen.CodeGenerator: Code generated in 17.361392 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,170 INFO codegen.CodeGenerator: Code generated in 11.303564 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,190 INFO codegen.CodeGenerator: Code generated in 5.307643 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,230 INFO codegen.CodeGenerator: Code generated in 13.003004 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,241 INFO codegen.CodeGenerator: Code generated in 6.010871 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,265 INFO codegen.CodeGenerator: Code generated in 5.564788 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,277 INFO codegen.CodeGenerator: Code generated in 6.583089 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,283 INFO codegen.CodeGenerator: Code generated in 4.245399 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,300 INFO codegen.CodeGenerator: Code generated in 15.16067 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,333 INFO codegen.CodeGenerator: Code generated in 18.948152 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,352 INFO codegen.CodeGenerator: Code generated in 5.791562 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,365 INFO codegen.CodeGenerator: Code generated in 7.71126 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,374 INFO codegen.CodeGenerator: Code generated in 5.72409 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,390 INFO codegen.CodeGenerator: Code generated in 13.334813 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,534 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,555 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:04,555 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,486 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,486 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,486 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,486 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,486 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,487 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,487 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,487 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,487 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,487 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,487 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,488 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,489 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,489 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,201 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 103) in 3178 ms on algo-1 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,415 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 56.0 (TID 104) in 3391 ms on algo-1 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,518 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 56.0 (TID 106) in 3494 ms on algo-1 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,518 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,519 INFO scheduler.DAGScheduler: ResultStage 56 (csv at NativeMethodAccessorImpl.java:0) finished in 3.515 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,519 INFO scheduler.DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,519 INFO cluster.YarnScheduler: Killing all running tasks in stage 56: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,519 INFO scheduler.DAGScheduler: Job 40 finished: csv at NativeMethodAccessorImpl.java:0, took 3.518256 s\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,928 INFO datasources.FileFormatWriter: Write Job b73754a5-5832-43b2-8599-959825641039 committed.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:07,929 INFO datasources.FileFormatWriter: Finished processing stats for write job b73754a5-5832-43b2-8599-959825641039.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,200 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.0.226.231:41985 in memory (size: 98.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,202 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:46763 in memory (size: 98.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,203 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.0.226.231:41985 in memory (size: 36.1 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,205 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:46763 in memory (size: 36.1 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stdeINFO:sagemaker_dataprep:{\"event_type\": \"mohave.backend.spark.performance\", \"request_context\": {\"client_request_id\": null}, \"app_context\": {\"wrangler_version\": \"1.3.1\", \"spark_version\": \"3.0.0+amzn.0\"}, \"engine\": \"spark\", \"metadata\": {\"operator_name\": \"sagemaker.spark.manage_columns_0.1\", \"disable_limits\": false}, \"spark\": {\"stage_ids\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56], \"stages\": [{\"stage_id\": 0, \"task_metrics\": [{\"executor_runtime\": 699.0, \"executor_deserialize_time\": 625.0, \"scheduler_delay\": 60.0, \"gc_time\": 9.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1903.0, \"bytes_read\": 65536.0, \"records_read\": 1.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 1, \"task_metrics\": [{\"executor_runtime\": 408.0, \"executor_deserialize_time\": 1451.0, \"scheduler_delay\": 10.0, \"gc_time\": 81.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1665.0, \"bytes_read\": 152051.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 2, \"task_metrics\": [{\"executor_runtime\": 65.0, \"executor_deserialize_time\": 61.0, \"scheduler_delay\": 13.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 2488.0, \"result_size\": 1929.0, \"bytes_read\": 206808.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 3, \"task_metrics\": [{\"executor_runtime\": 26.0, \"executor_deserialize_time\": 1.0, \"scheduler_delay\": 17.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1305.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 4, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 5, \"task_metrics\": [{\"executor_runtime\": 0.0, \"executor_deserialize_time\": 1.0, \"scheduler_delay\": 26.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2383.0, \"bytes_read\": 24.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 6, \"task_metrics\": [{\"executor_runtime\": 63.0, \"executor_deserialize_time\": 38.0, \"scheduler_delay\": 11.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 449.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 7, \"task_metrics\": [{\"executor_runtime\": 30.0, \"executor_deserialize_time\": 32.0, \"scheduler_delay\": 7.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 8, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 9, \"task_metrics\": [{\"executor_runtime\": 723.0, \"executor_deserialize_time\": 63.0, \"scheduler_delay\": 11.0, \"gc_time\": 12.0, \"result_serialization_time\": 2.0, \"peak_execution_memory\": 0.0, \"result_size\": 3281.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 596.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 10, \"task_metrics\": [{\"executor_runtime\": 441.0, \"executor_deserialize_time\": 39.0, \"scheduler_delay\": 5.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2166.0, \"bytes_read\": 152051.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 11, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 12, \"task_metrics\": [{\"executor_runtime\": 82.0, \"executor_deserialize_time\": 21.0, \"scheduler_delay\": 6.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3675.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 13, \"task_metrics\": [{\"executor_runtime\": 5.0, \"executor_deserialize_time\": 9.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 361.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 14, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1607.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 15, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 16, \"task_metrics\": [{\"executor_runtime\": 13.0, \"executor_deserialize_time\": 13.0, \"scheduler_delay\": 5.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 787.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 17, \"task_metrics\": [{\"executor_runtime\": 4.0, \"executor_deserialize_time\": 9.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 365.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 18, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 6.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 19, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 20, \"task_metrics\": [{\"executor_runtime\": 22.0, \"executor_deserialize_time\": 11.0, \"scheduler_delay\": 5.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 560.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 21, \"task_metrics\": [{\"executor_runtime\": 29.0, \"executor_deserialize_time\": 8.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2166.0, \"bytes_read\": 152051.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 22, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 23, \"task_metrics\": [{\"executor_runtime\": 33.0, \"executor_deserialize_time\": 9.0, \"scheduler_delay\": 5.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3547.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 24, \"task_metrics\": [{\"executor_runtime\": 3.0, \"executor_deserialize_time\": 8.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 367.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 25, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 6.0, \"scheduler_delay\": 5.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 26, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 27, \"task_metrics\": [{\"executor_runtime\": 11.0, \"executor_deserialize_time\": 13.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 668.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 28, \"task_metrics\": [{\"executor_runtime\": 6.0, \"executor_deserialize_time\": 25.0, \"scheduler_delay\": 6.0, \"gc_time\": 17.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1201.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 365.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 29, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 30, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 31, \"task_metrics\": [{\"executor_runtime\": 9.0, \"executor_deserialize_time\": 12.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 560.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 32, \"task_metrics\": [{\"executor_runtime\": 34.0, \"executor_deserialize_time\": 6.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2166.0, \"bytes_read\": 152051.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 33, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 34, \"task_metrics\": [{\"executor_runtime\": 25.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 6.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3538.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 35, \"task_metrics\": [{\"executor_runtime\": 4.0, \"executor_deserialize_time\": 8.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 377.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 36, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 6.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 37, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 38, \"task_metrics\": [{\"executor_runtime\": 10.0, \"executor_deserialize_time\": 10.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 647.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 39, \"task_metrics\": [{\"executor_runtime\": 3.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 365.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 40, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 5.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 41, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 42, \"task_metrics\": [{\"executor_runtime\": 8.0, \"executor_deserialize_time\": 10.0, \"scheduler_delay\": 5.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 560.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 43, \"task_metrics\": [{\"executor_runtime\": 23.0, \"executor_deserialize_time\": 5.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2166.0, \"bytes_read\": 152051.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 44, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 45, \"task_metrics\": [{\"executor_runtime\": 21.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3546.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 46, \"task_metrics\": [{\"executor_runtime\": 3.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 361.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 47, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 5.0, \"scheduler_delay\": 3.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 48, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 49, \"task_metrics\": [{\"executor_runtime\": 8.0, \"executor_deserialize_time\": 10.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 653.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 50, \"task_metrics\": [{\"executor_runtime\": 3.0, \"executor_deserialize_time\": 7.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1158.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 365.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 51, \"task_metrics\": [{\"executor_runtime\": 1.0, \"executor_deserialize_time\": 5.0, \"scheduler_delay\": 3.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 1608.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 52, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 53, \"task_metrics\": [{\"executor_runtime\": 9.0, \"executor_deserialize_time\": 9.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 3195.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 560.0, \"records_written\": 1.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 54, \"task_metrics\": [{\"executor_runtime\": 1271.0, \"executor_deserialize_time\": 408.0, \"scheduler_delay\": 6.0, \"gc_\u001b[0m\n",
      "\u001b[34mtime\": 135.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 67141632.0, \"result_size\": 4802.0, \"bytes_read\": 65536.0, \"records_read\": 1000.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 55, \"task_metrics\": [{\"attempt_id\": 0, \"attempt_status\": \"SKIPPED\"}]}, {\"stage_id\": 56, \"task_metrics\": [{\"executor_runtime\": 1503.0, \"executor_deserialize_time\": 102.0, \"scheduler_delay\": 4.0, \"gc_time\": 0.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 65536.0, \"result_size\": 4545.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}], \"aggregate_metrics\": [{\"executor_id\": \"driver\", \"total_runtime\": 0, \"total_storage_memory\": 1056807321, \"storage_memory_used\": 67825, \"peak_memory_metrics\": {\"storage_memory\": 904248, \"execution_memory\": 0}}, {\"executor_id\": \"1\", \"total_runtime\": 26952, \"total_storage_memory\": 30984477081, \"storage_memory_used\": 67825, \"peak_memory_metrics\": {\"storage_memory\": 67825, \"execution_memory\": 0}}]}}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,959 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,965 INFO server.AbstractConnector: Stopped Spark@a6dda1c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,966 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.226.231:4040\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,970 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,987 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:08,987 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,083 INFO launcher.ContainerLaunch: Container container_1616660305706_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,092 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,093 INFO launcher.ContainerCleanup: Cleaning up container container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,093 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,094 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,095 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,097 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,097 INFO application.ApplicationImpl: Removing container_1616660305706_0001_01_000002 from application application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,098 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1616660305706_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,098 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,100 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,100 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000002#011RESOURCE=<memory:61316, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,101 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,108 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,108 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,108 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/container_tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,108 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/container_tokens]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,108 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/sysfs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,109 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000002/sysfs]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,114 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,116 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,118 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,123 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,132 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,132 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,132 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ea931401-eb0b-406b-a889-38e05e429399\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,136 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-30f95d52-eb30-4396-b88a-8d9e13197fe4\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,139 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ea931401-eb0b-406b-a889-38e05e429399/pyspark-f0b4894d-ca10-4920-bb39-88fffd3102af\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,143 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,143 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,144 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,144 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1616660305706_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,145 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,145 INFO rmapp.RMAppImpl: Updating application application_1616660305706_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,145 INFO recovery.RMStateStore: Updating info for app: application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,145 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,145 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,145 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m03-25 08:19 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34mrr] 2021-03-25 08:19:05,635 INFO mapred.SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210325081903_0056_m_000002_105\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,635 INFO mapred.SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20210325081903_0056_m_000004_107\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,636 INFO executor.Executor: Finished task 2.0 in stage 56.0 (TID 105). 4545 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:05,636 INFO executor.Executor: Finished task 4.0 in stage 56.0 (TID 107). 4545 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,199 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081903_0056_m_000000_103' to s3a://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,199 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081903_0056_m_000000_103: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,200 INFO executor.Executor: Finished task 0.0 in stage 56.0 (TID 103). 4631 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,413 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081903_0056_m_000001_104' to s3a://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,413 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081903_0056_m_000001_104: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,414 INFO executor.Executor: Finished task 1.0 in stage 56.0 (TID 104). 4674 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,516 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210325081903_0056_m_000003_106' to s3a://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,516 INFO mapred.SparkHadoopMapRedUtil: attempt_20210325081903_0056_m_000003_106: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:07,517 INFO executor.Executor: Finished task 3.0 in stage 56.0 (TID 106). 4674 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:08,993 INFO executor.YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:09,009 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:09,011 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1616660305706_0001/container_1616660305706_0001_01_000002/stderr] 2021-03-25 08:19:09,019 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1Starting clean up\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,245 INFO resourcemanager.ApplicationMasterService: application_1616660305706_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,632 INFO launcher.ContainerLaunch: Container container_1616660305706_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,632 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,632 INFO launcher.ContainerCleanup: Cleaning up container container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,633 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,633 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,634 INFO container.ContainerImpl: Container container_1616660305706_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,634 INFO application.ApplicationImpl: Removing container_1616660305706_0001_01_000001 from application application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,634 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,634 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,636 INFO rmcontainer.RMContainerImpl: container_1616660305706_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,636 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,636 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,637 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,639 INFO attempt.RMAppAttemptImpl: appattempt_1616660305706_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,642 INFO rmapp.RMAppImpl: application_1616660305706_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,642 INFO capacity.CapacityScheduler: Application Attempt appattempt_1616660305706_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,642 INFO scheduler.AppSchedulingInfo: Application application_1616660305706_0001 requests cleared\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,642 INFO amlauncher.AMLauncher: Cleaning master appattempt_1616660305706_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,643 INFO capacity.LeafQueue: Application removed - appId: application_1616660305706_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,643 INFO capacity.ParentQueue: Application removed - appId: application_1616660305706_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,643 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1616660305706_0001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,645 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1616660305706_0001,name=processing_entrypoint.py,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1616660305706_0001/,appMasterHost=10.0.226.231,submitTime=1616660316051,startTime=1616660316113,launchTime=1616660317165,finishTime=1616660349145,finalStatus=SUCCEEDED,memorySeconds=1641605,vcoreSeconds=58,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=1641605 MB-seconds\\, 58 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,646 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,646 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,646 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/container_tokens\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,646 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/container_tokens]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,646 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/sysfs\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,646 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1616660305706_0001/container_1616660305706_0001_01_000001/sysfs]\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,650 INFO ipc.Server: Auth successful for appattempt_1616660305706_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,653 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-25 08:19:09,653 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.226.231#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1616660305706_0001#011CONTAINERID=container_1616660305706_0001_01_000001\u001b[0m\n",
      "\u001b[34mINFO:entrypoint:Processing Job Config: {\n",
      "    \"ProcessingJobArn\": \"arn:aws:sagemaker:us-east-1:867352166187:processing-job/credit-flow-2021-03-25-08-14-10\",\n",
      "    \"ProcessingJobName\": \"credit-flow-2021-03-25-08-14-10\",\n",
      "    \"AppSpecification\": {\n",
      "        \"ImageUri\": \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\",\n",
      "        \"ContainerEntrypoint\": null,\n",
      "        \"ContainerArguments\": [\n",
      "            \"--output-config '{\\\"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\\\": {\\\"content_type\\\": \\\"CSV\\\"}}'\"\n",
      "        ]\n",
      "    },\n",
      "    \"ProcessingInputs\": [\n",
      "        {\n",
      "            \"InputName\": \"flow\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "                \"LocalPath\": \"/opt/ml/processing/flow\",\n",
      "                \"S3Uri\": \"s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/flow/credit-data.flow\",\n",
      "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3InputMode\": \"File\",\n",
      "                \"S3CompressionType\": \"None\",\n",
      "                \"S3DownloadMode\": \"StartOfJob\"\n",
      "            },\n",
      "            \"DatasetDefinition\": null\n",
      "        },\n",
      "        {\n",
      "            \"InputName\": \"german.csv\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "                \"LocalPath\": \"/opt/ml/processing/german.csv\",\n",
      "                \"S3Uri\": \"s3://creditmodel-yudhiesh-mlrawdata-867352166187-us-east-1/german.csv\",\n",
      "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3InputMode\": \"File\",\n",
      "                \"S3CompressionType\": \"None\",\n",
      "                \"S3DownloadMode\": \"StartOfJob\"\n",
      "            },\n",
      "            \"DatasetDefinition\": null\n",
      "        }\n",
      "    ],\n",
      "    \"ProcessingOutputConfig\": {\n",
      "        \"Outputs\": [\n",
      "            {\n",
      "                \"OutputName\": \"e61491fa-5fc6-4761-a5f2-4175fdc3ac72.default\",\n",
      "                \"AppManaged\": false,\n",
      "                \"S3Output\": {\n",
      "                    \"LocalPath\": \"/opt/ml/processing/output\",\n",
      "                    \"S3Uri\": \"s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler\",\n",
      "                    \"S3UploadMode\": \"EndOfJob\"\n",
      "                },\n",
      "                \"FeatureStoreOutput\": null\n",
      "            }\n",
      "        ],\n",
      "        \"KmsKeyId\": null\n",
      "    },\n",
      "    \"ProcessingResources\": {\n",
      "        \"ClusterConfig\": {\n",
      "            \"InstanceCount\": 1,\n",
      "            \"InstanceType\": \"ml.m5.4xlarge\",\n",
      "            \"VolumeSizeInGB\": 30,\n",
      "            \"VolumeKmsKeyId\": null\n",
      "        }\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::867352166187:role/DataScience/SC-867352166187-pp-7pfghfxa-SageMakerExecutionRole-P4ECNYCU9XXE\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll create a unique name each time the job is run:\n",
    "processing_job_name = util.uid.append_timestamp(\"credit-flow\")\n",
    "print(f\"Creating Processing job with name {processing_job_name}\")\n",
    "\n",
    "# ...And transparently the flow file to S3 as required for the processing job input:\n",
    "flow_upload_s3uri = f\"s3://{project_config.sandbox_bucket}/data-wrangler/{processing_job_name}/flow/credit-data.flow\"\n",
    "print(f\"Uploading flow file to {flow_upload_s3uri}\")\n",
    "\n",
    "# Processing outputs automatically create jobname/outputnames subfolders:\n",
    "base_output_s3uri = f\"s3://{project_config.sandbox_bucket}/data-wrangler\"\n",
    "print(f\"Storing results to {base_output_s3uri}\")\n",
    "\n",
    "processor.run(\n",
    "    # TODO: Change here to the \"credit-data.flow\" if using your own flow instead!\n",
    "    inputs=util.wrangler.create_processing_inputs(\"credit-prebuilt.flow\", flow_upload_s3uri),\n",
    "    # ^^^\n",
    "    outputs=[\n",
    "        util.wrangler.create_s3_output(target_output_name, base_output_s3uri),\n",
    "#         # Alternative for a feature store job:\n",
    "#         util.wrangler.create_featurestore_output(\n",
    "#             target_output_name,\n",
    "#             feature_group_name,\n",
    "#         )\n",
    "    ],\n",
    "    arguments=util.wrangler.create_container_arguments(target_output_name),\n",
    "    wait=True,\n",
    "    #logs=False,\n",
    "    job_name=processing_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our processing job completes successfully, the result data will be stored in Amazon S3.\n",
    "\n",
    "We can list the contents of the output folder, read it into a Pandas DataFrame here on the notebook, and even go check the data is visible in the [Amazon S3 Console](https://s3.console.aws.amazon.com/s3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/\n",
      "2021-03-25 08:19:07        852 part-00000-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "2021-03-25 08:19:07      47830 part-00001-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "2021-03-25 08:19:07     110396 part-00003-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n"
     ]
    }
   ],
   "source": [
    "flow_output_s3uri = f\"{base_output_s3uri}/{processing_job_name}/{target_output_name.replace('.', '/')}/\"\n",
    "print(flow_output_s3uri)\n",
    "!aws s3 ls $flow_output_s3uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/part-00000-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "Loading data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/part-00001-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "Loading data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/part-00003-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_default</th>\n",
       "      <th>txn_id</th>\n",
       "      <th>txn_timestamp</th>\n",
       "      <th>checking_acct_status</th>\n",
       "      <th>duration_months</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>present_employment_yrs_lt</th>\n",
       "      <th>installment_rate_disp_income_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>other_parties_none</th>\n",
       "      <th>other_parties_guarantor</th>\n",
       "      <th>other_parties_coapplicant</th>\n",
       "      <th>other_installment_plans_none</th>\n",
       "      <th>other_installment_plans_bank</th>\n",
       "      <th>other_installment_plans_stores</th>\n",
       "      <th>housing_own</th>\n",
       "      <th>housing_rent</th>\n",
       "      <th>housing_for_free</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1616653095</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>2225</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>835</td>\n",
       "      <td>1616653095</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>1616653095</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3915</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>1616653095</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>7485</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>335</td>\n",
       "      <td>1616653095</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  credit_default txn_id txn_timestamp checking_acct_status duration_months  \\\n",
       "0              1     54    1616653095                    2              36   \n",
       "1              1    835    1616653095                    1              12   \n",
       "2              1    192    1616653095                    2              27   \n",
       "3              1    175    1616653095                    0              30   \n",
       "4              1    335    1616653095                    1               6   \n",
       "\n",
       "  credit_history credit_amount savings_status present_employment_yrs_lt  \\\n",
       "0              3          2225              1                        10   \n",
       "1              0          1082              1                         4   \n",
       "2              2          3915              1                         4   \n",
       "3              1          7485              0                         0   \n",
       "4              4          3384              1                         4   \n",
       "\n",
       "  installment_rate_disp_income_pct  ... other_parties_none  \\\n",
       "0                                4  ...                1.0   \n",
       "1                                4  ...                1.0   \n",
       "2                                4  ...                1.0   \n",
       "3                                4  ...                1.0   \n",
       "4                                1  ...                1.0   \n",
       "\n",
       "  other_parties_guarantor other_parties_coapplicant  \\\n",
       "0                     0.0                       0.0   \n",
       "1                     0.0                       0.0   \n",
       "2                     0.0                       0.0   \n",
       "3                     0.0                       0.0   \n",
       "4                     0.0                       0.0   \n",
       "\n",
       "  other_installment_plans_none other_installment_plans_bank  \\\n",
       "0                          0.0                          1.0   \n",
       "1                          0.0                          1.0   \n",
       "2                          1.0                          0.0   \n",
       "3                          0.0                          1.0   \n",
       "4                          1.0                          0.0   \n",
       "\n",
       "  other_installment_plans_stores housing_own housing_rent housing_for_free  \\\n",
       "0                            0.0         0.0          0.0              1.0   \n",
       "1                            0.0         1.0          0.0              0.0   \n",
       "2                            0.0         1.0          0.0              0.0   \n",
       "3                            0.0         1.0          0.0              0.0   \n",
       "4                            0.0         0.0          1.0              0.0   \n",
       "\n",
       "   dataset  \n",
       "0    train  \n",
       "1    train  \n",
       "2    train  \n",
       "3    train  \n",
       "4    train  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = util.data.dataframe_from_s3_folder(flow_output_s3uri)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Check:** the data snapshot above looks OK!\n",
    "\n",
    "Since this dataset is small enough to load in Pandas, we can also perform some diagnostics here to check that our dataset was split correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "test          0.152\n",
       "train         0.698\n",
       "validation    0.150\n",
       "Name: credit_default, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset split is OK:\n",
    "dataset_lens = df.groupby([\"dataset\"])[\"credit_default\"].count()\n",
    "dataset_lens / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...That the stratification distributed samples as we hoped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset     credit_default\n",
       "test        0                 0.697368\n",
       "            1                 0.302632\n",
       "train       0                 0.700573\n",
       "            1                 0.299427\n",
       "validation  0                 0.700000\n",
       "            1                 0.300000\n",
       "Name: credit_default, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check stratification has worked:\n",
    "df.groupby([\"dataset\", \"credit_default\"])[\"credit_default\"].count() / dataset_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Or even whether there are any obvious problems with the distribution of non-stratified fields after the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset     gender_is_male\n",
       "test        0                 0.328947\n",
       "            1                 0.671053\n",
       "train       0                 0.312321\n",
       "            1                 0.687679\n",
       "validation  0                 0.280000\n",
       "            1                 0.720000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How about the distribution of non-stratified fields:\n",
    "df.groupby([\"dataset\", \"gender_is_male\"])[\"gender_is_male\"].count() / dataset_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all looks well, we're ready to move on to the next stage and start feeding our data into models!\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Extracting training/validation/test splits\n",
    "\n",
    "In a real-world use-case, it may be useful to store these features in a repository like [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/) to foster their discovery and re-use within our organization - rather than just as plain data in S3.\n",
    "\n",
    "We would then *query* this feature store to *realise* snapshots of the data for model training, validation, and test: And in the case of SageMaker Feature Store be able to use features like time travel to trace back changes between record versions.\n",
    "\n",
    "Since our environment for today's session doesn't support this feature, we'll **emulate** the idea using our existing data export and local `util` code as a feature store stand-in. The cell below will query the base export from data wrangler by the `dataset` column, to create separate folders in S3 for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/part-00000-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "Loading data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/part-00001-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "Loading data-wrangler/credit-flow-2021-03-25-08-14-10/e61491fa-5fc6-4761-a5f2-4175fdc3ac72/default/part-00003-4bb2c456-9acf-4906-8b41-fcb1da1d9abc-c000.csv\n",
      "\n",
      "Split datasets out to:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/model-datasets/train/part0.csv'],\n",
       " 'validation': ['s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/model-datasets/validation/part0.csv'],\n",
       " 'test': ['s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/model-datasets/test/part0.csv']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emulate SageMaker Feature Store since we're not able to use it here:\n",
    "model_datasets_s3uri = f\"s3://{project_config.sandbox_bucket}/model-datasets\"\n",
    "\n",
    "model_datasets = util.data.mock_featurestore_dataset_split(\n",
    "    flow_output_s3uri,\n",
    "    model_datasets_s3uri,\n",
    "    dataset_label_col=\"dataset\",\n",
    "    datasets_with_headers=r\"train.*\",  # Only include headers on the 'train' set\n",
    "    drop_cols=[\"txn_id\", \"txn_timestamp\"],  # Feature store internal columns, not for modelling\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit datasets out to:\")\n",
    "model_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training an initial model with SageMaker XGBoost Algorithm\n",
    "\n",
    "In SageMaker, *algorithms* (whether built-in or custom) are defined by **container images**... So we'll first need to look up the URI of the SageMaker XGBoost container of the version we'd like to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"1.0-1\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then set up our inputs - training and validation format prepared on S3 in a compatible format for the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/model-datasets/train\n",
      "Validation data: s3://creditmodel-yudhiesh-mlsandbox-867352166187-us-east-1/model-datasets/validation\n"
     ]
    }
   ],
   "source": [
    "train_uri = f\"{model_datasets_s3uri}/train\"\n",
    "print(f\"Training data: {train_uri}\")\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(train_uri, content_type=\"csv\")\n",
    "\n",
    "val_uri = f\"{model_datasets_s3uri}/validation\"\n",
    "print(f\"Validation data: {val_uri}\")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(val_uri, content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and finally, actually create the training job using the high-level [Estimator API](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html).\n",
    "\n",
    "The `Estimator` class provides a familiar, scikit-learn-like API for `fit()`ting models to data, `deploy()`ing models to real-time endpoints, or running batch inference jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 08:21:18 Starting - Starting the training job...\n",
      "2021-03-25 08:21:22 Starting - Launching requested ML instancesProfilerReport-1616660478: InProgress\n",
      ".........\n",
      "2021-03-25 08:23:06 Starting - Preparing the instances for training...\n",
      "2021-03-25 08:23:49 Downloading - Downloading input data\n",
      "2021-03-25 08:23:49 Training - Downloading the training image...\n",
      "2021-03-25 08:24:15 Training - Training image download completed. Training in progress..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[08:24:17] 699x40 matrix with 27960 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[08:24:17] 150x40 matrix with 6000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 699 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 150 rows\u001b[0m\n",
      "\u001b[34m[08:24:17] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.21459#011validation-error:0.24667\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.18741#011validation-error:0.28000\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.18026#011validation-error:0.26667\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.16309#011validation-error:0.30667\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.14592#011validation-error:0.26667\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.13448#011validation-error:0.27333\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.12303#011validation-error:0.26667\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.11159#011validation-error:0.25333\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.10873#011validation-error:0.23333\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.10157#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.09871#011validation-error:0.26000\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.09156#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.08584#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.08155#011validation-error:0.24667\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.08155#011validation-error:0.23333\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.07153#011validation-error:0.24667\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.06438#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.06438#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.06438#011validation-error:0.23333\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.05722#011validation-error:0.22000\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.05579#011validation-error:0.24667\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.05436#011validation-error:0.24667\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[113]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[114]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[115]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[116]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[117]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[118]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[119]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[120]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[121]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[122]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[123]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[124]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[125]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[126]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[127]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[128]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[129]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[130]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[131]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[132]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[133]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[134]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[135]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[136]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[137]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[138]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[139]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[140]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[141]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[142]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[143]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[144]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[145]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[146]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[147]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[148]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\u001b[34m[149]#011train-error:0.05436#011validation-error:0.24000\u001b[0m\n",
      "\n",
      "2021-03-25 08:24:46 Uploading - Uploading generated training model\n",
      "2021-03-25 08:24:46 Completed - Training job completed\n",
      "Training seconds: 51\n",
      "Billable seconds: 51\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an XGBoost estimator object\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,  # XGBoost algorithm container\n",
    "    instance_type=\"ml.m5.xlarge\",  # type of training instance\n",
    "    instance_count=1,  # number of instances to be used\n",
    "    role=sagemaker.get_execution_role(),  # Just use the current notebook's IAM role\n",
    "    max_run=20*60,  # Maximum allowed active runtime\n",
    "    base_job_name=f\"{project_id}-xgboost\",\n",
    "    output_folder=f\"s3://{project_config.sandbox_bucket}/model-training\",  # TODO: Maybe not respecting this?\n",
    "#     use_spot_instances=True,  # Use spot instances to reduce cost\n",
    "#     max_wait=30*60,  # Maximum clock time (including spot delays)\n",
    ")\n",
    "\n",
    "# define its hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "    num_round=150,  # int: [1,300]\n",
    "    max_depth=5,  # int: [1,10]\n",
    "    alpha=2.5,  # float: [0,5]\n",
    "    eta=0.5,  # float: [0,1]\n",
    "    objective=\"binary:logistic\",\n",
    ")\n",
    "\n",
    "# start a training (fitting) job\n",
    "estimator.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above will wait until the training job completes, and pull through logs from the job as it runs. You can also see additional information on the [Training jobs page of the SageMaker Console](https:/console.aws.amazon.com/sagemaker/home?#/jobs): Such infrastructure usage and algorithm metrics; and the parameters of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Testing the model with SageMaker Batch Transform\n",
    "\n",
    "While the training job itself produced error/accuracy metrics on the training and validation datasets; it's often beneficial to *evaluate* the trained model against some test dataset to see detailed results and drill in to performance.\n",
    "\n",
    "Below, we run a [SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) job using our trained model against the `test` dataset we created earlier and kept away from the training job.\n",
    "\n",
    "Because Batch Transform supports all kinds of models (not just tabular data, but e.g. computer vision or text too), there are additional configuration options for controlling how it feeds data into our model and consolidates result files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................\u001b[34m[2021-03-25:08:29:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-03-25:08:29:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-03-25:08:29:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-03-25:08:29:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-03-25:08:29:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-03-25:08:29:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [17] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2021-03-25 08:29:38 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [17] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35m[2021-03-25 08:29:38 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2021-03-25:08:29:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-03-25:08:29:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [25/Mar/2021:08:29:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [25/Mar/2021:08:29:43 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-03-25:08:29:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-03-25:08:29:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [25/Mar/2021:08:29:43 +0000] \"POST /invocations HTTP/1.1\" 200 2981 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [25/Mar/2021:08:29:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [25/Mar/2021:08:29:43 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-03-25:08:29:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-03-25:08:29:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [25/Mar/2021:08:29:43 +0000] \"POST /invocations HTTP/1.1\" 200 2981 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-03-25T08:29:43.157:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_uri = f\"{model_datasets_s3uri}/test\"\n",
    "test_output_uri = f\"s3://{project_config.sandbox_bucket}/model-eval/{estimator.latest_training_job.name}\"\n",
    "\n",
    "transformer = estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=test_output_uri,\n",
    "    accept=\"text/csv\",  # XGBoost algorithm supports multiple response formats - we want CSV\n",
    "    assemble_with=\"Line\",  # Consolidate inference responses as a single file, newline-separated (like a CSV)\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=test_data_uri,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"text/csv\",  # XGBoost algorithm supports multiple input formats - we have CSV\n",
    "    split_type=\"Line\",  # Batch Transform can split large input files into batches - on any newline is fine\n",
    "    join_source=\"Input\",  # Add the input columns in to the output file (easier accuracy reconciliation!)\n",
    "    input_filter=\"$[1:]\",  # Don't send the first (ground truth) column in to the model!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the batch transform job has completed successfully, our result files will be available in S3 at the output location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-25 08:29:44      23505 model-eval/creditmodel-yudhiesh-xgboost-2021-03-25-08-21-17-854/part0.csv.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $test_output_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read this data in to our notebook as before - but need to set up the column names manually since the file itself doesn't have a header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model-eval/creditmodel-yudhiesh-xgboost-2021-03-25-08-21-17-854/part0.csv.out\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_default</th>\n",
       "      <th>checking_acct_status</th>\n",
       "      <th>duration_months</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>present_employment_yrs_lt</th>\n",
       "      <th>installment_rate_disp_income_pct</th>\n",
       "      <th>present_residence_since</th>\n",
       "      <th>highest_property</th>\n",
       "      <th>...</th>\n",
       "      <th>other_parties_none</th>\n",
       "      <th>other_parties_guarantor</th>\n",
       "      <th>other_parties_coapplicant</th>\n",
       "      <th>other_installment_plans_none</th>\n",
       "      <th>other_installment_plans_bank</th>\n",
       "      <th>other_installment_plans_stores</th>\n",
       "      <th>housing_own</th>\n",
       "      <th>housing_rent</th>\n",
       "      <th>housing_for_free</th>\n",
       "      <th>credit_default_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>674</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>1808</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>5096</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>939</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>4605</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.699565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_default  checking_acct_status  duration_months  credit_history  \\\n",
       "0               1                     1               12               2   \n",
       "1               1                     0               18               3   \n",
       "2               1                     2               48               4   \n",
       "3               1                     3               12               4   \n",
       "4               1                     1               48               0   \n",
       "\n",
       "   credit_amount  savings_status  present_employment_yrs_lt  \\\n",
       "0            674               2                          7   \n",
       "1           1808               1                          7   \n",
       "2           5096               1                          4   \n",
       "3            939               3                          7   \n",
       "4           4605               1                         10   \n",
       "\n",
       "   installment_rate_disp_income_pct  present_residence_since  \\\n",
       "0                                 4                        1   \n",
       "1                                 4                        1   \n",
       "2                                 2                        3   \n",
       "3                                 4                        2   \n",
       "4                                 3                        4   \n",
       "\n",
       "   highest_property  ...  other_parties_none  other_parties_guarantor  \\\n",
       "0                 2  ...                 1.0                      0.0   \n",
       "1                 1  ...                 1.0                      0.0   \n",
       "2                 3  ...                 1.0                      0.0   \n",
       "3                 1  ...                 1.0                      0.0   \n",
       "4                 4  ...                 1.0                      0.0   \n",
       "\n",
       "   other_parties_coapplicant  other_installment_plans_none  \\\n",
       "0                        0.0                           1.0   \n",
       "1                        0.0                           1.0   \n",
       "2                        0.0                           1.0   \n",
       "3                        0.0                           1.0   \n",
       "4                        0.0                           1.0   \n",
       "\n",
       "   other_installment_plans_bank  other_installment_plans_stores  housing_own  \\\n",
       "0                           0.0                             0.0          1.0   \n",
       "1                           0.0                             0.0          1.0   \n",
       "2                           0.0                             0.0          1.0   \n",
       "3                           0.0                             0.0          1.0   \n",
       "4                           0.0                             0.0          0.0   \n",
       "\n",
       "   housing_rent  housing_for_free  credit_default_pred  \n",
       "0           0.0               0.0             0.222375  \n",
       "1           0.0               0.0             0.023244  \n",
       "2           0.0               0.0             0.418617  \n",
       "3           0.0               0.0             0.151963  \n",
       "4           0.0               1.0             0.699565  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_colnames = list(\n",
    "    filter(lambda c: c not in {\"dataset\", \"txn_id\", \"txn_timestamp\"}, df.columns.tolist())\n",
    ") + [\"credit_default_pred\"]\n",
    "\n",
    "test_result_df = util.data.dataframe_from_s3_folder(test_output_uri, header=None, names=test_result_colnames)\n",
    "test_result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the model score in the final column `credit_default_pred` with the ground truth value in the first column `credit_default` - by whatever our preferred metrics are for analyzing the result.\n",
    "\n",
    "The cell below will create a classification model summary report from the two columns, including a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), a [Receiver Operating Characteristic (ROC) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic), and some other summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util.plotting.generate_classification_report(\n",
    "    y_real=test_result_df[\"credit_default\"],\n",
    "    y_predict_proba=test_result_df[\"credit_default_pred\"],\n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"good\", \"default\"],\n",
    "    title=\"Initial credit risk model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our original test, this initial model achieved an AUC of around 0.685; with a final validation error (in the training job) of 0.24000... Plenty of room for improvement with better feature engineering and hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Used SageMaker Data Wrangler to **analyze and featurize** raw data from our project's `raw_bucket`\n",
    "- **Ran our Data Wrangler flow** as a SageMaker Processing Job, to output featurized data to our project `sandbox_bucket` (although we might have used SageMaker Feature Store as our repository instead)\n",
    "- **Queried** this main feature store to realise our individual `train`, `validation` and `test` datasets in S3\n",
    "- **Trained** a first model with SageMaker's first-party XGBoost Algorithm implementation\n",
    "- **Evaluated** the model's predictions on our `test` dataset\n",
    "\n",
    "Next, we'll consider ways to automate workflows for model (re)-training, evaluation, approval, deployment, and monitoring - introducing [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/)!"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
